{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"Learn/DeepLearning%20knowledge/","title":"DeepLearning","text":""},{"location":"Learn/DeepLearning%20knowledge/#_1","title":"\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u77e5\u8bc6","text":"<p>\u8fd9\u91cc\u8bb0\u5f55\u7684\u5c31\u4e0d\u662f\u5c0f\u7684\u77e5\u8bc6\u70b9\u4e86\uff0c\u4f1a\u68b3\u7406\u4e00\u4e9b\u53ef\u80fd\u5bb9\u6613\u6df7\u6dc6\u7684\u6df1\u5ea6\u5b66\u4e60\u6982\u5ff5\uff0c\u9700\u8981\u66f4\u52a0\u7684\u7cfb\u7edf\u5316\u3002</p>"},{"location":"Learn/DeepLearning%20knowledge/#batchepoch","title":"batch\u548cepoch\u4e4b\u95f4\u7684\u533a\u522b","text":"<p>\u8fd9\u4e24\u4e2a\u6982\u5ff5\u5e94\u8be5\u662f\u521d\u5b66\u8005\u975e\u5e38\u5bb9\u6613\u6df7\u6dc6\u7684\u5185\u5bb9\uff0c\u8fd9\u91cc\u8fdb\u884c\u4e00\u4e9b\u4ecb\u7ecd\u3002 1. \u9996\u5148\uff0cepoch\u4e2d\u662f\u5305\u542b\u4e0d\u540c\u7684batch\u7684\uff0c\u5c06\u4e00\u4e2a\u5927\u578b\u7684\u6570\u636e\u96c6\u5206\u6210\u5c0f\u7684batch\u53ef\u4ee5\u51cf\u5c11\u5185\u5b58\u538b\u529b\uff0c\u52a0\u5feb\u8ba1\u7b97\u6548\u7387\uff0c\u8fd9\u662f\u6700\u4e3b\u8981\u7684\u4f18\u70b9\u3002 2. \u53e6\u4e00\u65b9\u9762\uff0c\u6211\u8ba4\u4e3abatch\u4e2d\u6240\u505a\u7684\u8d21\u732e\u8981\u5927\u4e8e\u6bcf\u4e2aepoch\u7684\u3002\u5728\u6bcf\u4e2a\u4e0d\u540c\u7684batch\u4e2d\uff0c\u6a21\u578b\u90fd\u9700\u8981\u7ecf\u8fc7\u524d\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u635f\u5931\uff0c\u53cd\u5411\u4f20\u64ad\uff0c\u66f4\u65b0\u53c2\u6570\u7684\u6b65\u9aa4\u3002 3. \u5728\u6bcf\u4e2aepoch\u4e4b\u95f4\uff0c\u51fa\u4e86\u9700\u8981\u904d\u5386batch\u5916\uff0c\u53ef\u4ee5\u9009\u62e9\u8ba1\u7b97\u6a21\u578b\u7684\u8868\u73b0\u6548\u679c\uff0c\u6bd4\u5982\u51c6\u786e\u7387\uff0c\u4e5f\u53ef\u4ee5\u66f4\u6539\u6a21\u578b\u7684\u5b66\u4e60\u7387\uff1b\u540c\u6837\u4e5f\u53ef\u4ee5\u9009\u62e9\u4fdd\u5b58\u6a21\u578b\u3002</p>"},{"location":"Learn/DeepLearning%20knowledge/#dataloader","title":"DataLoader\u90fd\u505a\u4e86\u4e9b\u4ec0\u4e48\uff1f","text":"<p>\u5728PyTorch\u4e2d\uff0cDataLoader\u662f\u4e00\u4e2a\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u548c\u6279\u6b21\u5904\u7406\u7684\u5b9e\u7528\u7a0b\u5e8f\u7c7b\u3002\u5b83\u63d0\u4f9b\u4e86\u4ee5\u4e0b\u529f\u80fd\uff1a</p> <ol> <li> <p>\u6570\u636e\u52a0\u8f7d\uff1a DataLoader\u53ef\u4ee5\u4ece\u7ed9\u5b9a\u7684\u6570\u636e\u96c6\uff08\u5982PyTorch\u7684Dataset\u5bf9\u8c61\uff09\u4e2d\u52a0\u8f7d\u6570\u636e\u3002\u6570\u636e\u96c6\u53ef\u4ee5\u662f\u81ea\u5b9a\u4e49\u7684\uff0c\u4e5f\u53ef\u4ee5\u662fPyTorch\u63d0\u4f9b\u7684\u9884\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u5982MNIST\u3001CIFAR-10\u7b49\u3002</p> </li> <li> <p>\u6279\u6b21\u5904\u7406\uff1a DataLoader\u80fd\u591f\u5c06\u6570\u636e\u96c6\u5206\u6210\u6279\u6b21\uff08batches\uff09\u3002\u901a\u8fc7\u6307\u5b9a\u6279\u6b21\u5927\u5c0f\uff08batch size\uff09\uff0cDataLoader\u4f1a\u81ea\u52a8\u4ece\u6570\u636e\u96c6\u4e2d\u6309\u7167\u6307\u5b9a\u5927\u5c0f\u52a0\u8f7d\u6570\u636e\u5e76\u7ec4\u6210\u6279\u6b21\u3002</p> </li> <li> <p>\u6570\u636e\u968f\u673a\u5316\uff1a DataLoader\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u968f\u673a\u5316\u5904\u7406\u3002\u5b83\u4f1a\u5728\u6bcf\u4e2a epoch \u5f00\u59cb\u65f6\u91cd\u65b0\u6d17\u724c\u6570\u636e\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u5728\u4e0d\u540c\u7684\u6279\u6b21\u4e2d\u770b\u5230\u591a\u6837\u5316\u7684\u6837\u672c\u3002\u8fd9\u5bf9\u4e8e\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u975e\u5e38\u91cd\u8981\u3002</p> </li> <li> <p>\u5e76\u884c\u52a0\u8f7d\uff1a DataLoader\u652f\u6301\u591a\u7ebf\u7a0b\u5e76\u884c\u6570\u636e\u52a0\u8f7d\u3002\u5b83\u53ef\u4ee5\u5229\u7528\u591a\u4e2a\u7ebf\u7a0b\u540c\u65f6\u4ece\u5b58\u50a8\u4ecb\u8d28\uff08\u5982\u786c\u76d8\uff09\u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u5e76\u5728\u9700\u8981\u65f6\u9884\u5148\u51c6\u5907\u4e0b\u4e00\u6279\u6570\u636e\uff0c\u4ece\u800c\u51cf\u5c11\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6570\u636e\u52a0\u8f7d\u7b49\u5f85\u65f6\u95f4\u3002</p> </li> <li> <p>\u6570\u636e\u8f6c\u6362\uff1a DataLoader\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u6570\u636e\u53d8\u6362\uff08\u5982\u56fe\u50cf\u7f29\u653e\u3001\u88c1\u526a\u3001\u6807\u51c6\u5316\u7b49\uff09\u6765\u5bf9\u52a0\u8f7d\u7684\u6570\u636e\u8fdb\u884c\u9884\u5904\u7406\u3002\u8fd9\u4e9b\u53d8\u6362\u53ef\u4ee5\u901a\u8fc7\u7ec4\u5408PyTorch\u7684Transforms\u6765\u5b9a\u4e49\uff0c\u5e76\u5728DataLoader\u52a0\u8f7d\u6570\u636e\u65f6\u5e94\u7528\u5230\u6bcf\u4e2a\u6837\u672c\u4e0a\u3002</p> </li> <li> <p>\u8fed\u4ee3\u652f\u6301\uff1a DataLoader\u662f\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u5bf9\u8c61\uff0c\u53ef\u4ee5\u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\u4f7f\u7528for\u5faa\u73af\u9010\u4e2a\u8bbf\u95ee\u6bcf\u4e2a\u6279\u6b21\u7684\u6570\u636e\u3002\u8fd9\u6837\u53ef\u4ee5\u65b9\u4fbf\u5730\u5c06\u6570\u636e\u4f20\u9012\u7ed9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002</p> </li> </ol> <p>\u901a\u8fc7\u4f7f\u7528DataLoader\uff0c\u53ef\u4ee5\u7b80\u5316\u6570\u636e\u52a0\u8f7d\u548c\u6279\u6b21\u5904\u7406\u7684\u8fc7\u7a0b\uff0c\u4f7f\u6570\u636e\u51c6\u5907\u90e8\u5206\u4e0e\u6a21\u578b\u8bad\u7ec3\u90e8\u5206\u5206\u79bb\uff0c\u4ece\u800c\u4f7f\u4ee3\u7801\u66f4\u52a0\u6e05\u6670\u3001\u53ef\u8bfb\u6027\u66f4\u5f3a\uff0c\u5e76\u63d0\u9ad8\u6570\u636e\u52a0\u8f7d\u7684\u6548\u7387\u3002</p>"},{"location":"Learn/DeepLearning%20knowledge/#position-encoding","title":"Position encoding","text":""},{"location":"Learn/DeepLearning%20knowledge/#scaled-dot-product-attention","title":"Scaled Dot-Product Attention","text":"<p>\u5982\u4f55\u5c31\u7b97\u7684\uff1f\u5982\u4f55mask\u7684\uff1f</p>"},{"location":"Learn/DeepLearning%20knowledge/#multi-head-attention","title":"Multi-Head Attention","text":"<p>\u7531Scaled Dot-Product Attention\u7684\u539f\u7406\u6211\u4eec\u53ef\u4ee5\u77e5\u9053\uff0c\u5176\u4e2d\u662fQ\uff0cK\uff0cV\u4e4b\u95f4\u7684\u77e9\u9635\u4e58\u6cd5\uff0c\u5e76\u6ca1\u6709\u53ef\u4ee5\u5b66\u4e60\u7684\u53c2\u6570\u3002\u90a3\u4e48\u4e3a\u4e86\u5339\u914d\u4e0d\u540c\u95ee\u9898\u4e2d\u7684\u4e0d\u540c\u7684\u6a21\u5f0f\uff0c\u7ed9\u6a21\u578b\u66f4\u591a\u7684\u7684\u5b66\u4e60\u673a\u4f1a\uff0c\u6240\u4ee5\u5c31\u4f7f\u7528\u4e86\u8fd9\u79cd\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u3002\u4ed6\u662f\u600e\u4e48\u505a\u7684\u5462\uff1f\u4ed6\u5c06Q\uff0cK\uff0cV\u503c\u901a\u8fc7\u7ebf\u6027\u5c42\u6620\u5c04\u4e86h\u6b21\uff0c\u5206\u522b\u8ba1\u7b97\u6bcf\u6b21\u7684\u70b9\u79ef\u6ce8\u610f\u529b\uff0cconcat\u4e4b\u540e\u518d\u7ecf\u8fc7\u4e00\u4e2a\u7ebf\u6027\u5c42\u8f93\u51fa\uff0c\u8fd9\u6837\u5c31\u589e\u52a0\u4e86\u5f88\u591a\u7684\u5b66\u4e60\u673a\u4f1a\u3002</p> <p>\u6bcf\u4e2a\u6620\u5c04\u5c42\u7684\u7eac\u5ea6\u662fd_model/h,\u56e0\u6b64\u5728\u4ee3\u7801\u5b9e\u64cd\u4e2d\u4f1a\u6d89\u53ca\u62c6\u5206\u7684\u6b65\u9aa4\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u5e76\u884c\u8ba1\u7b97\uff0c\u4f46\u5b9e\u9645\u539f\u7406\u4e0a\u597d\u50cf\u662f\u6709\u5dee\u5f02\u7684\uff1f\u8fd9\u91cc\u7684\u7406\u89e3\u6709\u70b9\u95ee\u9898\uff0c\u53ef\u4ee5\u662f\u793a\u610f\u56fe\u7ed9\u51fa\u6765\u7684\u4e0d\u592a\u5408\u9002\u3002\u5728\u5b9e\u9645\uff08\u4ee3\u7801\uff09\u64cd\u4f5c\u4e0a\uff0c\u662f\u5c06\u6620\u5c04\u540e\u7684\u7279\u5f81\u5411\u91cf\u62c6\u5206\u6210head\u4efd\uff0c\u6765\u5206\u522b\u8ba1\u7b97\u6bcf\u4e2ahead\u7684\u6ce8\u610f\u529b\u5f97\u5206\u3002</p> <ol> <li>\u8bba\u6587\u7684\u63cf\u8ff0\u662f\u901a\u8fc7\u591a\u4e2aW\uff08\u7ebf\u6027\u5c42\uff09\u6620\u5c04\u5230\u5b50\u7a7a\u95f4\u91cc\uff0c\u4f46\u662f\u5b9e\u9645\u64cd\u4f5c\u65f6\u662f\u901a\u8fc7\u4e00\u4e2a\u6620\u5c04\u7136\u540e\u5207\u5206\uff0c\u539f\u7406\u4e0a\u4e00\u6837\uff0c\u4e3a\u4ec0\u4e48\u4e00\u6837\u5462\uff1f</li> </ol> <p>\u7b54\u6848\uff1a\u6587\u7ae0\u63cf\u8ff0\u662f\u65e0\u95ee\u9898\u7684\uff0c\u4e0d\u8fc7\u662f\u5728\u5b9e\u9645\u5e94\u7528\u7684\u65f6\u5019\uff0c\u901a\u8fc7reshape\u548c\u77e9\u9635\u4e58\u6cd5\u7b49\u64cd\u4f5c\u8fdb\u884c\u5e76\u884c\u8ba1\u7b97\u4e86\uff0c\u4f46\u662f\u4e3a\u4ec0\u4e48\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97\u4e86\uff0c\u6211\u8fd8\u662f\u4e0d\u80fd\u7406\u89e3\u3002</p> <p>\u4e3b\u8981\u95ee\u9898\u5c31\u662f\u4e0b\u56fe\u63cf\u8ff0\u7684\uff0c\u4e3a\u4ec0\u4e48\u4e24\u8005\u7b49\u4ef7\uff1f </p>"},{"location":"Learn/DeepLearning%20knowledge/#transformer-decoder","title":"Transformer Decoder","text":"<p>Docoder\u76f8\u8f83\u4e8eEncoder\u6bd4\u8f83\u9ebb\u70e6\uff0c\u5b58\u5728\u4e00\u4e2amask\u64cd\u4f5c\uff0cmask\u7684\u542b\u4e49\u662f\uff0c\u5728t\u65f6\u523b\u7684\u8f93\u5165\uff0cdecoder\u7684\u8f93\u5165\u662f\u65e0\u6cd5\u770b\u5230t+1\u65f6\u523b\u7684\u5185\u5bb9\u7684\uff0c\u90a3\u4e48\u5728\u5b9e\u9645\u64cd\u4f5c\u7684\u65f6\u5019\uff0c\u662f\u901a\u8fc7\u4e00\u4e2a\u4e09\u89d2\u77e9\u9635\uff08\u53f3\u4e0a\u89d2\u4e3a0\uff09\u5c06\u540e\u9762\u7684\u5185\u5bb9\u53d8\u4e3a0\uff0c\u5728\u4ee3\u7801\u4e2d\u66ff\u6362\u4e3a-inf\u3002\u56e0\u6b64\u597d\u50cf\u8fd9\u4e00\u6b65\u64cd\u4f5c\u4e0d\u662f\u5728\u6a21\u578b\u4ee3\u7801\u4e2d\u6784\u5efa\u7684\uff0c\u800c\u662f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6784\u9020\u7684\u3002 </p> <p>\u8fd9\u91cc\u5176\u5b9e\u662f\u6bd4\u8f83\u597d\u7406\u89e3\u7684\u3002\u9996\u5148\uff0c\u5728\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cDecoder\u662f\u8fdb\u884c\u5e76\u884c\u8ba1\u7b97\u7684\uff0c\u56e0\u4e3a\u6b64\u65f6\u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u5f97\u5230\u5168\u90e8\u7684\u8f93\u5165\uff0c\u867d\u7136\u8f93\u5165\u662f\u7ecf\u8fc7mask\u7684\uff0c\u800c\u5728\u6a21\u578b\u6d4b\u8bd5\u6216\u8005\u8bf4\u662f\u5e94\u7528\u65f6\uff0c\u5e76\u4e0d\u662f\u5e76\u884c\u7684\uff0c\u800c\u662f\u9700\u8981\u4e00\u4e2a\u4e2a\u7684\u83b7\u5f97\u8f93\u5165\uff0c\u8fdb\u884cself masked attention.</p>"},{"location":"Learn/DeepLearning%20knowledge/#src-mask-tar_mask","title":"src-mask &amp; tar_mask","text":"<p>src-mask\u4e0etar-mask\u4e4b\u95f4\u6709\u7740\u672c\u8d28\u6027\u7684\u4e0d\u540c\uff0c\u867d\u7136\u4e24\u79cd\u90fd\u662f\u7528\u6765\u4f5c\u4e3a\u5c4f\u853d\u77e9\u9635\u7684\u3002\u4f46\u662fsrc-mask\u662f\u5728encoder\u4e2d\u4f7f\u7528\u7684\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u5c4f\u853d\u90a3\u4e9bpadding\u7684\u4f4d\u7f6e\uff0csrc-mask\u5e94\u7528\u4e8e\u8f93\u5165\u7684\u5e8f\u5217\u3002tar_mask\u5219\u662f\u5e94\u7528\u4e8e\u76ee\u6807\u5e8f\u5217\uff0c\u5b83\u9664\u4e86\u5c4f\u853dpadding\u7684\u4f4d\u7f6e\u5916\uff0c\u8fd8\u5e94\u7528\u4e8e\u5c4f\u853d\u672a\u6765\u4fe1\u606f\u3002</p>"},{"location":"Learn/How-build-statical-website/","title":"MKdoc","text":"<p>2023.4.14</p>"},{"location":"Learn/How-build-statical-website/#mkdocs","title":"\u901a\u8fc7MKdocs\u6784\u5efa\u9759\u6001\u7f51\u7ad9","text":""},{"location":"Learn/How-build-statical-website/#_1","title":"\u9700\u6c42","text":"<p>\u4e3b\u8981\u662f\u6784\u5efa\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u6587\u6863\u3002\u6700\u5f00\u59cb\u7684\u65f6\u5019\u662f\u57fa\u4e8eRmarkdown\u6765\u6784\u5efa\u9759\u6001\u7f51\u9875\u6587\u6863\u7684\uff0c\u4f46\u662f\u5728\u6211\u7684\u4e0a\u4e00\u4e2a\u9879\u76eeTLimmuno2\u4e2d\uff0c\u6211\u662f\u6709\u5927\u91cf\u7684python\u4ee3\u7801\uff08\u6211\u540e\u7eed\u7684\u9879\u76ee\u4e5f\u4f1a\u4e3b\u8981\u57fa\u4e8epython\uff09\uff0c\u56e0\u6b64\u53ea\u80fd\u901a\u8fc7reticulate\u5728R\u4e2d\u8c03\u7528python\u3002\u6574\u4f53\u6765\u8bf4\u4f1a\u9ebb\u70e6\uff0c\u800c\u4e14\u5f53\u65f6Rmakrdown\u7684\u7f16\u8bd1\u65f6\u95f4\u4e5f\u8f83\u957f\u3002</p> <p>\u90a3\u4e48\u6709\u6ca1\u6709python\u4e2d\u7684\u6587\u6863\u6784\u5efa\u5305\u5462\uff1f\u80af\u5b9a\u662f\u6709\u7684</p> <p>\u6700\u5f00\u59cb\u662f\u57fa\u4e8esphinx\uff0c\u4f46\u662f\u6709\u70b9\u53cd\u4eba\u7c7b\uff08\u53cd\u6211\uff09\u7684\u662f\uff0c\u5b83\u7684\u8bed\u6cd5\u662f\u57fa\u4e8erestrueturetext\uff0c\u8981\u5b66\u65b0\u7684\u8bed\u6cd5\uff0c\u66f4\u9ebb\u70e6\u3002</p> <p>\u540e\u6765\u5728\u627epytorch\u7684\u5b66\u4e60\u8d44\u6599\u65f6\uff0c\u53d1\u73b0\u4e86\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u7684\u6587\u6863\uff0c\u8fd9\u4e2a\u6587\u6863\u4e5f\u540c\u65f6\u53ef\u4ee5\u8bfb\u53d6jupyter notebook\u7684\u4fe1\u606f\uff0c\u975e\u5e38\u7b26\u5408\u6211\u7684\u9700\u6c42\uff0c\u7ecf\u8fc7\u8be2\u95ee\u4e86\u89e3\u5230\u4e86MKdocs\u3002</p>"},{"location":"Learn/How-build-statical-website/#_2","title":"\u5feb\u901f\u5f00\u59cb","text":"<p>\u8fd9\u91cc\u6211\u4eec\u4e3b\u8981\u662f\u57fa\u4e8eMKdocs\u7684\u4e00\u4e2a\u4e3b\u9898\uff1amkdocs-material\uff0c\u5b83\u4f1a\u81ea\u52a8\u5b89\u88c5\u6240\u6709\u7684\u4f9d\u8d56\uff0c\u5305\u62ec\u6700\u4e3b\u8981\u7684MKdocs\u3002</p> <p><code>pip install mkdocs-material</code>.</p> <p>\u540c\u6837\u7684\uff0c\u4ed6\u4e5f\u652f\u6301docker\uff0c\u8fd9\u91cc\u6211\u4eec\u5e76\u4e0d\u4f1a\u6d89\u53ca\u3002</p> <p>\u5728\u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\uff0c\u901a\u8fc7<code>mkdocs new file-name</code>\u7684\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5728\u5f53\u524d\u6587\u4ef6\u5939\uff0c\u6587\u4ef6\u5939\u4e0b\u5305\u542b\u4e24\u4e2a\u5185\u5bb9 docs/index.md\u548cmkdocs.yml\u3002</p> <p>\u5176\u4e2d\u91cd\u8981\u7684\u662fmakdocs.yml\uff0c\u5b83\u53ef\u4ee5\u63a7\u5236\u6574\u4e2a\u9875\u9762\u7684\u5c55\u793a\u6548\u679c\uff0c\u63a7\u5236\u5404\u79cd\u5404\u6837\u7684\u529f\u80fd\uff0c\u60f3\u8981\u5bf9\u7f51\u9875\u8fdb\u884c\u5b9a\u5236\u5316\uff0c\u5219\u9700\u8981\u4e86\u89e3\u5176\u76f8\u5173\u7684\u9009\u9879\u3002</p>"},{"location":"Learn/How-build-statical-website/#github-pages","title":"Github pages","text":"<p>\u6587\u6863\u4e2d\u7ed9\u51fa\u4e86\u8be6\u7ec6\u7684\u6b65\u9aa4,\u4e3b\u8981\u662f\u6839\u636egithub action\u6765\u8fdb\u884c\u81ea\u52a8\u7684\u90e8\u7f72\uff0c\u9700\u8981\u6ce8\u610f\u7684\u989d\u5916\u7684\u5185\u5bb9\u662f\u8981\u6839\u636e\u81ea\u5df1\u7684\u6240\u5b89\u88c5\u7684\u63d2\u4ef6(plugin)\u6765\u4fee\u6539yml\u6587\u4ef6\u3002</p> <p>\u8fd9\u91cc\u591a\u5199\u4e00\u70b9\uff1a\u76f4\u89c2\u7684\u611f\u89c9github action\u4f1a\u662f\u4e00\u4e2a\u975e\u5e38\u6709\u7528\u4e14\u4fbf\u5229\u7684\u65b9\u5f0f\uff0c\u6709\u673a\u4f1a\u7684\u8bdd\u53ef\u4ee5\u53bb\u5b66\u4e60\u4e00\u4e0b\u3002</p>"},{"location":"Learn/How-build-statical-website/#_3","title":"\u57df\u540d\u89e3\u6790","text":"<p>\u5728\u8d2d\u4e70\u597d\u57df\u540d\u4e4b\u540e\uff0c\u53ea\u9700\u6837\u76f8\u4e92\u7406\u89e3\u89e3\u6790\u5c31\u53ef\u4ee5\u4e86\uff0c\u6709\u8be6\u7ec6\u7684\u6559\u7a0b\u3002</p>"},{"location":"Learn/How-build-statical-website/#customization","title":"customization\uff1a","text":""},{"location":"Learn/How-build-statical-website/#_4","title":"\u52a8\u6001\u9875\u9762","text":"<p>\u5982\u4f55\u53bb\u6784\u5efa\u4e00\u4e2a\u50cfmkdocs\u4e00\u6837\u7684\u52a8\u6001\u7684homepage\uff1f</p> <p>\u6839\u636e\u4f5c\u8005\u7684\u63cf\u8ff0\u77e5\u9053\u7531\u4e8e\u6d89\u53ca\u5230\u7248\u6743\u7684\u95ee\u9898\uff0c\u8be5\u90e8\u5206\u7684\u4ee3\u7801\u662f\u4e0d\u516c\u5e03\u7684\uff0c\u9664\u975e\u79f0\u4e3a\u8d5e\u52a9\u8005\uff08insider\uff09\u3002\u4f46\u662f\u5728\u76f8\u5e94\u7684issue\u4e2d\uff0c\u5df2\u7ecf\u6709\u4eba\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5c31\u662f\u5728index.md\u6587\u4ef6\u4e2d\u6307\u5b9a\u6211\u4eec\u60f3\u8981\u7684html\u7684template\uff0c\u5373\uff1a</p> <pre><code>---\ntitle: Home\ntemplate: home.html\n---\n</code></pre> <p>\u800c\u5176\u4e2d\u7684home.html\u5e94\u8be5\u5728material/override\u8def\u5f84\u4e0b\uff08\u8be5\u8def\u5f84\u5e94\u8be5\u5728mkdocs.yml\u4e2d\u6307\u5b9a\uff09</p> <p>\u56e0\u4e3a\u8fd9\u91cc\u9700\u8981\u7684Html&amp;css\u77e5\u8bc6\u592a\u591a\u4e86\uff0c\u6211\u662f\u65e0\u6cd5\u5904\u7406\u7684\uff0c\u6240\u4ee5\u5c31\u57fa\u4e8e\u73b0\u6709\u7684\u9879\u76ee\u6765\u8fdb\u884c\u66f4\u6539\uff0c\u8fd9\u6837\u4e0d\u4ec5\u7b80\u4fbf\u800c\u4e14\u4e5f\u597d\u770b\u3002</p>"},{"location":"Learn/How-build-statical-website/#_5","title":"\u66f4\u6539\u56fe\u6807","text":"<p>\u56fe\u6807\u66f4\u6539\u9700\u8981\u653e\u5230docs\u7684assets\u6587\u4ef6\u5939\u4e0b\uff0c\u5e76\u4e14\u8981\u5728mkdocs.yml\u4e2d\u6307\u5b9a\u8def\u5f84\u3002\u56fe\u6807\u7ed8\u5236\u53ef\u4ee5\u9009\u62e9\u5728\u7ebf\u7f51\u7ad9\uff0c\u4f8b\u5982favicon.io\u3002</p>"},{"location":"Learn/How-build-statical-website/#mkdocsymlfeature","title":"mkdocs.yml\u4e2d\u7684feature\u5143\u7d20","text":"<p>\u8be5\u5143\u7d20\u662f\u5bf9\u9875\u9762\u7684\u5c55\u793a\u8fdb\u884c\u5b9a\u5236\u5316\u8c03\u6574\uff0c\u8be6\u7ec6\u7684\u5185\u5bb9\u5728\u8fd9\u91cc.</p>"},{"location":"Learn/How-build-statical-website/#change-log","title":"change log\u7684\u6784\u5efa","text":"<p>\u597d\u50cf\u53ef\u4ee5\u81ea\u52a8\u540c\u6b65\u6bcf\u6b21\u63d0\u4ea4\u7684\u53d8\u5316\uff0c</p>"},{"location":"Learn/How-build-statical-website/#_6","title":"\u8e29\u5751","text":""},{"location":"Learn/How-build-statical-website/#_7","title":"\u62a5\u9519\u4fe1\u606f\uff1a","text":"<p><code>\u201c A relative path to 'MKdoc.md' is included in the 'nav' configuration, which is not found in the documentation file\uff1a\u201c</code></p> <p>\u4ea7\u751f\u7684\u539f\u56e0\u662f\u540d\u79f0\u7684\u95ee\u9898\uff0c\u5c06\u540d\u5b57\u6539\u4e3a123.md\u4e4b\u540e\uff0c\u5c31\u4e0d\u5b58\u5728\u8fd9\u4e2a\u95ee\u9898\u4e86\uff0c\u53ef\u80fd\u662fmkdoc.md\u7684\u540d\u79f0\u662f\u88ab\u4fdd\u7559\u7684\uff0c\u4f7f\u7528\u7684\u8bdd\u5c31\u4f1a\u62a5\u9519\u3002</p> <p>\u5e76\u4e0d\u662f\u4e0a\u9762\u7684\u539f\u56e0\uff0c\u91cd\u65b0\u4fee\u6539\u5176\u4ed6\u7684\u540d\u79f0\u4e4b\u540e\uff0c\u53ea\u8981\u5305\u542b\u5b57\u6bcd\u5c31\u51fa\u9519\uff0c\u539f\u56e0\uff1f</p> <p>\u597d\u50cf\u786e\u5b9e\u662f\u4e0a\u9762\u7684\u539f\u56e0\uff0c\u8981\u5b8c\u5168\u907f\u514dMKdocs\u5143\u7d20\u7684\u5b58\u5728\uff0c\u5c31\u5f88\u5947\u602a\u3002</p>"},{"location":"Learn/How-build-statical-website/#table-of-content","title":"\u8fd8\u6709\u4e00\u4e2a\u95ee\u9898\uff1atable of content\u4e0d\u663e\u793a","text":"<p>\u89e3\u7b54\uff1ahttps://github.com/squidfunk/mkdocs-material/issues/818#issuecomment-629646709</p> <p>\u4e3b\u8981\u539f\u56e0\u5c31\u662fmkdocs\u53ea\u4eceh2\u6807\u9898\u5f00\u59cb\u6293\u53d6\u5236\u4f5ctable of content\uff0c\u56e0\u6b64\u5728\u4f7f\u7528\u7684\u65f6\u5019\uff0ch1\u6807\u9898\u53ea\u4f7f\u7528\u4e00\u4e2a\uff0c\u540e\u7eed\u591a\u7528h2\u7684\u6807\u9898\u3002</p>"},{"location":"Learn/How-build-statical-website/#github-pages_1","title":"github pages\u9519\u8bef","text":"<p>\u6700\u5f00\u59cb\u76f4\u63a5\u6210\u529f\u6784\u5efa\u4e86\u9875\u9762\uff0c\u4f46\u662f\u5728\u7b2c\u4e8c\u6b21\u63d0\u4ea4\u4e4b\u540e\uff0c\u9875\u9762\u6784\u5efa\u5931\u8d25\u4e86\uff0c \u53ef\u80fd\u539f\u56e0\u662f 1.\u6ca1\u6709\u7b49\u5f85\u4e00\u4f1a 2.\u4ee3\u7801\u5185\u5bb9\u6709\u9519\u8bef\u3002</p> <p>\u53d1\u73b0\u597d\u50cf\u662fsafri\u7684\u95ee\u9898\uff0c\u81ea\u52a8\u628apages\u540c\u6b65\u5230\u6211\u7684\u57df\u540d\uff0c\u800c\u6211\u7684\u57df\u540d\u88ab\u91cd\u65b0\u63d0\u4ea4\u4ed3\u5e93\u5bfc\u81f4\u53d6\u6d88\uff0c\u7136\u540e\u5c31404\u4e86\u3002</p> <p>\u91cd\u65b0\u63d0\u4ea4\u4ed3\u5e93\u4e4b\u540e\uff0c\u81ea\u5b9a\u4e49\u7684\u57df\u540d\u53d6\u6d88\u4e86\uff0c\u5982\u4f55\u81ea\u52a8\u540c\u6b65\u5462\uff1f</p> <p>\u901a\u8fc7\u8be2\u95eechtagpt\u627e\u5230\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5728Github action\u4e2d\u6dfb\u52a0\u76f8\u5e94\u7684\u4ee3\u7801\uff0c\u8ba9\u6bcf\u6b21workflow\u4e2d\u90fd\u81ea\u5b9a\u4e8e\u5230\u7279\u5b9a\u57df\u540d\u4e0a\uff1a <pre><code>- name: Deploy to Custom Domain\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          personal_token: ${{ secrets.PERSONAL_TOKEN }} #\u8c03\u7528token\n          external_repository: GuangShuaiWang/personal-website\n          publish_dir: ./site\n          cname: wanggsh.cn\n          target_branch: gh-pages\n          commit_message: 'Deploy to custom domain'\n</code></pre></p> <p>\u5176\u4e2d\u7684secrets.PERSONAL_TOKEN\u8981\u5728github\u7684\u4e3b\u9875Settings/Developer settings\u4e0a\u751f\u6210\uff0c\u5e76\u5c06\u751f\u6210\u7684token\u653e\u5728\u5bf9\u5e94\u4ed3\u5e93\u7684setting/secrets and variables\u91cc\uff0c\u540d\u79f0\u4e3aPERSONAL_TOKEN\u3002</p>"},{"location":"Learn/Knowledage_block/","title":"knowledage","text":""},{"location":"Learn/Knowledage_block/#_1","title":"\u77e5\u8bc6\u5757","text":"<p>\u8fd9\u91cc\u5c31\u4e0d\u662f\u90a3\u4e9b\u53ef\u4ee5\u4e00\u53e5\u8bdd\u5c31\u80fd\u8bf4\u6e05\u695a\u7684\u77e5\u8bc6\u4e86\u3002</p>"},{"location":"Learn/Knowledage_block/#vscode","title":"vscode\u65e0\u5bc6\u94fe\u63a5\u670d\u52a1\u5668","text":"<p>\u6574\u4f53\u6d41\u7a0b\u662f\u5f88\u7b80\u7b54\u7684\uff0c\u901a\u8fc7ssh-keygen\u751f\u6210\u516c\u94a5\u548c\u79c1\u94a5\uff0c\u7136\u540e\u5c06\u516c\u94a5\u653e\u5230\u670d\u52a1\u8d77\u7684~/.ssh/authorized_keys\u4e2d\uff0c\u5e76\u672c\u5730\u914d\u7f6e\u597d\u81ea\u5df1\u7684\u79c1\u94a5\u8def\u5f84\uff0c\u4f46\u662f\uff0c\u6211\u8fd9\u91cc\u5c31\u4e00\u76f4\u6709\u95ee\u9898\u3002</p>"},{"location":"Learn/Knowledage_block/#slumr","title":"slumr\u8c03\u5ea6\u7cfb\u7edf","text":"<p>\u4ea4\u4e92\u6a21\u5f0f\u63d0\u4ea4\u4efb\u52a1\uff1a\u4f7f\u7528\u7684\u662fsalloc\u547d\u4ee4\uff0c</p>"},{"location":"Learn/Small_knowledge_point/","title":"knowledage","text":""},{"location":"Learn/Small_knowledge_point/#_1","title":"\u8bb0\u5f55\u4e00\u4e9b\u5e38\u7528\u7684\u77e5\u8bc6\u70b9","text":""},{"location":"Learn/Small_knowledge_point/#_2","title":"\u5c0f\u547d\u4ee4","text":"<ol> <li>\u589e\u52a0\u8fd0\u884c\u6743\u9650 chmod +x file.sh</li> <li>pip\u955c\u50cf\uff1a<ul> <li>\u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66 : https://pypi.mirrors.ustc.edu.cn/simple</li> <li>\u8c46\u74e3\uff1ahttp://pypi.douban.com/simple/</li> <li>\u963f\u91cc\u4e91\uff1ahttp://mirrors.aliyun.com/pypi/simple/</li> <li>\u6e05\u534e\uff1ahttps://pypi.tuna.tsinghua.edu.cn/simple</li> </ul> </li> <li>corn\u53ef\u4ee5\u5b9a\u65f6\u6267\u884c\u811a\u672c\uff0c<code>crontab -e</code>\u6765\u542f\u52a8\uff0c\u6570\u636e\u683c\u5f0f\u4e3a<code>0 0 * * * /path/to/your/script.sh</code>,\u524d\u4e94\u4e2a\u901a\u914d\u7b26\u5206\u522b\u4e3amin\uff0chour\uff0cdays\uff0cmonth\uff0cyear\u3002</li> <li>shell\u811a\u672c\u4e2d\uff0c\u53ef\u4ee5\u7528date\u547d\u4ee4\u6765\u83b7\u5f97\u65f6\u95f4\uff0cdate +%m/%d/%Y \u8f93\u51fa\u7684\u662f04/18/2023\u3002</li> <li>bashrc\u4e2d\u7684\u524d\u7f00\u663e\u793a\u53ef\u4ee5\u6309\u7167\u81ea\u5df1\u7684\u9700\u6c42\u66f4\u6539\uff0c\u4e3b\u8981\u6539PS1\u53c2\u6570\uff0c\u793a\u4f8b\uff1a<code>expert PS1='\\[\\033[42m\\]\\[\\033[1;37m\\][\\d \\t]\\[\\033[0m\\] \\[\\033[1;30m\\]\\u\\[\\033[0m\\]:\\[\\033[1;34m\\]\\w\\[\\033[0m\\]\\$ '</code></li> <li>chatgpt\u7f51\u5740\uff1achatgpt</li> <li>torch.manual_seed(3407) is all you need?</li> <li>\u8868\u683c\u56fe\u7247\u5728\u7ebf\u5236\u4f5c\uff1ahttps://www.canva.cn\u3002 \u76ee\u524d\u597d\u50cf\u6bd4\u4e0d\u597d\u7528</li> <li>axs.flatten() \u62c9\u5e73\u8f74\u7ebf</li> <li>next()\u51fd\u6570\u5728\u5904\u7406\u8fed\u4ee3\u5668\u6216\u53ef\u8fed\u4ee3\u5bf9\u8c61\u65f6\u975e\u5e38\u6709\u7528\u3002\u5b83\u53ef\u4ee5\u8ba9\u6211\u4eec\u9010\u4e2a\u8bbf\u95ee\u5143\u7d20\uff0c\u800c\u4e0d\u9700\u8981\u663e\u5f0f\u5730\u4f7f\u7528for\u5faa\u73af\u6216\u5c06\u6240\u6709\u5143\u7d20\u52a0\u8f7d\u5230\u5185\u5b58\u4e2d\u3002\u5b83\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u5927\u578b\u6570\u636e\u96c6\u6216\u9700\u8981\u6309\u9700\u83b7\u53d6\u6570\u636e\u7684\u60c5\u51b5\u3002</li> <li>iter()\u51fd\u6570\u7684\u5de5\u4f5c\u539f\u7406\u5982\u4e0b\uff1a\u5982\u679citerable\u672c\u8eab\u5df2\u7ecf\u662f\u4e00\u4e2a\u8fed\u4ee3\u5668\u5bf9\u8c61\uff0citer()\u51fd\u6570\u4f1a\u76f4\u63a5\u8fd4\u56de\u5b83\u81ea\u5df1\uff0c\u5426\u5219\uff0c\u5b83\u4f1a\u8c03\u7528\u53ef\u8fed\u4ee3\u5bf9\u8c61\u7684__iter__()\u65b9\u6cd5\uff0c\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u8fed\u4ee3\u5668\u5bf9\u8c61\u3002</li> <li>torch.randint(start,end,size) \u5176\u4e2dsize\u662f\u8f93\u51fa\u7684\u5f20\u91cf\u5f62\u72b6\uff1bsqueeze\u662f\u538b\u7f29\u5411\u91cf\uff0c\u964d\u4f4e\u7eac\u5ea6\uff0c\u76f8\u53cd\u7684\u64cd\u4f5c\u662fupsqueeze\u3002</li> <li>batch_normalization \u662f\u6837\u672c\u4e4b\u95f4\u8fdb\u884c\u6807\u51c6\u5316\uff0clayer_normalization\u662f\u6837\u672c\u81ea\u8eab\u8fdb\u884c\u6807\u51c6\u5316</li> <li>tensor.view()\u53ef\u4ee5\u6539\u53d8\u5f20\u91cf\u5f62\u72b6,\u5176\u4e2d\u7684-1\u8868\u793a\u6709\u8ba1\u7b97\u89c9\u5f97\u7ef4\u5ea6\uff0c.transpose()\u8f6c\u7f6e\uff0c\u53ef\u4ee5\u4ea4\u6362\u7ef4\u5ea6</li> <li>scores.masked_fill(mask == 0, float('-inf')) \u5c06score\u4e2d\u4e3a0\u7684\u6570\u503c\uff0c\u66ff\u6362\u4e3a\u8d1f\u65e0\u7a77\uff0c\u7ecf\u8fc7softmax\u4f1a\u53d8\u4e3a0.</li> <li>Transformer\u8be6\u7ec6\u7684\u68b3\u7406\u6587\u7ae0</li> <li>torch\u4e2d\u53ea\u8981\u662f\u6a21\u578b\u8fd0\u884c\uff0c\u5c31\u4e00\u5b9a\u662f\u8c03\u7528forward\u65b9\u6cd5\uff0c\u8f93\u5165\u8981\u7b26\u5408forward\u7ed3\u6784</li> <li>Dropout\u7684\u539f\u7406\u662f\u5c06\u67d0\u4e9b\u795e\u7ecf\u539f\u5931\u6d3b\uff0c\u5176\u5b9e\u73b0\u65b9\u5f0f\u662f\u5c06\u8f93\u51fa\u7684\u67d0\u4e9b\u4f4d\u7f6e\u7684\u5411\u91cf\u53d8\u62100.</li> <li>Jupyter\u4e2d\u589e\u52a0kernel\uff1a<code>Ipython kernel install --user --name=kernel_name</code>. <code>--user</code>\u8868\u793a\u5f53\u524d\u7528\u6237\uff0c<code>--name</code>\u5236\u5b9a\u5b89\u88c5\u7684\u540d\u79f0\u3002</li> <li><code>jupyter-lab --no-browser --ip \"\" --port 00000</code> \u56fa\u5b9aIP\u548c\u7aef\u53e3\uff0c\u63d0\u524d\u8bbe\u7f6e\u5bc6\u7801\uff08jupyter server password\uff09</li> <li>MPI\u548cOpenMPI\u662f\u7528\u4e8e\u9ad8\u6027\u80fd\u670d\u52a1\u5668\u4e4b\u95f4\u8fdb\u884c\u4fe1\u606f\u4ea4\u4e92\u7684\u3002</li> <li><code>hostname -I</code> \u53ef\u4ee5\u663e\u793a\u5f53\u524d\u670d\u52a1\u5668\u7684IP.</li> <li>salloc\u547d\u4ee4\u4ea4\u4e92\u6a21\u5f0f\u7533\u8bf7\u8d44\u6e90<code>salloc --job-name=test --nodes=1 --gres=gpu:2 --partition=AI4Molecule --quotatype=reserved</code>\uff0c<code>srun --jobid=&lt;jobid&gt; --pty bash</code>\u8fdb\u5165\u7533\u8bf7\u7684\u8d44\u6e90\u3002</li> <li></li> </ol>"},{"location":"Learn/Steam_game/","title":"Steam","text":""},{"location":"Learn/Steam_game/#steam","title":"\u81ea\u52a8\u83b7\u5f97Steam\u6e38\u620f\u5e93\u4e2d\u6e38\u620f\u65f6\u95f4","text":"<p>\u73b0\u5728\u7684\u95ee\u9898\u662f\u4e0d\u77e5\u9053\u5728github action\u4e2d\u5c06\u6587\u4ef6\u5199\u5230\u7279\u5b9a\u7684\u8def\u5f84\u91cc\u662f\u600e\u4e48\u505a\u7684</p> <p>\u627e\u5230\u95ee\u9898\u4e86\uff0cgithub action\u8fd0\u884c\u811a\u672c\u7684\u8def\u5f84\u662f\u5f53\u524d\u4ed3\u5e93\u6839\u76ee\u5f55\uff0c\u4f46\u662f\u6211\u8fd9\u91cc\u7684\u95ee\u9898\u662f\uff0cgit\u4e0d\u4f1a\u4e0a\u4f20\u7a7a\u6587\u4ef6\u5939\uff0c\u5bfc\u81f4\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\uff0c\u51fa\u73b0\u95ee\u9898\u3002\u4fee\u6539\u540e\u5c31\u89e3\u51b3\u4e86\u3002</p> <p>\u6e38\u620f\u5e93\u540c\u6b65\u5b8c\u6210\uff0c\u6559\u7a0b\u4e4b\u540e\u518d\u5199\uff0c\u76ee\u524d\u8fd8\u5b58\u5728\u7684\u95ee\u9898\uff1a</p> <p>\u65f6\u95f4\u8f6c\u5316\u4e3a\u5c0f\u65f6\u5206\u949f\u5f62\u5f0f\u3002\u518d\u6dfb\u52a0\u4e00\u4e9b\u6587\u5b57\u7684\u63cf\u8ff0\u3002</p>"},{"location":"Learn/Steam_game/#_1","title":"\u6559\u7a0b","text":"<p>\u57fa\u672c\u4e0a\u662f\u975e\u5e38\u7b80\u5355\u7684\u722c\u866b\u51fd\u6570\uff0c\u901a\u8fc7\u8c03\u7528request\u7684\u5e93\u6765\u8fdb\u884c\u6570\u636e\u7684\u6293\u53d6\uff0c\u5173\u952e\u7684\u56e0\u7d20\u662f\u6709\u4e24\u4e2a\uff1a1.\u83b7\u5f97setam\u7684API Key\uff0c\u8fd9\u4e2a\u975e\u5e38\u5173\u952e\uff0c\u51b3\u5b9a\u662f\u5426\u80fd\u591f\u8bbf\u95eesteam\u63d0\u4f9b\u7684\u5bf9\u5e94\u7684API\uff0c2.\u60f3\u8981\u67e5\u770b\u7684steam\u7684\u5e93\u7684\u4e2a\u4ebaID\uff0c\u8fd9\u4e2a\u662f\u4e00\u4e2a\u6570\u5b57ID\uff0c\u53ef\u4ee5\u4ece\u5bf9\u5e94\u7528\u6237\u7684\u5546\u5e97\u9875\u9762\u4e2d\uff0c\u5728\u7f51\u5740\u680f\u91cc\u627e\u5230\u3002</p>"},{"location":"Learn/Steam_game/#steamapi-key","title":"\u7533\u8bf7Steam\u7684API key","text":"<p>\u8981\u83b7\u5f97\u81ea\u5df1\u7684Steam Web API\u5bc6\u94a5\uff0c\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u64cd\u4f5c\uff1a\uff08chatgpt\uff09</p> <ol> <li> <p>\u8f6c\u5230 Steam \u5f00\u53d1\u8005\u7f51\u7ad9\uff1ahttps://steamcommunity.com/dev/</p> </li> <li> <p>\u70b9\u51fb\u9875\u9762\u53f3\u4e0a\u89d2\u7684\u201c\u767b\u5f55\u201d\u6309\u94ae\uff0c\u5e76\u4f7f\u7528\u60a8\u7684 Steam \u5e10\u6237\u767b\u5f55\u3002</p> </li> <li> <p>\u70b9\u51fb\u201c\u6211\u7684\u5e10\u6237\u201d\u6309\u94ae\uff0c\u7136\u540e\u5728\u4e0b\u62c9\u83dc\u5355\u4e2d\u9009\u62e9\u201c\u6ce8\u518c\u5e94\u7528\u7a0b\u5e8f\u201d\u3002</p> </li> <li> <p>\u8f93\u5165\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u540d\u79f0\u548c\u63cf\u8ff0\uff0c\u7136\u540e\u9009\u62e9\u201cWeb API\u201d\u4f5c\u4e3a\u5e94\u7528\u7a0b\u5e8f\u7c7b\u578b\u3002</p> </li> <li> <p>\u5728\u5e94\u7528\u7a0b\u5e8f\u8be6\u60c5\u9875\u9762\u4e2d\uff0c\u60a8\u5c06\u627e\u5230\u201cAPI \u5bc6\u94a5\u201d\u4e00\u680f\u3002\u5355\u51fb\u201c\u83b7\u53d6\u65b0\u5bc6\u94a5\u201d\u6309\u94ae\uff0c\u7cfb\u7edf\u4f1a\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u65b0\u7684API\u5bc6\u94a5\u3002</p> </li> <li> <p>\u5c06 API \u5bc6\u94a5\u4fdd\u5b58\u5230\u5b89\u5168\u7684\u4f4d\u7f6e\uff0c\u5e76\u5728\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u4e2d\u4f7f\u7528\u5b83\u6765\u8bbf\u95ee Steam Web API\u3002</p> </li> </ol> <p>\u4f46\u662f\u7531\u4e8eAPI key\u7684\u79c1\u5bc6\u6027\uff0c\u4e0d\u80fd\u660e\u6587\u4fdd\u5b58\u5728\u4ee3\u7801\u4e2d\uff0c\u56e0\u6b64\u5c06\u5176\u5199\u5230\u4e86Github\u4ed3\u5e93\u7684serects\u91cc\uff0c\u5728github action\u4e2d\u8fdb\u884c\u8c03\u7528\u65b9\u5f0f\u5982\u4e0b\uff1a</p> <pre><code> - name: Run script\n        env : \n          API_KEY: ${{ secrets.STEAM_API }}\n        run: |\n          pip install requests\n          pip install pandas\n          pip install tabulate\n          python ./script/steam.py\n</code></pre> <p>python\u811a\u672c\u91cc\uff1a <pre><code>API_KEY = os.environ[\"API_KEY\"]\n</code></pre></p>"},{"location":"Learn/Steam_game/#_2","title":"\u5176\u4ed6\u6b65\u9aa4","text":"<p>\u6211\u6240\u60f3\u8bbf\u95ee\u7684\u662f\u6e38\u620f\u540d\uff0c\u6e38\u620f\u5c01\u9762\u56fe\uff0c\u6e38\u620f\u65f6\u95f4\u548c\u6700\u540e\u6e38\u73a9\u65e5\u671f\uff0c\u6839\u636e\u5177\u4f53\u7684api\u63d0\u4ea4request.get\u8bf7\u6c42\u5373\u53ef\u83b7\u5f97\uff0c\u6700\u7ec8\u5c06\u5176\u81ea\u52a8\u7684\u5199\u5165\u5230markdown\u91cc\u9762\u3002</p>"},{"location":"Learn/Tensorboard%26pytorch/","title":"Tensorboard","text":""},{"location":"Learn/Tensorboard%26pytorch/#tensorboardpytorch","title":"Tensorboard\u7684\u4ecb\u7ecd\u4e0e\u5728pytorch\u4e0b\u7684\u4f7f\u7528","text":""},{"location":"Learn/Tensorboard%26pytorch/#_1","title":"\u7b80\u8981\u4ecb\u7ecd","text":"<p>Tensorboard\u539f\u672c\u662f\u57fa\u4e8eTensorflow\u7684\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u4f46\u4ecepytorch1.2\u5f00\u59cb\uff0c\u5b83\u4e5f\u652f\u6301Tensorboard\u7684\u3002\u5b83\u88ab\u7528\u6765\u7406\u89e3\u3002\u8c03\u8bd5\u548c\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6a21\u578b\u548c\u6570\u636e\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u76f4\u89c2\u7684web\u754c\u9762\uff0c\u53ef\u4ee5\u67e5\u770b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5404\u79cd\u7edf\u8ba1\u4fe1\u606f\uff0c\u56fe\u8868\uff0c\u6458\u8981\u548c\u53ef\u89c6\u5316\u7ed3\u679c\u3002\u5b83\u4e3b\u8981\u6709\u4ee5\u4e0b\u4e3b\u8981\u7684\u529f\u80fd\uff1a 1. \u53ef\u89c6\u5316\u6a21\u578b\u56fe\uff1aTensorboard\u53ef\u4ee5\u53ef\u89c6\u5316\u6a21\u578b\u7684\u8ba1\u7b97\u56fe\uff0c\u7528\u6765\u67e5\u770b\u6a21\u578b\u7684\u5c42\u7ea7\u7ed3\u6784\uff0c\u5f20\u91cf\u6d41\u52a8\u8def\u5f84\u548c\u5404\u4e2a\u8282\u70b9\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002 2. \u76d1\u63a7\u8bad\u7ec3\u6307\u6807\uff1aTensorboard\u53ef\u4ee5\u663e\u793a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5404\u79cd\u6307\u6807\uff0c\u6bd4\u5982\u635f\u5931\u51fd\u6570\uff0c\u51c6\u786e\u7387\uff0c\u5b66\u4e60\u7387\u7b49\uff0c\u53ef\u4ee5\u6839\u636e\u8bad\u7ec3\u7684\u65f6\u95f4\u67e5\u770b\u6307\u6807\u7684\u53d8\u5316\uff0c\u4ee5\u53ca\u5bf9\u4e0d\u540c\u6a21\u578b\u548c\u8d85\u53c2\u6570\u4e4b\u95f4\u8fdb\u884c\u6bd4\u8f83\u3002 3. \u53ef\u89c6\u5316\u5f20\u91cf\u6570\u636e\uff1aTensorboard\u53ef\u4ee5\u663e\u793a\u56fe\u50cf\uff0c\u97f3\u9891\u548c\u6587\u672c\u7c7b\u578b\u7684\u5f20\u91cf\u6570\u636e\uff0c\u8ba9\u4f60\u66f4\u76f4\u89c2\u7684\u4e86\u89e3\u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ed3\u679c\u3002 4. \u663e\u793a\u76f4\u65b9\u56fe\u548c\u5206\u5e03\uff1aTensorBoard \u63d0\u4f9b\u4e86\u76f4\u65b9\u56fe\u548c\u5206\u5e03\u56fe\uff0c\u7528\u4e8e\u67e5\u770b\u5f20\u91cf\u7684\u503c\u5206\u5e03\u60c5\u51b5\u3002\u8fd9\u5bf9\u4e8e\u4e86\u89e3\u6743\u91cd\u548c\u68af\u5ea6\u7684\u5206\u5e03\u3001\u89c2\u5bdf\u5b83\u4eec\u7684\u53d8\u5316\u4ee5\u53ca\u68c0\u6d4b\u6f5c\u5728\u7684\u95ee\u9898\u975e\u5e38\u6709\u5e2e\u52a9\u3002 5. \u53ef\u89c6\u5316\u5d4c\u5165\u5411\u91cf\uff1a\u53ef\u4ee5\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7406\u89e3\u5d4c\u5165\u5411\u91cf\uff0c\u4e86\u89e3\u4ed6\u4eec\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u5173\u7cfb\u3002</p>"},{"location":"Learn/Tensorboard%26pytorch/#_2","title":"\u5b89\u88c5","text":"<p><pre><code>conda install tensorboard\n</code></pre> \u901a\u8fc7conda\u8fdb\u884c\u5b89\u88c5\u662f\u6700\u7701\u5fc3\u7684\u65b9\u5f0f</p>"},{"location":"Learn/Tensorboard%26pytorch/#_3","title":"\u6700\u76f4\u767d\u7684\u4f7f\u7528","text":"<p>pytorch\u4e2d\u4f7f\u7528Tensorboard\u53ea\u9700\u8981\u5bfc\u5165<code>SummaryWriter</code>\u51fd\u6570\u5373\u53ef\uff0c\u5b83\u5c06\u60f3\u8981\u4fdd\u5b58\u7684\u6570\u636e\u4ee5\u7279\u5b9a\u7684\u683c\u5f0f\u5199\u5165\u7279\u5b9a\u7684\u6587\u4ef6\u5939\uff0c\u518d\u901a\u8fc7\u547d\u4ee4\u884c\u547d\u4ee4\u8bfb\u53bb\u8be5\u6587\u4ef6\u5939\u901a\u8fc7web\u663e\u793a\u3002 <pre><code>from torch.utils.tensorboard import SummaryWriter\nwrite = SummaryWriter(\"./log\")\nwriter.add_scalar(tag,scalar_value,global_step)\n</code></pre></p> <pre><code>tensorboard --logdir=./log --port 10003 --host=ip_adress\n</code></pre>"},{"location":"Learn/Tensorboard%26pytorch/#overview","title":"\u5e38\u7528\u7684\u53c2\u6570overview","text":"<p>\u5b98\u65b9\u6587\u6863\u91cc\u5199\u4e86\u6bd4\u8f83\u8be6\u7ec6\u7684\u5185\u5bb9\uff0c\u4ecb\u7ecd\u4e86\u5e38\u7528\u7684\u51fd\u6570</p> <p><code>SummaryWriter</code> \u7c7b\u7684 <code>add_</code> \u65b9\u6cd5\u7528\u4e8e\u5c06\u5404\u79cd\u6458\u8981\u6570\u636e\uff08summaries\uff09\u5199\u5165\u5230 TensorBoard\u3002\u4ee5\u4e0b\u662f <code>add_</code> \u65b9\u6cd5\u4e2d\u5e38\u7528\u7684\u53c2\u6570\u53ca\u5176\u7528\u9014\uff1a</p> <ol> <li> <p><code>add_scalar(tag, scalar_value, global_step=None, walltime=None)</code>    \u7528\u4e8e\u6dfb\u52a0\u6807\u91cf\u6570\u636e\uff08scalar\uff09\u3002\u53ef\u4ee5\u4f7f\u7528\u8be5\u65b9\u6cd5\u8bb0\u5f55\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u635f\u5931\u3001\u51c6\u786e\u7387\u7b49\u6307\u6807\u3002<code>tag</code> \u662f\u6807\u7b7e\u540d\u79f0\uff0c<code>scalar_value</code> \u662f\u8981\u8bb0\u5f55\u7684\u6807\u91cf\u503c\uff0c<code>global_step</code> \u662f\u53ef\u9009\u7684\u5168\u5c40\u6b65\u6570\uff0c\u7528\u4e8e\u6307\u5b9a\u8bb0\u5f55\u7684\u6b65\u9aa4\uff0c<code>walltime</code> \u662f\u53ef\u9009\u7684\u65f6\u95f4\u6233\uff0c\u7528\u4e8e\u6307\u5b9a\u8bb0\u5f55\u7684\u65f6\u95f4\u3002</p> </li> <li> <p><code>add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')</code>    \u7528\u4e8e\u6dfb\u52a0\u56fe\u50cf\u6570\u636e\u3002\u53ef\u4ee5\u4f7f\u7528\u8be5\u65b9\u6cd5\u8bb0\u5f55\u8f93\u5165\u6570\u636e\u3001\u6a21\u578b\u8f93\u51fa\u7b49\u56fe\u50cf\u6570\u636e\u3002<code>tag</code> \u662f\u6807\u7b7e\u540d\u79f0\uff0c<code>img_tensor</code> \u662f\u56fe\u50cf\u5f20\u91cf\uff0c<code>global_step</code> \u548c <code>walltime</code> \u662f\u53ef\u9009\u7684\uff0c\u7528\u4e8e\u6307\u5b9a\u8bb0\u5f55\u7684\u6b65\u9aa4\u548c\u65f6\u95f4\uff0c<code>dataformats</code> \u662f\u53ef\u9009\u7684\uff0c\u7528\u4e8e\u6307\u5b9a\u56fe\u50cf\u7684\u6570\u636e\u683c\u5f0f\u3002</p> </li> <li> <p><code>add_figure(tag, figure, global_step=None, close=True, walltime=None)</code>    \u7528\u4e8e\u6dfb\u52a0\u56fe\u5f62\u6570\u636e\u3002\u53ef\u4ee5\u4f7f\u7528\u8be5\u65b9\u6cd5\u5c06 Matplotlib \u7684\u56fe\u5f62\u5bf9\u8c61\u6dfb\u52a0\u5230 TensorBoard\u3002<code>tag</code> \u662f\u6807\u7b7e\u540d\u79f0\uff0c<code>figure</code> \u662f Matplotlib \u7684\u56fe\u5f62\u5bf9\u8c61\uff0c<code>global_step</code>\u3001<code>close</code> \u548c <code>walltime</code> \u662f\u53ef\u9009\u7684\uff0c\u7528\u4e8e\u6307\u5b9a\u8bb0\u5f55\u7684\u6b65\u9aa4\u3001\u662f\u5426\u5173\u95ed\u56fe\u5f62\u5bf9\u8c61\u4ee5\u53ca\u8bb0\u5f55\u7684\u65f6\u95f4\u3002</p> </li> <li> <p><code>add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None)</code>    \u7528\u4e8e\u6dfb\u52a0\u76f4\u65b9\u56fe\u6570\u636e\u3002\u53ef\u4ee5\u4f7f\u7528\u8be5\u65b9\u6cd5\u8bb0\u5f55\u6743\u91cd\u3001\u68af\u5ea6\u7b49\u6570\u636e\u7684\u5206\u5e03\u60c5\u51b5\u3002<code>tag</code> \u662f\u6807\u7b7e\u540d\u79f0\uff0c<code>values</code> \u662f\u8981\u8bb0\u5f55\u7684\u6570\u503c\u6570\u636e\uff0c<code>global_step</code>\u3001<code>bins</code> \u548c <code>walltime</code> \u662f\u53ef\u9009\u7684\uff0c\u7528\u4e8e\u6307\u5b9a\u8bb0\u5f55\u7684\u6b65\u9aa4\u3001\u76f4\u65b9\u56fe\u7684\u5206\u7bb1\u65b9\u5f0f\u548c\u8bb0\u5f55\u7684\u65f6\u95f4\u3002</p> </li> <li> <p><code>add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None)</code>    \u7528\u4e8e\u6dfb\u52a0\u5d4c\u5165\u5411\u91cf\uff08embedding\uff09\u6570\u636e\u3002\u53ef\u4ee5\u4f7f\u7528\u8be5\u65b9\u6cd5\u53ef\u89c6\u5316\u9ad8\u7ef4\u5d4c\u5165\u5411\u91cf\u7684\u805a\u7c7b\u548c\u76f8\u4f3c\u6027\u3002<code>mat</code> \u662f\u5d4c\u5165\u5411\u91cf\u7684\u6570\u636e\u77e9\u9635\uff0c<code>metadata</code> \u662f\u53ef\u9009\u7684\u5143\u6570\u636e\uff0c<code>label_img</code> \u662f\u53ef\u9009\u7684\u56fe\u50cf\u6570\u636e\uff0c<code>global_step</code> \u548c <code>tag</code> \u662f\u53ef\u9009\u7684\uff0c\u7528\u4e8e\u6307\u5b9a\u8bb0\u5f55\u7684\u6b65\u9aa4\u548c\u6807\u7b7e\u540d\u79f0\uff0c<code>metadata_header</code> \u662f\u53ef\u9009\u7684\uff0c\u7528\u4e8e\u6307\u5b9a\u5143\u6570\u636e\u7684\u6807\u9898\u3002</p> </li> </ol> <p>\u4e3e\u4e24\u4e2a\u4f8b\u5b50</p> <p>\u5bf9\u6a21\u578b\u548c\u6807\u91cf\u8fdb\u884c\u53ef\u89c6\u5316</p>"},{"location":"Learn/Tensorboard%26pytorch/#_4","title":"\u53ef\u89c6\u5316\u8ba1\u7b97\u56fe","text":"<p>\u6784\u5efa\u4e86\u4e00\u4e2a\u6700\u7b80\u5355\u7684\u6a21\u578b\u6765\u663e\u793a\u6a21\u578b\u56fe <pre><code>from torch.utils.tensorboard import SummaryWriter\nimport torch\nimport torch.nn as nn\n\n# \u521b\u5efa\u4e00\u4e2a SummaryWriter \u5bf9\u8c61\uff0c\u6307\u5b9a\u65e5\u5fd7\u4fdd\u5b58\u7684\u76ee\u5f55\nwriter = SummaryWriter('logs')\n\n# \u521b\u5efa\u4e00\u4e2a\u6a21\u578b\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc = nn.Linear(10, 1)\n\n    def forward(self, x):\n        return self.fc(x)\n\nmodel = MyModel()\n\n# \u5c06\u6a21\u578b\u56fe\u5199\u5165 TensorBoard\ndummy_input = torch.randn(1, 10)  # \u521b\u5efa\u4e00\u4e2a\u865a\u62df\u8f93\u5165\nwriter.add_graph(model, dummy_input)\n\n# \u5173\u95ed SummaryWriter\nwriter.close()\n</code></pre></p> <p></p>"},{"location":"Learn/Tensorboard%26pytorch/#_5","title":"\u8bad\u7ec3\u8fc7\u7a0b\u7684\u53ef\u89c6\u5316","text":"<p>\u8fd9\u4e00\u6b65\u5e94\u8be5\u662f\u6700\u5e38\u7528\u7684\uff0c\u53ef\u4ee5\u67e5\u770b\u60f3\u76d1\u63a7\u7684\u6570\u503c\uff0c\u6bd4\u5982\u51c6\u786e\u7387\uff0cAUC\u7b49\u6570\u503c\u7684\u53d8\u5316</p>"},{"location":"Learn/Tensorboard%26pytorch/#referebce","title":"Referebce","text":"<p>csdn_bolg</p> <p>blog2</p> <p>\u5b98\u65b9\u6587\u6863</p> <p>chatgpt</p>"},{"location":"Learn/Transformer_structure/","title":"Transformer","text":"In\u00a0[94]: Copied! <pre>import torch\nimport torch.nn as nn\nimport math\nimport numpy as np\n</pre> import torch import torch.nn as nn import math import numpy as np In\u00a0[95]: Copied! <pre>class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len, dropout=0.0):\n\"\"\"\n        :param d_model: length of vector\n        :param max_len: max sequence length\n        :param dropout: dropout rate\n        \"\"\"\n        super().__init__()\n        \n        self.d_model = d_model\n        self.dropout = nn.Dropout(dropout)\n        self.encoding = torch.zeros(1,max_len, d_model)\n        self.encoding.requires_grad = False\n        \n        pos = torch.arange(0,max_len)\n        pos = pos.float().unsqueeze(dim=1)\n        \n        _2i = torch.arange(0, d_model, step=2).float()\n        self.encoding[:, :, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n        self.encoding[:, :, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n        \n    def forward(self,x):\n        x = x * math.sqrt(self.d_model)\n        x = x + self.encoding.to(x.device)\n        return self.dropout(x)\n</pre> class PositionalEncoding(nn.Module):     def __init__(self, d_model, max_len, dropout=0.0):         \"\"\"         :param d_model: length of vector         :param max_len: max sequence length         :param dropout: dropout rate         \"\"\"         super().__init__()                  self.d_model = d_model         self.dropout = nn.Dropout(dropout)         self.encoding = torch.zeros(1,max_len, d_model)         self.encoding.requires_grad = False                  pos = torch.arange(0,max_len)         pos = pos.float().unsqueeze(dim=1)                  _2i = torch.arange(0, d_model, step=2).float()         self.encoding[:, :, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))         self.encoding[:, :, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))              def forward(self,x):         x = x * math.sqrt(self.d_model)         x = x + self.encoding.to(x.device)         return self.dropout(x) <p>\u8fd9\u4e2a\u6a21\u5757\u5b8c\u6210\u7684\u6a21\u578b\u8f93\u5165\u90e8\u5206\uff0c\u4e3b\u8981\u662f\u5305\u62ec\u7f16\u7801\u5411\u91cf\u548c\u4f4d\u7f6e\u7f16\u7801\u7684\u76f8\u52a0\u3002\u540c\u65f6\u4e5f\u8003\u8651\u4e86dropout\u5c42\u548cbacth_size</p> In\u00a0[96]: Copied! <pre>def Scale_Dot_Produce_attention(query,key,value,mask=None):\n    d_k = query.size(-1)\n    scores = torch.matmul(query,key.transpose(-2,-1)) \\\n                                / math.sqrt(d_k)\n    \n    if mask is not None:\n        scores = scores.masked_fill(mask == 0,\"-inf\")\n    attention = torch.matmul(nn.softmax(dim = -1)(scores),value)\n    return attention\n</pre> def Scale_Dot_Produce_attention(query,key,value,mask=None):     d_k = query.size(-1)     scores = torch.matmul(query,key.transpose(-2,-1)) \\                                 / math.sqrt(d_k)          if mask is not None:         scores = scores.masked_fill(mask == 0,\"-inf\")     attention = torch.matmul(nn.softmax(dim = -1)(scores),value)     return attention In\u00a0[97]: Copied! <pre>class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n'''\n        q,k,v.shape = (b:batch_size,l:seq_len,d_m: dim of input)\n        '''\n        assert d_model % num_heads == 0\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        \n        self.query_linear = nn.Linear(d_model, d_model)\n        self.key_linear = nn.Linear(d_model, d_model)\n        self.value_linear = nn.Linear(d_model, d_model)\n        self.output_linear = nn.Linear(d_model, d_model)\n        \n    def forward(self, query,key,value, mask=None):\n        batch_size = query.size(0)\n        \n        # Linear transformation for query, key, and value\n        query = self.query_linear(query)   # (b,l,d_m) -&gt; (b,l,d_m)\n        key = self.key_linear(key)\n        value = self.value_linear(value)\n        \n        # Splitting heads\n        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2) \n        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n'''\n        .view(): (b,l,d_m) -&gt; (b,l,h,d_m/h)\n        .transpose(1,2): (b,l,h,d_m/h) -&gt; (b,h,l,d_m/h)\n        '''\n        # Attention scores and scaled dot-product attention\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim) #q*transpose(k) (b,h,l,d_m/h) * (b,h,d_m/h,l) = (b,h,l,l)\n       \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf')) # make zero to -inf\n        \n        attention = nn.Softmax(dim=-1)(scores)\n        output = torch.matmul(attention, value) # (b,h,l,l) * (b,h,l,d_m/h) = (b,h,l,d_m/h)\n        \n        # Concatenating heads and linear transformation\n        output = output.transpose(1, 2).view(batch_size, -1, self.d_model) \n'''\n        .transpose(1,2): (b,h,l,d_m/h) -&gt; (b,l,h,d_m/h)\n        .view() : (b,l,h,d_m/h) -&gt; (b,l,d_m)\n        '''\n        output = self.output_linear(output)\n        \n        return output,attention\n</pre> class MultiHeadAttention(nn.Module):     def __init__(self, d_model, num_heads):         super().__init__()         '''         q,k,v.shape = (b:batch_size,l:seq_len,d_m: dim of input)         '''         assert d_model % num_heads == 0                  self.d_model = d_model         self.num_heads = num_heads         self.head_dim = d_model // num_heads                  self.query_linear = nn.Linear(d_model, d_model)         self.key_linear = nn.Linear(d_model, d_model)         self.value_linear = nn.Linear(d_model, d_model)         self.output_linear = nn.Linear(d_model, d_model)              def forward(self, query,key,value, mask=None):         batch_size = query.size(0)                  # Linear transformation for query, key, and value         query = self.query_linear(query)   # (b,l,d_m) -&gt; (b,l,d_m)         key = self.key_linear(key)         value = self.value_linear(value)                  # Splitting heads         query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)          key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)         value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)         '''         .view(): (b,l,d_m) -&gt; (b,l,h,d_m/h)         .transpose(1,2): (b,l,h,d_m/h) -&gt; (b,h,l,d_m/h)         '''         # Attention scores and scaled dot-product attention         scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim) #q*transpose(k) (b,h,l,d_m/h) * (b,h,d_m/h,l) = (b,h,l,l)                 if mask is not None:             scores = scores.masked_fill(mask == 0, float('-inf')) # make zero to -inf                  attention = nn.Softmax(dim=-1)(scores)         output = torch.matmul(attention, value) # (b,h,l,l) * (b,h,l,d_m/h) = (b,h,l,d_m/h)                  # Concatenating heads and linear transformation         output = output.transpose(1, 2).view(batch_size, -1, self.d_model)          '''         .transpose(1,2): (b,h,l,d_m/h) -&gt; (b,l,h,d_m/h)         .view() : (b,l,h,d_m/h) -&gt; (b,l,d_m)         '''         output = self.output_linear(output)                  return output,attention  In\u00a0[98]: Copied! <pre>class EncoderBlock(nn.Module):\n    def __init__(self,d_model,num_heads,dim_FFN,dropout=0.0):\n        super().__init__()\n        self.self_attention = MultiHeadAttention(d_model,num_heads)\n        \n        #Feed forward\n        self.FFW = torch.nn.Sequential(\n            nn.Linear(d_model,dim_FFN),\n            nn.Dropout(dropout),\n            nn.ReLU(),\n            nn.Linear(dim_FFN,d_model))\n        \n        #LayerNorm\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self,x,mask = None):\n        atten_out,atten = self.self_attention(x,x,x,mask)\n        x = x + self.dropout(atten_out)\n        x = self.norm1(x)\n        x = self.FFW(x)\n        x = x + self.dropout(atten_out)\n        x = self.norm2(x)\n        \n        return x\n</pre> class EncoderBlock(nn.Module):     def __init__(self,d_model,num_heads,dim_FFN,dropout=0.0):         super().__init__()         self.self_attention = MultiHeadAttention(d_model,num_heads)                  #Feed forward         self.FFW = torch.nn.Sequential(             nn.Linear(d_model,dim_FFN),             nn.Dropout(dropout),             nn.ReLU(),             nn.Linear(dim_FFN,d_model))                  #LayerNorm         self.norm1 = nn.LayerNorm(d_model)         self.norm2 = nn.LayerNorm(d_model)         self.dropout = nn.Dropout(dropout)              def forward(self,x,mask = None):         atten_out,atten = self.self_attention(x,x,x,mask)         x = x + self.dropout(atten_out)         x = self.norm1(x)         x = self.FFW(x)         x = x + self.dropout(atten_out)         x = self.norm2(x)                  return x In\u00a0[99]: Copied! <pre>class TransformerEncoder(nn.Module):\n    \n    def __init__(self,n_layers,max_len,d_model,num_heads,dim_FNN,dropout):\n        super().__init__()\n        self.input = PositionalEncoding(d_model,max_len)\n        self.layers = nn.ModuleList([EncoderBlock(d_model,num_heads,dim_FNN,dropout) for _ in range(n_layers)])\n    \n    def forward(self,x,mask=None):\n        x = self.input(x)\n        for l in self.layers:\n            x = l(x,mask = mask)\n        return x\n    \n    def get_attention_maps(self,x,mask=None):\n        attention_maps = []\n        for l in self.layers:\n            _,atten_map = l.self_attention(x,x,x,mask)\n            attention_maps.append(atten_map)\n            x = l(x)\n        return attention_maps\n</pre> class TransformerEncoder(nn.Module):          def __init__(self,n_layers,max_len,d_model,num_heads,dim_FNN,dropout):         super().__init__()         self.input = PositionalEncoding(d_model,max_len)         self.layers = nn.ModuleList([EncoderBlock(d_model,num_heads,dim_FNN,dropout) for _ in range(n_layers)])          def forward(self,x,mask=None):         x = self.input(x)         for l in self.layers:             x = l(x,mask = mask)         return x          def get_attention_maps(self,x,mask=None):         attention_maps = []         for l in self.layers:             _,atten_map = l.self_attention(x,x,x,mask)             attention_maps.append(atten_map)             x = l(x)         return attention_maps In\u00a0[100]: Copied! <pre>class DecoderBlock(nn.Module):\n    def __init__(self,d_model,num_heads,dim_FFN,dropout=0.0):\n        super().__init__()\n        self.self_attention = MultiHeadAttention(d_model,num_heads)\n        \n        #Feed forward\n        self.FFW = torch.nn.Sequential(\n            nn.Linear(d_model,dim_FFN),\n            nn.Dropout(dropout),\n            nn.ReLU(),\n            nn.Linear(dim_FFN,d_model))\n        \n        #LayerNorm\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self,x,memory,src_mask = None,tar_mask = None):\n        self_atten_out,_ = self.self_attention(x,x,x,tar_mask)\n        x = x + self.dropout(self_atten_out)\n        x = self.norm1(x)\n        encoder_decoder_atten,_ = self.self_attention(x,memory,memory,src_mask)\n        x = x + self.dropout(encoder_decoder_atten)\n        x = self.norm2(x)\n        x = self.FFW(x)\n        x = x + self.dropout(x)\n        x = self.norm3(x)\n        \n        return x\n</pre> class DecoderBlock(nn.Module):     def __init__(self,d_model,num_heads,dim_FFN,dropout=0.0):         super().__init__()         self.self_attention = MultiHeadAttention(d_model,num_heads)                  #Feed forward         self.FFW = torch.nn.Sequential(             nn.Linear(d_model,dim_FFN),             nn.Dropout(dropout),             nn.ReLU(),             nn.Linear(dim_FFN,d_model))                  #LayerNorm         self.norm1 = nn.LayerNorm(d_model)         self.norm2 = nn.LayerNorm(d_model)         self.norm3 = nn.LayerNorm(d_model)         self.dropout = nn.Dropout(dropout)              def forward(self,x,memory,src_mask = None,tar_mask = None):         self_atten_out,_ = self.self_attention(x,x,x,tar_mask)         x = x + self.dropout(self_atten_out)         x = self.norm1(x)         encoder_decoder_atten,_ = self.self_attention(x,memory,memory,src_mask)         x = x + self.dropout(encoder_decoder_atten)         x = self.norm2(x)         x = self.FFW(x)         x = x + self.dropout(x)         x = self.norm3(x)                  return x In\u00a0[101]: Copied! <pre>class TransformerDecoder(nn.Module):\n    \n    def __init__(self,n_layers,max_len,d_model,num_heads,dim_FNN,dropout):\n        super().__init__()\n        self.input = PositionalEncoding(d_model,max_len)\n        self.layers = nn.ModuleList([DecoderBlock(d_model,num_heads,dim_FNN,dropout) for _ in range(n_layers)])\n    \n    def forward(self,x,memory,src_mask = None,tar_mask = None):\n        x = self.input(x)\n        for layer in self.layers:\n            x = layer(x,memory,src_mask,tar_mask)\n        return x\n</pre> class TransformerDecoder(nn.Module):          def __init__(self,n_layers,max_len,d_model,num_heads,dim_FNN,dropout):         super().__init__()         self.input = PositionalEncoding(d_model,max_len)         self.layers = nn.ModuleList([DecoderBlock(d_model,num_heads,dim_FNN,dropout) for _ in range(n_layers)])          def forward(self,x,memory,src_mask = None,tar_mask = None):         x = self.input(x)         for layer in self.layers:             x = layer(x,memory,src_mask,tar_mask)         return x In\u00a0[102]: Copied! <pre>class Transformer(nn.Module):\n    def __init__(self,encoder,decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n    \n    def forward(self,x,src_mask, tar_mask, tar):\n        x = self.encoder(x,src_mask)\n        return self.decoder(tar,x,src_mask,tar_mask)\n</pre> class Transformer(nn.Module):     def __init__(self,encoder,decoder):         super().__init__()         self.encoder = encoder         self.decoder = decoder          def forward(self,x,src_mask, tar_mask, tar):         x = self.encoder(x,src_mask)         return self.decoder(tar,x,src_mask,tar_mask)              In\u00a0[103]: Copied! <pre>n_layers = 4\nmax_len = 30\nd_model= 512\nnum_heads = 8\ndim_FNN = 128\ndropout = 0.0\n</pre> n_layers = 4 max_len = 30 d_model= 512 num_heads = 8 dim_FNN = 128 dropout = 0.0 In\u00a0[104]: Copied! <pre>encoder = TransformerEncoder(n_layers,max_len,d_model,num_heads,dim_FNN,dropout)\ndecoder = TransformerDecoder(n_layers,max_len,d_model,num_heads,dim_FNN,dropout)\nmodel = Transformer(encoder,decoder)\nmodel\n</pre> encoder = TransformerEncoder(n_layers,max_len,d_model,num_heads,dim_FNN,dropout) decoder = TransformerDecoder(n_layers,max_len,d_model,num_heads,dim_FNN,dropout) model = Transformer(encoder,decoder) model Out[104]: <pre>Transformer(\n  (encoder): TransformerEncoder(\n    (input): PositionalEncoding(\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (layers): ModuleList(\n      (0-3): 4 x EncoderBlock(\n        (self_attention): MultiHeadAttention(\n          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (FFW): Sequential(\n          (0): Linear(in_features=512, out_features=128, bias=True)\n          (1): Dropout(p=0.0, inplace=False)\n          (2): ReLU()\n          (3): Linear(in_features=128, out_features=512, bias=True)\n        )\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n  )\n  (decoder): TransformerDecoder(\n    (input): PositionalEncoding(\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (layers): ModuleList(\n      (0-3): 4 x DecoderBlock(\n        (self_attention): MultiHeadAttention(\n          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (FFW): Sequential(\n          (0): Linear(in_features=512, out_features=128, bias=True)\n          (1): Dropout(p=0.0, inplace=False)\n          (2): ReLU()\n          (3): Linear(in_features=128, out_features=512, bias=True)\n        )\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n  )\n)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Learn/Transformer_structure/#basic-transformer-code","title":"basic Transformer code\u00b6","text":"<p>\u8d44\u6599</p> <p>https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch</p> <p>model figure are form Attention is all you need</p>"},{"location":"Learn/Transformer_structure/#postional-encoding","title":"Postional Encoding\u00b6","text":""},{"location":"Learn/Transformer_structure/#scale-dot-product-attention","title":"Scale Dot-Product attention\u00b6","text":"<p>\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u662f\u9700\u8981\u76f8\u5e94\u7684\u8ba1\u7b97\u65b9\u5f0f\u7684\uff0cTransformer\u4f7f\u7528\u7684\u662f\u70b9\u79ef\u6ce8\u610f\u529b\u673a\u5236\u3002</p>"},{"location":"Learn/Transformer_structure/#multi-head-attention","title":"multi head attention\u00b6","text":"<p>\u8bba\u6587\u7684\u63cf\u8ff0\u662f\u901a\u8fc7\u591a\u4e2aW\uff08\u7ebf\u6027\u5c42\uff09\u6620\u5c04\u5230\u5b50\u7a7a\u95f4\u91cc\uff0c\u4f46\u662f\u5b9e\u9645\u64cd\u4f5c\u65f6\u662f\u901a\u8fc7\u4e00\u4e2a\u6620\u5c04\u7136\u540e\u5207\u5206\uff0c\u5e76\u4e14\u5728\u4ee3\u7801\u64cd\u4f5c\u4e2d\u4e5f\u6ca1\u6709\u5207\u5206\uff0c\u800c\u662f\u76f4\u63a5\u901a\u8fc7\u6539\u53d8\u5f62\u72b6\u5b9e\u73b0\u4e86\u5e76\u884c\u8fd0\u7b97\u3002</p>"},{"location":"Learn/Transformer_structure/#encoder-block","title":"Encoder block\u00b6","text":"<p>\u63a5\u4e0b\u6765\u5b9e\u73b0\u5b83\u7684Encoder\u90e8\u5206\uff0c\u8fd9\u90e8\u5206\u8981\u8003\u8651\u7684\u5c31\u662f\u6b8b\u5dee\u8fde\u63a5\uff0c\u5c42\u6807\u51c6\u5316\u548c\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u3002</p> <p>\u5148\u6b8b\u5dee\u4e86\u8fde\u63a5\uff0c\u7136\u540e\u5728\u5c42\u6807\u51c6\u5316\u3002</p>"},{"location":"Learn/Transformer_structure/#dcoder-block","title":"Dcoder block\u00b6","text":"<p>Docoder\u76f8\u8f83\u4e8eEncoder\u6bd4\u8f83\u9ebb\u70e6\uff0c\u5b58\u5728\u4e00\u4e2amask\u64cd\u4f5c\uff0cmask\u7684\u542b\u4e49\u662f\uff0c\u5728t\u65f6\u523b\u7684\u8f93\u5165\uff0cdecoder\u7684\u8f93\u5165\u662f\u65e0\u6cd5\u770b\u5230t+1\u65f6\u523b\u7684\u5185\u5bb9\u7684\uff0c\u90a3\u4e48\u5728\u5b9e\u9645\u64cd\u4f5c\u7684\u65f6\u5019\uff0c\u662f\u901a\u8fc7\u4e00\u4e2a\u4e09\u89d2\u77e9\u9635\uff08\u53f3\u4e0a\u89d2\u4e3a0\uff09\u5c06\u540e\u9762\u7684\u5185\u5bb9\u53d8\u4e3a0\uff0c\u5728\u4ee3\u7801\u4e2d\u66ff\u6362\u4e3a-inf\u3002\u56e0\u6b64\u597d\u50cf\u8fd9\u4e00\u6b65\u64cd\u4f5c\u4e0d\u662f\u5728\u6a21\u578b\u4ee3\u7801\u4e2d\u6784\u5efa\u7684\uff0c\u800c\u662f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6784\u9020\u7684\u3002</p> <p>\u5728attention\u6a21\u5757\u4e2d\uff0c\u5df2\u7ecf\u5b9e\u73b0\u4e86mask\u7684\u64cd\u4f5c\uff0c\u5373\u5bf9query\uff0ckey\u8ba1\u7b97\u5f97\u5230\u7684\u6743\u91cd\u8fdb\u884cmask:</p> <pre><code>if mask is not None:\n    scores = scores.masked_fill(mask == 0, float('-inf'))\n</code></pre> <p>\u4ed6\u4f7f\u7528\u7684\u4e00\u4e2a\u4e0b\u4e09\u89d2\u77e9\u9635\uff0c\u76f8\u5f53\u4e8e\u8986\u76d6\u5728\u6743\u91cd\u77e9\u9635\u4e0a\uff0c\u5c06\u4e09\u89d2\u77e9\u9635\u4e3a\u96f6\u7684\u5bf9\u5e94\u6743\u91cd\u77e9\u9635\u7684\u4f4d\u7f6e\u66ff\u6362\u4e3a\u8d1f\u65e0\u7a77\uff08\u9700\u8981\u7406\u89e3masked_fill\u7684\u539f\u7406\uff09\u3002</p> <p>\u56e0\u6b64\u5728\u6784\u5efaDecoder\u65f6\u76f4\u63a5\u5806\u53e0\u5df2\u7ecf\u5efa\u597d\u7684\u6a21\u5757\u5373\u53ef\u3002</p> <p>\u5728Decoder\u4e2d\u7684decoder-encoder attntion\u7684q,k,v\uff1a\u5176\u4e2dquery\u6765\u81ea\u4e8eDecoder\u7684self atention\uff0ck,v\u6765\u81ea\u4e8eencoder\u7684\u8f93\u51fa</p>"},{"location":"Learn/Transformer_structure/#transformer","title":"\u6700\u7ec8\u7684Transformer\u00b6","text":""},{"location":"Learn/Transformer_structure/#summary","title":"Summary\u00b6","text":"<p>\u57fa\u672c\u4e0a\u5df2\u7ecf\u5b8c\u5168\u91cc\u6e05\u695aTransformer\u7684\u6a21\u578b\u6846\u67b6\u4e86\uff0c\u4e0a\u9762\u7684\u4ee3\u7801\u76ee\u524d\u770b\u8d77\u6765\u6ca1\u6709\u95ee\u9898\uff0c\u4e0d\u8fc7\u8fd8\u9700\u8981\u5b9e\u9645\u6d4b\u8bd5\u624d\u80fd\u77e5\u9053\u3002\u8fd8\u6709\u5c31\u662f\u5bf9\u4e8e\u8f93\u5165\u7684\u5d4c\u5165\u5e76\u6ca1\u6709\u5b9a\u4e49\uff0c\u56e0\u4e3a\u76f4\u63a5\u4f7f\u7528<code>nn.embedding()</code>\u5c42\u53ef\u884c\u662f\u53ef\u884c\uff0c\u4e0d\u8fc7\u76f4\u89c2\u89c9\u5f97\u4e5f\u9700\u8981\u81ea\u5df1\u6765\u5b9a\u4e49\u7f16\u7801\u65b9\u5f0f\u3002\u5176\u6b21\uff0c\u539f\u59cb\u7684\u6a21\u578b\u6846\u67b6\u662f\u7528\u6765\u8fdb\u884c\u7ffb\u8bd1\u7684\u4f1a\u6709soruce_sequence\u548ctarget_sequence\u3002\u4f46\u662f\u6211\u5e76\u4e0d\u505a\u673a\u5668\u7ffb\u8bd1\uff0c\u73b0\u6709\u7684\u6846\u67b6\u5e76\u4e0d\u9002\u5408\u6211\u89e3\u51b3\u95ee\u9898\uff0c\u9700\u8981\u6839\u636e\u81ea\u5df1\u7684\u5185\u5bb9\u6765\u8fdb\u884c\u4fee\u6539\u3002</p>"},{"location":"Learn/git/","title":"Git","text":"<p>\u4e00\u822c\u5e38\u7528\u7684\u662fGithub desktop\uff0c\u6240\u4ee5\u5bf9Git\u7684\u547d\u4ee4\u5e76\u4e0d\u662f\u5f88\u719f\u6089\uff0c\u5728\u8fd9\u91cc\u8bb0\u5f55\u4e0b\u76f8\u5173\u77e5\u8bc6\uff0c\u7528\u5230\u7684\u65f6\u5019\u53ef\u4ee5\u76f4\u63a5\u590d\u5236\uff1a</p> <p></p>"},{"location":"Learn/git/#git","title":"Git\u7684\u5e94\u7528\u76ee\u7684\u548c\u7406\u8bba","text":"<p>\u76ee\u7684\u662f\u4e3a\u4e86\u66f4\u597d\u7684\u7248\u672c\u63a7\u5236\u3002GIt\u672c\u5730\u6709\u4e09\u4e2a\u5de5\u4f5c\u533a\u57df\uff1a\u5de5\u4f5c\u76ee\u5f55\uff08Working Directory\uff09\uff0c\u6682\u5b58\u533a\uff08Stage/Index\uff09\uff0c\u8d44\u6e90\u5e93\uff08Repository\uff09\u3002\u5916\u52a0\u4e0a\u8fdc\u7a0b\u7684git\u4ed3\u5e93\uff08Remote Directory\uff09\u3002\u4ed6\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u5982\u4e0b\u6240\u793a\uff1a</p> <p></p>"},{"location":"Learn/git/#git-status","title":"git status","text":"<p>\u7528\u4e8e\u67e5\u770b\u6587\u4ef6\u73b0\u5b58\u7684\u72b6\u6001\u3002</p> <p>Untracked\uff1a\u672a\u8ffd\u8e2a\uff0c\u5df2\u7ecf\u65b0\u521b\u5efa\u7684\u6587\u4ef6\uff0c\u4f46\u6ca1\u6709\u6dfb\u52a0\u5230git\u5e93\u4e2d\uff0c\u4e0d\u63a5\u53d7\u7248\u672c\u63a7\u5236\uff0c\u901a\u8fc7git add\u6dfb\u52a0\u4e3aStaged\u72b6\u6001</p> <p>Unmodify\uff1a\u672a\u4fee\u6539\u3002\u6587\u4ef6\u5df2\u5728git\u5e93\u4e2d\uff0c\u6ca1\u6709\u4fee\u6539\uff0c\u8fd9\u4e2a\u6709\u4e24\u79cd\u72b6\u6001\u53ef\u4ee5\u6539\u53d8\uff0c\u4fee\u6539\u540e\u4f1a\u53d8\u4e3aModified\u72b6\u6001\uff1b\u4f7f\u7528git rm\u547d\u4ee4\u4f1a\u4f7f\u5176\u53d8\u4e3a Untracted\u72b6\u6001\u3002</p> <p>Modified\uff1a\u6587\u4ef6\u5df2\u4fee\u6539\u3002\u901a\u8fc7git add\u547d\u4ee4\u4f7f\u5176\u8fdb\u5165\u6682\u5b58\u72b6\u6001\uff1b\u6216\u8005\u901a\u8fc7git checkout\u53d8\u4e3aunmodify\u72b6\u6001\u3002</p> <p>Staged\uff1a\u6682\u5b58\u72b6\u6001\u3002git commit\u53ef\u4ee5\u4f7f\u6587\u4ef6\u540c\u6b65\u5230\u5e93\u4e2d\uff0c\u53d8\u4e3aunmodify\u7684\u72b6\u6001\uff1b\u901a\u8fc7git reset HEAD filename\u53ef\u4ee5\u53d6\u6d88\u6682\u5b58\uff0c\u53d8\u4e3amodified\u72b6\u6001\u3002</p>"},{"location":"Learn/git/#_1","title":"\u5ffd\u7565\u6587\u4ef6","text":"<p>.gitignore\u6587\u4ef6\u53ef\u4ee5\u4f7f\u7528\u901a\u914d\u7b26\u6765\u5ffd\u7565\u4e00\u4e9b\u6587\u4ef6</p>"},{"location":"Learn/git/#git_1","title":"Git\u5e38\u7528\u547d\u4ee4","text":"<pre><code>git config -l #\u67e5\u770b\u73af\u5883\u914d\u7f6e\ngit status #\u67e5\u770b\u6587\u4ef6\u73b0\u5b58\u7684\u72b6\u6001\n\n#\u63a8\u9001\u6d41\u7a0b(\u9879\u76ee\u63a8\u9001\u5230\u8fdc\u7a0b\u4ed3\u5e93\uff09\n\ngit add .\ngit commit -m \"comment\"\ngit push\n\n#\u83b7\u5f97\u4ed3\u5e93\ngit pull #\u521b\u5efa\u4ed3\u5e93\uff0c\u4e24\u79cd\u65b9\u6cd5\ngit init #\uff08\u521d\u59cb\u5316\u9879\u76ee\uff09\ngit clone [url] #\u514b\u9686\u5230\u8fdc\u7a0b\u4ed3\u5e93\n</code></pre>"},{"location":"Learn/git/#github-push","title":"Github push","text":"<p>Github\u5220\u9664\u6389\u4e86\u901a\u8fc7\u8d26\u53f7\u5bc6\u7801\u7684\u65b9\u5f0f\u8fdb\u884cpush\uff0c\u8f6c\u800c\u4f7f\u7528token\u7684\u65b9\u5f0f\uff0c\u8fd9\u91cc\u9700\u8981\u751f\u6210token\uff0c\u5e76\u4e14\u5728\u8f93\u5165\u5bc6\u7801\u65f6\uff0c\u9009\u62e9\u901a\u8fc7token\u8fdb\u884c\u767b\u9646\u3002</p>"},{"location":"Learn/git/#pushchatgpt","title":"\u5982\u4f55\u4e0d\u518d\u6bcf\u6b21push\u65f6\u90fd\u91cd\u65b0\u8f93\u5165\u5e10\u53f7\u5bc6\u7801\u5462\uff08Chatgpt\uff09\uff1f","text":"<p>\u8fd9\u91cc\u6709\u4e24\u79cd\u65b9\u6cd5\uff1a</p> <ol> <li>\u901a\u8fc7<code>git config --global credential.helper store</code>\u547d\u4ee4\uff0c\u8be5\u547d\u4ee4\u5c06 Git \u51ed\u636e\u5b58\u50a8\u5728 ~/.git-credentials \u6587\u4ef6\u4e2d,\u4f46\u7531\u4e8e\u65f6\u660e\u6587\u4fdd\u5b58\u5f88\u4e0d\u5b89\u5168\u3002</li> <li>\u8fd9\u79cd\u65b9\u5f0f\u5c31\u5f88\u9ebb\u70e6\u4e86\uff0c\u4e0d\u8fc7\u5b89\u5168\uff1a<ul> <li>\u5728\u590d\u5236\u4ed3\u5e93\u7684\u65f6\u5019\uff0c\u4f7f\u7528 SSH URL \u6765\u514b\u9686\u6216\u8bbf\u95ee\u4ee3\u7801\u5e93\uff0c\u800c\u4e0d\u662f\u4f7f\u7528 HTTPS URL\u3002\u7136\u540e\uff0c\u5728\u60a8\u7684\u672c\u5730\u4ee3\u7801\u5e93\u4e2d\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u8bbe\u7f6e SSH URL\uff1a\u201cgit remote set-url origin git@github.com:username/repository.git\u201d</li> <li>\u751f\u6210ssh\u5bc6\u94a5\uff1a<code>ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"</code>,\u90fd\u9009\u62e9\u9ed8\u8ba4\u5373\u53ef\u3002</li> <li>\u6dfb\u52a0\u516c\u94a5\u5230 Github\uff0c\u5c06\u516c\u94a5\u6dfb\u52a0\u5230\u60a8\u7684 Github \u5e10\u6237\u4e2d\u3002\u9996\u5148\uff0c\u590d\u5236\u516c\u94a5\u6587\u4ef6\u4e2d\u7684\u5185\u5bb9\uff1a<code>cat ~/.ssh/id_rsa.pub</code>\u7136\u540e\uff0c\u767b\u5f55\u5230\u60a8\u7684 Github \u5e10\u6237\uff0c\u5728\u53f3\u4e0a\u89d2\u7684\u5934\u50cf\u4e0b\u62c9\u83dc\u5355\u4e2d\u9009\u62e9\u300cSettings\u300d\u3002\u5728\u5de6\u4fa7\u83dc\u5355\u4e2d\u9009\u62e9\u300cSSH and GPG keys\u300d\uff0c\u7136\u540e\u5355\u51fb\u300cNew SSH key\u300d\u3002\u5728\u300cTitle\u300d\u5b57\u6bb5\u4e2d\u8f93\u5165\u4e00\u4e2a\u63cf\u8ff0\uff0c\u7136\u540e\u5c06\u516c\u94a5\u7c98\u8d34\u5230\u300cKey\u300d\u5b57\u6bb5\u4e2d\u3002\u5355\u51fb\u300cAdd SSH key\u300d\u4fdd\u5b58\u3002</li> <li>\u4fee\u6539\u672c\u5730 Git \u914d\u7f6e\uff0c\u6253\u5f00\u7ec8\u7aef\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u914d\u7f6e Git\uff1a<code>git config --global user.email \"your_email@example.com\"</code>,<code>git config --global user.name \"Your Name\"</code>\u5176\u4e2d\uff0cyour_email@example.com \u662f\u60a8\u7684\u7535\u5b50\u90ae\u4ef6\u5730\u5740\uff0cYour Name \u662f\u60a8\u7684\u7528\u6237\u540d\u3002</li> <li>\u6d4b\u8bd5\u8fde\u63a5 \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u901a\u8fc7 SSH \u8fde\u63a5\u5230 Github\uff1a<code>ssh -T git@github.com</code>\u5982\u679c\u8fde\u63a5\u6210\u529f\uff0c\u60a8\u5e94\u8be5\u4f1a\u6536\u5230\u4ee5\u4e0b\u6d88\u606f\uff1a<code>Hi username! You've successfully authenticated, but GitHub does not provide shell access.</code>\u73b0\u5728\uff0c\u60a8\u5e94\u8be5\u53ef\u4ee5\u5728\u4e0d\u8f93\u5165\u8d26\u53f7\u548c\u5bc6\u7801\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c Github \u64cd\u4f5c\u4e86\u3002</li> </ul> </li> </ol>"},{"location":"Learn/learn_homepage/","title":"Overview","text":"<p>\u8be5\u9875\u9762\u4e0b\u5305\u542b\u4e86\u6700\u8fd1\u5b66\u4e60\u548c\u5df2\u7ecf\u5b66\u4e60\u7684\u5185\u5bb9\u3002</p>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/","title":"Chapter 00 Pytorch \u57fa\u7840","text":"<p>\u524d\u8a00\uff1a\u76ee\u524d\u6211\u4e5f\u4e0d\u662f\u521d\u5b66\u8005\u4e86\uff0c\u5bf9\u4e8e\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u7684\u77e5\u8bc6\u5df2\u7ecf\u5efa\u7acb\u4e86\u76f8\u5e94\u7684\u4e86\u89e3\uff0c\u4f46\u662f\u8bf4\u5b9e\u8bdd\u662f\u6ca1\u6709\u5f62\u6210\u77e5\u8bc6\u6846\u67b6\u7684\uff0c\u56e0\u6b64\u8fd8\u662f\u9700\u8981\u518d\u770b\u7684\u3002</p> <p>\u5176\u6b21\u6211\u6700\u5f00\u59cb\u7684\u9879\u76ee\u662f\u57fa\u4e8eTensorflow\u7684\uff0c\u6211\u5bf9\u4ed6\u7684\u611f\u89c9\u662f\u6a21\u5757\u6027\u597d\uff0c\u4f46\u5b9a\u5236\u6027\u4e0d\u591f\u3002\u901a\u8fc7\u5b83\u6765\u4f5c\u4e3a\u65b0\u624b\u5b66\u4e60\u7684\u8bdd\u5e76\u4e0d\u662f\u975e\u5e38\u6709\u5229\u4e8e\u77e5\u8bc6\u7684\u4e86\u89e3\u3002\u800cpytorch\u6211\u89c9\u5f97\u66f4\u52a0\u7684\u76f4\u89c2\uff0c\u5f00\u653e\u3002</p> <p>pytorch\u7684\u76f8\u5173\u5185\u5bb9\u6211\u5df2\u7ecf\u4e86\u89e3\u8fc7\u4e86\uff0c\u4f46\u662f\u65f6\u95f4\u95f4\u9694\u4e00\u4e2a\u6708\uff0c\u53c8\u653e\u4e0b\u4e86\uff0c\u6240\u4ee5\u8fd9\u91cc\u91cd\u65b0\u5728\u770b\u4e00\u4e0b\u3002</p> In\u00a0[1]: Copied! <pre>import torch\ntorch.__version__\n</pre> import torch torch.__version__ Out[1]: <pre>'2.0.0'</pre> In\u00a0[2]: Copied! <pre>#Scalar\nscalar = torch.tensor(5)\nscalar,scalar.ndim,scalar.item()\n</pre> #Scalar scalar = torch.tensor(5) scalar,scalar.ndim,scalar.item() Out[2]: <pre>(tensor(5), 0, 5)</pre> <p>\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u4e3a5\u7684\u6807\u91cf\uff0c\u901a\u8fc7ndim\u5c5e\u6027\uff08attribute\uff09\u67e5\u770b\u4e86\u5b83\u7684\u7ef4\u5ea6\u662f0\u7ef4\uff0c\u901a\u8fc7item\uff08\uff09\u65b9\u6cd5\uff08method\uff09\u67e5\u770b\u4e86\u5305\u542b\u7684\u5143\u7d20\u3002</p> In\u00a0[3]: Copied! <pre>#vector\nvector = torch.tensor([5,5,5])\nvector,vector.ndim,vector.shape\n</pre> #vector vector = torch.tensor([5,5,5]) vector,vector.ndim,vector.shape Out[3]: <pre>(tensor([5, 5, 5]), 1, torch.Size([3]))</pre> <p>\u5728\u6211\u4eec\u6784\u5efa\u5b8c\u6210\u5411\u91cf\u4e4b\u540e\uff0c\u53ef\u4ee5\u901a\u8fc7ndim\u53d1\u73b0\u5b83\u7684\u7ef4\u5ea6\u662f1\u7ef4\u7684\uff0c\u901a\u8fc7shape\u53c2\u6570\u53ef\u4ee5\u53d1\u73b0\u5176\u4e2d\u5305\u542b\u591a\u5c11\u4e2a\u53c2\u6570</p> In\u00a0[4]: Copied! <pre>## matrix\nmatrix = torch.tensor([[1,2],[3,4]])\nmatrix,matrix.ndim,matrix.shape\n#\u8fd9\u91cc\u7ed3\u679c\u662f2\u7ef4\u7684\uff0cshape\u5c55\u793a\u51fa\u662f2\u7ef4\uff0c\u6bcf\u4e2a\u7ef4\u5ea6\u53c8\u4e24\u4e2a\n</pre> ## matrix matrix = torch.tensor([[1,2],[3,4]]) matrix,matrix.ndim,matrix.shape #\u8fd9\u91cc\u7ed3\u679c\u662f2\u7ef4\u7684\uff0cshape\u5c55\u793a\u51fa\u662f2\u7ef4\uff0c\u6bcf\u4e2a\u7ef4\u5ea6\u53c8\u4e24\u4e2a Out[4]: <pre>(tensor([[1, 2],\n         [3, 4]]),\n 2,\n torch.Size([2, 2]))</pre> In\u00a0[5]: Copied! <pre># tensor\nTensor = torch.tensor([[[1,2],[3,4]]])\nTensor,Tensor.ndim,Tensor.shape\n</pre> # tensor Tensor = torch.tensor([[[1,2],[3,4]]]) Tensor,Tensor.ndim,Tensor.shape Out[5]: <pre>(tensor([[[1, 2],\n          [3, 4]]]),\n 3,\n torch.Size([1, 2, 2]))</pre> <p>\u901a\u8fc7\u4e0a\u9762\u7684\u4f8b\u5b50\uff0c\u5c31\u53ef\u4ee5\u5bf9Tensor\u6709\u4e00\u5b9a\u7684\u7406\u89e3\u3002\u800c\u5728\u5b9e\u9645\u7684\u5e94\u7528\u4e2d\uff0c\u6211\u4eec\u5728\u6781\u5c11\u7684\u60c5\u51b5\u4e0b\u624d\u9700\u8981\u624b\u52a8\u6784\u5efa\u5f20\u91cf\u3002</p> <p>\u76f8\u53cd\u7684\u662f\uff0c\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u6211\u4eec\u9700\u8981\u7ecf\u5e38\u968f\u673a\u6784\u5efa\u8db3\u591f\u5927\u7684\u5f20\u91cf\uff0c\u7136\u540e\u5728\u540e\u7eed\u7684\u8bad\u7ec3\u4e2d\u4e0d\u65ad\u66f4\u65b0\u548c\u4f18\u5316\u8fd9\u4e2a\u5f20\u91cf\uff0c\u4f7f\u6a21\u578b\u8fbe\u5230\u66f4\u597d\u7684\u6548\u679c\uff0c\u8be6\u7ec6\u7684\u5185\u5bb9\u540e\u9762\u4f1a\u6d89\u53ca\u3002</p> In\u00a0[6]: Copied! <pre>## random tensor\nrandom_tensor = torch.rand(size = (3,4))\nrandom_tensor,random_tensor.shape\n</pre> ## random tensor random_tensor = torch.rand(size = (3,4)) random_tensor,random_tensor.shape Out[6]: <pre>(tensor([[0.8368, 0.3126, 0.4129, 0.6687],\n         [0.3175, 0.1760, 0.7171, 0.5520],\n         [0.7791, 0.1630, 0.0240, 0.7963]]),\n torch.Size([3, 4]))</pre> <p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7<code>torch.rand</code>\u6765\u6784\u5efa\u968f\u673a\u5411\u91cf\uff0c\u5176\u4e2dsize\u53c2\u6570\u7528\u6765\u5236\u5b9a\u5f20\u91cf\u7684\u5f62\u72b6\u3002</p> <p>\u9664\u4e86\u968f\u673a\u5411\u91cf\u5916\uff0c\u6211\u4eec\u6709\u65f6\u9700\u8981\u6784\u5efa\u5168\u4e3a0\u62161\u7684\u5f20\u91cf\uff0ctorch\u4e5f\u63d0\u4f9b\u4e86\u7279\u5b9a\u7684\u51fd\u6570\u6765\u6784\u5efa,\u4e8e\u968f\u673a\u5f20\u91cf\u7684\u6784\u5efa\u903b\u8f91\u662f\u76f8\u540c\u7684\u3002</p> In\u00a0[7]: Copied! <pre># zero&amp;one\nzeros = torch.zeros(size = (3,4))\nones = torch.ones(size = (3,4))\nzeros,ones\n</pre> # zero&amp;one zeros = torch.zeros(size = (3,4)) ones = torch.ones(size = (3,4)) zeros,ones Out[7]: <pre>(tensor([[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]),\n tensor([[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]))</pre> <p>\u6211\u4eec\u8fd8\u53ef\u4ee5\u901a\u8fc7\u7279\u5b9a\u8303\u56f4\u548c\u6b65\u957f\u6765\u6784\u5efa\u4e00\u4e2a\u5f20\u91cf\uff0c\u9700\u8981\u4f7f\u7528<code>torch.arange</code>\u51fd\u6570\u53ef\u4ee5\u5b9e\u73b0\uff0c\u9700\u8981\u5236\u5b9a\u5f00\u59cb\uff08start\uff09\uff0c\u7ed3\u675f\uff08end\uff09\u548c\u6b65\u957f\uff08step\uff09</p> In\u00a0[8]: Copied! <pre>range_tensor = torch.arange(start=10,end = 100,step = 10)\nrange_tensor\n</pre> range_tensor = torch.arange(start=10,end = 100,step = 10) range_tensor Out[8]: <pre>tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])</pre> <p>\u8fd8\u6709\u4e00\u79cd\u60c5\u51b5\u662f\u6211\u4eec\u9700\u8981\u6839\u636e\u5df2\u6709\u5411\u91cf\u6765\u6784\u5efa\u5f62\u72b6\u4e00\u6837\u7684\u65b0\u7684\u5411\u91cf\uff0c\u8fd9\u91cc\u53ef\u4ee5\u4f7f\u7528<code>torch.zeros_like(input)</code>\u548c<code>torch.ones_like(input)</code>\u6765\u6784\u5efa\uff0cinput\u662f\u5df2\u6709\u7684\u5411\u91cf</p> In\u00a0[9]: Copied! <pre>torch.zeros_like(range_tensor)\n</pre> torch.zeros_like(range_tensor) Out[9]: <pre>tensor([0, 0, 0, 0, 0, 0, 0, 0, 0])</pre> <p>pytorch\u4e2dtensor\u5177\u6709\u975e\u5e38\u591a\u7684\u6570\u636e\u7c7b\u578b\uff0c\u4e0d\u540c\u7684\u6570\u636e\u7c7b\u578b\u5bf9\u540e\u7eed\u6a21\u578b\u7684\u8ba1\u7b97\u6709\u7740\u5f88\u5927\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u91cd\u8981\u7684\u4e00\u4e2a\u662f\u6570\u636e\u8d8a\u7cbe\u786e\uff0c\u8ba1\u7b97\u91cf\u5c31\u8d8a\u5927\u3002\u5728\u6784\u5efatensor\u7684\u65f6\u5019\u8fd8\u6709\u51e0\u4e2a\u53c2\u6570\u53ef\u4ee5\u8003\u8651\uff0cdtype\u4e3a\u6570\u636e\u7c7b\u578b\uff0cdevice\u4e3a\u5b58\u50a8\u4f4d\u7f6e\uff0craquires_grad\u8ddf\u540e\u9762\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d\u65f6\u76f8\u5173\uff0c\u9ed8\u8ba4\u7684dtype\u662f float32\u3002\u6211\u4eec\u53ef\u67e5\u770b\u4e4b\u524d\u6784\u5efa\u7684\u968f\u673a\u5f20\u91cf\u76f8\u5173\u9ed8\u8ba4\u4fe1\u606f\uff1a</p> In\u00a0[10]: Copied! <pre>random_tensor.dtype,random_tensor.device,random_tensor.requires_grad\n</pre> random_tensor.dtype,random_tensor.device,random_tensor.requires_grad Out[10]: <pre>(torch.float32, device(type='cpu'), False)</pre> <p>\u7efc\u5408\u4e0a\u9762\u7684\u5185\u5bb9\uff0c\u6211\u4eec\u5728\u5f97\u5230\u4e00\u4e2atensor\u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7shape,dtype,device,ndim\u7b49\u5c5e\u6027\u67e5\u770b\u8fd9\u4e2a\u5f20\u91cf\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u8ba9\u6211\u4eec\u53ef\u4ee5\u5bf9\u8f93\u5165\u7684\u6570\u636e\u6709\u4e00\u4e2a\u57fa\u672c\u7684\u7406\u89e3\u3002</p> In\u00a0[11]: Copied! <pre>## \u57fa\u672c\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c+\uff0c-\uff0c*\ntensor = torch.tensor([1,2,3])\n#1. \u4e0d\u4f7f\u7528\u5185\u7f6e\u51fd\u6570\ntensor1 = tensor + 10\ntensor2 = tensor * 10\n#2. \u4f7f\u7528\u5185\u7f6e\u51fd\u6570\ntensor3 = torch.add(tensor,10) # \u7b49\u4ef7\u4e8etensor.add(10)\ntensor4 = torch.mul(tensor,10) # \u7b49\u4ef7\u4e8etensor.mal(10)\n\ntensor1,tensor2,tensor3,tensor4\n</pre> ## \u57fa\u672c\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c+\uff0c-\uff0c* tensor = torch.tensor([1,2,3]) #1. \u4e0d\u4f7f\u7528\u5185\u7f6e\u51fd\u6570 tensor1 = tensor + 10 tensor2 = tensor * 10 #2. \u4f7f\u7528\u5185\u7f6e\u51fd\u6570 tensor3 = torch.add(tensor,10) # \u7b49\u4ef7\u4e8etensor.add(10) tensor4 = torch.mul(tensor,10) # \u7b49\u4ef7\u4e8etensor.mal(10)  tensor1,tensor2,tensor3,tensor4 Out[11]: <pre>(tensor([11, 12, 13]),\n tensor([10, 20, 30]),\n tensor([11, 12, 13]),\n tensor([10, 20, 30]))</pre> <p>\u4ece\u4e0a\u9762\u7684\u8ba1\u7b97\u4e2d\u8fd8\u6709\u4e24\u4e2a\u70b9\uff0c1.\u5982\u679c\u4e0d\u91cd\u65b0\u8d4b\u503c\u7684\u8bdd\uff0c\u539f\u503c\u5e76\u4e0d\u4f1a\u4fee\u6539\u30022.\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u4e86\u5e7f\u64ad\u673a\u5236\uff08broadcast\uff09</p> In\u00a0[12]: Copied! <pre># \u5143\u7d20\u76f8\u4e58\u548c\u77e9\u9635\u76f8\u4e58\n# \u8fd9\u91cc\u7684\u77e9\u9635\u76f8\u4e58\u662f\u6d89\u53ca\u5230\u7ebf\u6027\u4ee3\u6570\u4e2d\u7684\u77e5\u8bc6\ntensor5 = tensor.mul(tensor)\ntensor6 = tensor.matmul(tensor)\ntensor5,tensor6\n</pre> # \u5143\u7d20\u76f8\u4e58\u548c\u77e9\u9635\u76f8\u4e58 # \u8fd9\u91cc\u7684\u77e9\u9635\u76f8\u4e58\u662f\u6d89\u53ca\u5230\u7ebf\u6027\u4ee3\u6570\u4e2d\u7684\u77e5\u8bc6 tensor5 = tensor.mul(tensor) tensor6 = tensor.matmul(tensor) tensor5,tensor6 Out[12]: <pre>(tensor([1, 4, 9]), tensor(14))</pre> <p>\u5728\u8fdb\u884c\u77e9\u9635\u4e58\u6cd5\u7684\u65f6\u5019\uff0c\u6700\u5bb9\u6613\u51fa\u9519\u7684\u70b9\u662f\u4e24\u4e2a\u77e9\u9635\u7684\u5f62\u72b6\u4e0d\u80fd\u76f8\u7b26\u5408\uff0c\u4e24\u4e2a\u77e9\u9635\u8981\u6ee1\u8db3 x * m,m * y\u7684\u5f62\u72b6\uff0c\u83b7\u5f97\u7684\u662fx * y\u7684\u77e9\u9635\u3002tensor\u4e2d\u63d0\u4f9b\u4e2a\u8f6c\u7f6e\u7684\u51fd\u6570\uff0c<code>tensor.T</code>\u548c<code>torch.transpose</code>\uff0c\u53ef\u7528\u6765\u5e94\u5bf9\u8fd9\u79cd\u60c5\u51b5\u3002</p> In\u00a0[13]: Copied! <pre>tensor_A = torch.tensor([[1, 2],\n                         [3, 4],\n                         [5, 6]], dtype=torch.float32)\n\ntensor_B = torch.tensor([[7, 10],\n                         [8, 11], \n                         [9, 12]], dtype=torch.float32)\nprint(tensor_B.T)\noutput = torch.matmul(tensor_A, tensor_B.T)\nprint(output) \ntensor_A.shape,tensor_B.shape,tensor_B.T.shape,output.shape\n</pre> tensor_A = torch.tensor([[1, 2],                          [3, 4],                          [5, 6]], dtype=torch.float32)  tensor_B = torch.tensor([[7, 10],                          [8, 11],                           [9, 12]], dtype=torch.float32) print(tensor_B.T) output = torch.matmul(tensor_A, tensor_B.T) print(output)  tensor_A.shape,tensor_B.shape,tensor_B.T.shape,output.shape <pre>tensor([[ 7.,  8.,  9.],\n        [10., 11., 12.]])\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])\n</pre> Out[13]: <pre>(torch.Size([3, 2]),\n torch.Size([3, 2]),\n torch.Size([2, 3]),\n torch.Size([3, 3]))</pre> <p>\u77e9\u9635\u76f8\u4e58\u7684\u76f8\u5173\u77e5\u8bc6\u53ef\u4ee5\u53bb\u770b\u4e00\u70b9\u7ebf\u6027\u4ee3\u6570\u7684\u5185\u5bb9\uff0c\u5b83\u7684\u53ef\u89c6\u5316\u8ba1\u7b97\u53ef\u4ee5\u770b\u8fd9\u4e2a\u7f51\u7ad9\u3002\u53ef\u4ee5\u4f7f\u7528<code>torch.mm</code>\u6765\u5f53\u4f5c\u77e9\u9635\u8ba1\u7b97\u7684\u7b80\u5199\u3002</p> <p>\u8fd9\u91cc\u7684\u5185\u5bb9\u53ef\u80fd\u4f1a\u53c8\u5199\u8d85\u524d\uff0c\u53ef\u4ee5\u5148\u63d0\u524d\u770b\u4e00\u4e0b\uff1a</p> <p>\u795e\u7ecf\u7f51\u7edc\u4e2d\u8ba1\u5212\u5168\u90e8\u90fd\u662f\u70b9\u4e58\u548c\u77e9\u9635\u8fd0\u7b97\uff0c\u5728pytorch\u4e2d<code>troch.nn.Linear()</code>\u5b9a\u4e49\u4e86\u6700\u7b80\u5355\u7684\u795e\u7ecf\u7f51\u7edc-\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u6216\u8005\u53eb\u5168\u94fe\u63a5\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5c06\u8f93\u5165\u7684\u5f20\u91cf<code>x</code>\u4e58\u4ee5\u6743\u91cd<code>A</code>\u7684\u8f6c\u7f6e\u5e76\u4e14\u52a0\u4e0a\u504f\u6267\u9879<code>b</code>\u5f97\u5230\u5230\u8f93\u51fa\uff0c\u6a21\u578b\u7684\u8bad\u7ec3\u5219\u662f\u4e0d\u65ad\u7684\u66f4\u65b0\u6743\u91cd<code>A</code>\uff0c\u662f\u6a21\u578b\u80fd\u591f\u66f4\u597d\u7684\u8868\u793a\u6570\u636e\u7684\u7279\u5f81\uff0c\u504f\u6267\u9879\uff08bias\uff09\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u66f4\u597d\u7684\u62df\u5408\u6570\u636e\u3002</p> <p>\u6570\u5b66\u516c\u5f0f\u4e3a\uff1a$y=x\\ast A^{T}+b$</p> In\u00a0[14]: Copied! <pre># \u8bbe\u5b9a\u968f\u673a\u6570\u79cd\u5b50\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u590d\u73b0\u4ee3\u7801\uff0c\u56e0\u4e3a\u795e\u7ecf\u7f51\u7edc\u5728\u521d\u59cb\u5316\u6743\u91cd\u7684\u65f6\u5019\u662f\u968f\u673a\u521d\u59cb\u5316\u7684\ntorch.manual_seed(42)\n# This uses matrix multiplication\nlinear = torch.nn.Linear(in_features=2, # \n                         out_features=6) # in_features\u548cout_features\u662f\u5236\u5b9a\u6743\u91cd\u77e9\u9635\u7684\u5f62\u72b6 2*6\nx = tensor_A\noutput = linear(x)\nprint(f\"Input shape: {x.shape}\\n\")\nprint(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")\n</pre> # \u8bbe\u5b9a\u968f\u673a\u6570\u79cd\u5b50\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u590d\u73b0\u4ee3\u7801\uff0c\u56e0\u4e3a\u795e\u7ecf\u7f51\u7edc\u5728\u521d\u59cb\u5316\u6743\u91cd\u7684\u65f6\u5019\u662f\u968f\u673a\u521d\u59cb\u5316\u7684 torch.manual_seed(42) # This uses matrix multiplication linear = torch.nn.Linear(in_features=2, #                           out_features=6) # in_features\u548cout_features\u662f\u5236\u5b9a\u6743\u91cd\u77e9\u9635\u7684\u5f62\u72b6 2*6 x = tensor_A output = linear(x) print(f\"Input shape: {x.shape}\\n\") print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\") <pre>Input shape: torch.Size([3, 2])\n\nOutput:\ntensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\nOutput shape: torch.Size([3, 6])\n</pre> In\u00a0[15]: Copied! <pre>tensor = torch.rand(size = (3,4))\ntensor,tensor.max(),tensor.min(),tensor.sum(),tensor.mean(),tensor.argmax(),tensor.argmin()\n</pre> tensor = torch.rand(size = (3,4)) tensor,tensor.max(),tensor.min(),tensor.sum(),tensor.mean(),tensor.argmax(),tensor.argmin() Out[15]: <pre>(tensor([[0.2666, 0.6274, 0.2696, 0.4414],\n         [0.2969, 0.8317, 0.1053, 0.2695],\n         [0.3588, 0.1994, 0.5472, 0.0062]]),\n tensor(0.8317),\n tensor(0.0062),\n tensor(4.2200),\n tensor(0.3517),\n tensor(5),\n tensor(11))</pre> In\u00a0[16]: Copied! <pre>#\u4e5f\u53ef\u4ee5\u53bb\u6539\u53d8tensor\u7684\u6570\u636e\u7c7b\u578b\uff0c\u4e0d\u8fc7\u8bf4\u5b9e\u8bdd\u8fd9\u4e2a\u9700\u8981\u4f7f\u7528\u5230\u7684\u573a\u666f\u5f88\u5c11\ntensor.dtype,tensor.type(torch.float64).dtype\n</pre> #\u4e5f\u53ef\u4ee5\u53bb\u6539\u53d8tensor\u7684\u6570\u636e\u7c7b\u578b\uff0c\u4e0d\u8fc7\u8bf4\u5b9e\u8bdd\u8fd9\u4e2a\u9700\u8981\u4f7f\u7528\u5230\u7684\u573a\u666f\u5f88\u5c11 tensor.dtype,tensor.type(torch.float64).dtype Out[16]: <pre>(torch.float32, torch.float64)</pre> In\u00a0[17]: Copied! <pre>x = torch.arange(1., 8.)\nx_reshaped = x.reshape(1, 7)\nprint(\"x: {}, x.reshape: {}\".format(x,x_reshaped))\nx[0] =10\nprint(\"x: {}, x.reshape: {}\".format(x,x_reshaped))\nx_reshaped[:,0] = 1\nprint(\"x: {}, x.reshape: {}\".format(x,x_reshaped))\n</pre> x = torch.arange(1., 8.) x_reshaped = x.reshape(1, 7) print(\"x: {}, x.reshape: {}\".format(x,x_reshaped)) x[0] =10 print(\"x: {}, x.reshape: {}\".format(x,x_reshaped)) x_reshaped[:,0] = 1 print(\"x: {}, x.reshape: {}\".format(x,x_reshaped)) <pre>x: tensor([1., 2., 3., 4., 5., 6., 7.]), x.reshape: tensor([[1., 2., 3., 4., 5., 6., 7.]])\nx: tensor([10.,  2.,  3.,  4.,  5.,  6.,  7.]), x.reshape: tensor([[10.,  2.,  3.,  4.,  5.,  6.,  7.]])\nx: tensor([1., 2., 3., 4., 5., 6., 7.]), x.reshape: tensor([[1., 2., 3., 4., 5., 6., 7.]])\n</pre> <p>\u4ece\u4e0a\u9762\u7684\u8f93\u51fa\u53ef\u4ee5\u770b\u51fa\uff0creshape\u53ea\u4f1a\u6539\u53d8size\uff0creshpe\u4e4b\u540e\u7684\u8d4b\u503c\u5411\u91cf\u662f\u4e8e\u4e4b\u524d\u5411\u91cf\u6709\u8fde\u63a5\u5173\u7cfb\u7684\uff08\u4f7f\u7528\u76f8\u540c\u7684\u5185\u5b58\uff09\u3002\u9700\u8981\u901a\u8fc7tensor.clone()\u6765\u4e3a\u5176\u6784\u5efa\u65b0\u7684\u5185\u5b58\uff0c\u5982\u4e0b\uff1a</p> In\u00a0[18]: Copied! <pre>a = torch.arange(1.0,6.0)\nb = a\nc = a.clone()\na[1] = 10\na,b,c\n#\u4ece\u7ed3\u679c\u53ef\u4ee5\u770b\u51fa.colne()\u4e4b\u540e\u5219\u4e0d\u4f1a\u8fdb\u884c\u540c\u6b65\u53d8\u5316\n</pre> a = torch.arange(1.0,6.0) b = a c = a.clone() a[1] = 10 a,b,c #\u4ece\u7ed3\u679c\u53ef\u4ee5\u770b\u51fa.colne()\u4e4b\u540e\u5219\u4e0d\u4f1a\u8fdb\u884c\u540c\u6b65\u53d8\u5316 Out[18]: <pre>(tensor([ 1., 10.,  3.,  4.,  5.]),\n tensor([ 1., 10.,  3.,  4.,  5.]),\n tensor([1., 2., 3., 4., 5.]))</pre> <p><code>torch.Tensor.view()</code>\u8fd9\u4e2a\u6211\u76ee\u524d\u8fd8\u662f\u4e0d\u80fd\u7406\u89e3\u5176\u7279\u6027\uff0c\u603b\u89c9\u5f97\u4ed6\u8ddfreshape\u662f\u76f8\u4e92\u91cd\u53e0\u7684\u3002</p> <p>\u901a\u8fc7<code>torch.stack()</code>\u51fd\u6570\u53ef\u4ee5\u5bf9\u5f20\u91cf\u5b89\u88c5\u67d0\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u8fdb\u884c\u5806\u53e0,\u5176\u4e2ddim(\u8303\u56f4\u662f[-2,1]))\u6307\u5b9a\u7684\u662f\u7ef4\u5ea6,\u5e38\u7528\u7684\u662fdim=0\u5c31\u8db3\u591f\u4e86,dim=1\u76f8\u5f53\u4e8e\u8f6c\u7f6e\u3002</p> In\u00a0[19]: Copied! <pre>print(torch.stack([x,x],dim=0))\nprint(torch.stack([x,x],dim=1))\n</pre> print(torch.stack([x,x],dim=0)) print(torch.stack([x,x],dim=1)) <pre>tensor([[1., 2., 3., 4., 5., 6., 7.],\n        [1., 2., 3., 4., 5., 6., 7.]])\ntensor([[1., 1.],\n        [2., 2.],\n        [3., 3.],\n        [4., 4.],\n        [5., 5.],\n        [6., 6.],\n        [7., 7.]])\n</pre> In\u00a0[20]: Copied! <pre>#squeeze\u548cunsqueeze\u4e24\u4e2a\u51fd\u6570\u662f\u5206\u522b\u5bf9\u5411\u91cf\u8fdb\u884c\u964d\u7ef4\u548c\u5347\u7ef4\uff0c\u611f\u89c9\u5e76\u4e0d\u7279\u6b8a\uff0creshape\u53ef\u4ee5\u5b8c\u6210\u8fd9\u4e24\u4e2a\u4efb\u52a1\u3002\nx,x.unsqueeze(dim=0),x.unsqueeze(dim=0).squeeze(),x.reshape([1,7]),x.reshape([1,7]).reshape(7) #\u539f\u59cb\uff0c\u5347\u7ef4\uff0c\u964d\u7ef4\uff0c\u901a\u8fc7reshape\u5347\u7ef4\uff0c\u901a\u8fc7reshape\u964d\u7ef4\n</pre> #squeeze\u548cunsqueeze\u4e24\u4e2a\u51fd\u6570\u662f\u5206\u522b\u5bf9\u5411\u91cf\u8fdb\u884c\u964d\u7ef4\u548c\u5347\u7ef4\uff0c\u611f\u89c9\u5e76\u4e0d\u7279\u6b8a\uff0creshape\u53ef\u4ee5\u5b8c\u6210\u8fd9\u4e24\u4e2a\u4efb\u52a1\u3002 x,x.unsqueeze(dim=0),x.unsqueeze(dim=0).squeeze(),x.reshape([1,7]),x.reshape([1,7]).reshape(7) #\u539f\u59cb\uff0c\u5347\u7ef4\uff0c\u964d\u7ef4\uff0c\u901a\u8fc7reshape\u5347\u7ef4\uff0c\u901a\u8fc7reshape\u964d\u7ef4 Out[20]: <pre>(tensor([1., 2., 3., 4., 5., 6., 7.]),\n tensor([[1., 2., 3., 4., 5., 6., 7.]]),\n tensor([1., 2., 3., 4., 5., 6., 7.]),\n tensor([[1., 2., 3., 4., 5., 6., 7.]]),\n tensor([1., 2., 3., 4., 5., 6., 7.]))</pre> <p>permute\u662f\u6bd4\u8f83\u7279\u6b8a\u7684\u51fd\u6570\uff0c\u5b83\u53ef\u4ee5\u7528\u6765\u6539\u53d8\u4e0d\u540c\u7ef4\u5ea6\u7684\u4f4d\u7f6e\uff0c\u4f8b\u5b50\u5982\u4e0b\uff1a</p> In\u00a0[21]: Copied! <pre>x_original = torch.rand(size=(224, 224, 3))\nx_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0\n\nprint(f\"Previous shape: {x_original.shape}\")\nprint(f\"New shape: {x_permuted.shape}\")\n</pre> x_original = torch.rand(size=(224, 224, 3)) x_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0  print(f\"Previous shape: {x_original.shape}\") print(f\"New shape: {x_permuted.shape}\") <pre>Previous shape: torch.Size([224, 224, 3])\nNew shape: torch.Size([3, 224, 224])\n</pre> In\u00a0[22]: Copied! <pre>x = torch.arange(1, 10).reshape(1, 3, 3)\nx,x[0],x[0][0],x[0][0][0] #\u7b2c\u4e00\u79cd\u65b9\u5f0f\u7c7b\u4f3c\u4e8eR\u4e2d\u7684\u6982\u5ff5\uff0c\u901a\u8fc7`[]`\u7684\u65b9\u5f0f\u6765\u89e3\u5f00\u4e2d\u62ec\u53f7\u3002\n</pre> x = torch.arange(1, 10).reshape(1, 3, 3) x,x[0],x[0][0],x[0][0][0] #\u7b2c\u4e00\u79cd\u65b9\u5f0f\u7c7b\u4f3c\u4e8eR\u4e2d\u7684\u6982\u5ff5\uff0c\u901a\u8fc7`[]`\u7684\u65b9\u5f0f\u6765\u89e3\u5f00\u4e2d\u62ec\u53f7\u3002 Out[22]: <pre>(tensor([[[1, 2, 3],\n          [4, 5, 6],\n          [7, 8, 9]]]),\n tensor([[1, 2, 3],\n         [4, 5, 6],\n         [7, 8, 9]]),\n tensor([1, 2, 3]),\n tensor(1))</pre> In\u00a0[23]: Copied! <pre>x[:,0,:],x[:,:,0],x[:,0,0] #\u7b2c\u4e8c\u79cd\u65b9\u5f0f\u4e0d\u592a\u597d\u7406\u89e3\uff0c\u4f46\u662f\u66f4\u52a0\u7684\u7075\u6d3b\uff0c\u53ef\u4ee5\u8fdb\u884c\u7ad6\u6392\u7684\u53d6\u3002\n</pre> x[:,0,:],x[:,:,0],x[:,0,0] #\u7b2c\u4e8c\u79cd\u65b9\u5f0f\u4e0d\u592a\u597d\u7406\u89e3\uff0c\u4f46\u662f\u66f4\u52a0\u7684\u7075\u6d3b\uff0c\u53ef\u4ee5\u8fdb\u884c\u7ad6\u6392\u7684\u53d6\u3002 Out[23]: <pre>(tensor([[1, 2, 3]]), tensor([[1, 4, 7]]), tensor([1]))</pre> In\u00a0[24]: Copied! <pre># # Set the random seed\nRANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\ntorch.manual_seed(seed=RANDOM_SEED) \nrandom_tensor_C = torch.rand(3, 4)\n\n# Have to reset the seed every time a new rand() is called \n# Without this, tensor_D would be different to tensor_C \ntorch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\uff0c\u6ce8\u91ca\u6389\u4e4b\u540e\u7ed3\u679c\u5c31\u4e0d\u518d\u60f3\u901a\u4e86\u3002\nrandom_tensor_D = torch.rand(3, 4)\n\nprint(f\"Tensor C:\\n{random_tensor_C}\\n\")\nprint(f\"Tensor D:\\n{random_tensor_D}\\n\")\nprint(f\"Does Tensor C equal Tensor D? (anywhere)\")\nrandom_tensor_C == random_tensor_D\n</pre> # # Set the random seed RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below torch.manual_seed(seed=RANDOM_SEED)  random_tensor_C = torch.rand(3, 4)  # Have to reset the seed every time a new rand() is called  # Without this, tensor_D would be different to tensor_C  torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\uff0c\u6ce8\u91ca\u6389\u4e4b\u540e\u7ed3\u679c\u5c31\u4e0d\u518d\u60f3\u901a\u4e86\u3002 random_tensor_D = torch.rand(3, 4)  print(f\"Tensor C:\\n{random_tensor_C}\\n\") print(f\"Tensor D:\\n{random_tensor_D}\\n\") print(f\"Does Tensor C equal Tensor D? (anywhere)\") random_tensor_C == random_tensor_D <pre>Tensor C:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nTensor D:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nDoes Tensor C equal Tensor D? (anywhere)\n</pre> Out[24]: <pre>tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])</pre> In\u00a0[25]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Mon Apr 17 15:47:15 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  On   | 00000000:B1:00.0 Off |                  N/A |\n| 38%   35C    P2   105W / 350W |    810MiB / 24576MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      2726      G   /usr/lib/xorg/Xorg                  4MiB |\n|    0   N/A  N/A   2199377      C   ...conda3/envs/dl/bin/python      802MiB |\n+-----------------------------------------------------------------------------+\n</pre> In\u00a0[26]: Copied! <pre>torch.cuda.is_available(),torch.cuda.device_count() #\u67e5\u770b\u663e\u5361\u662f\u5426\u53ef\u7528\u4e0e\u6709\u591a\u5c11\u4e2a\u663e\u5361\u53ef\u7528\n</pre> torch.cuda.is_available(),torch.cuda.device_count() #\u67e5\u770b\u663e\u5361\u662f\u5426\u53ef\u7528\u4e0e\u6709\u591a\u5c11\u4e2a\u663e\u5361\u53ef\u7528 Out[26]: <pre>(True, 1)</pre> In\u00a0[27]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\" #\u8fd9\u4e2a\u547d\u4ee4\u5c06\u662f\u540e\u7eed\u6700\u5e38\u7528\u7684\u547d\u4ee4\uff0c\u7528\u6765\u8bbe\u7f6e\u8fd0\u884c\u8bbe\u5907\uff0c\u6709\u663e\u5361\u7684\u8bdd\u5219\u4f1a\u5728\u663e\u5361\u4e0a\u8fd0\u884c\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" #\u8fd9\u4e2a\u547d\u4ee4\u5c06\u662f\u540e\u7eed\u6700\u5e38\u7528\u7684\u547d\u4ee4\uff0c\u7528\u6765\u8bbe\u7f6e\u8fd0\u884c\u8bbe\u5907\uff0c\u6709\u663e\u5361\u7684\u8bdd\u5219\u4f1a\u5728\u663e\u5361\u4e0a\u8fd0\u884c device Out[27]: <pre>'cuda'</pre> In\u00a0[28]: Copied! <pre>#\u6211\u4e48\u53ef\u4ee5\u8bb2tensor\u548cmodel\u653e\u5728GPU\u4e0a\u8fdb\u884c\u8fd0\u884c\ntensor,tensor.to(device)\n</pre> #\u6211\u4e48\u53ef\u4ee5\u8bb2tensor\u548cmodel\u653e\u5728GPU\u4e0a\u8fdb\u884c\u8fd0\u884c tensor,tensor.to(device) Out[28]: <pre>(tensor([[0.2666, 0.6274, 0.2696, 0.4414],\n         [0.2969, 0.8317, 0.1053, 0.2695],\n         [0.3588, 0.1994, 0.5472, 0.0062]]),\n tensor([[0.2666, 0.6274, 0.2696, 0.4414],\n         [0.2969, 0.8317, 0.1053, 0.2695],\n         [0.3588, 0.1994, 0.5472, 0.0062]], device='cuda:0'))</pre>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/#chapter-00-pytorch","title":"Chapter 00 Pytorch \u57fa\u7840\u00b6","text":""},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/","title":"\u8d44\u6599\u6765\u6e90\u00b6","text":"<p>Learn PyTorch for Deep Learning: Zero to Mastery book. link\uff0c\u4ece0\u5f00\u59cb\u7684pytorch\u5b66\u4e60\u3002</p> <p>\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u7b2c\u4e8c\u597d\u7684\u5b66\u4e60pytorch\u7684\u5730\u65b9\uff0c\uff08\u7b2c\u4e00\u597d\u7684\u662fpytorhc\u7684\u6587\u6863\uff09\u3002\u7c97\u770b\u8d77\u6765\u786e\u5b9e\u4e0d\u9519\uff0c\u63a5\u4e0b\u6765\u7684\u5185\u5bb9\u662f\u6839\u636e\u76f8\u5173\u7684\u5185\u5bb9\u6dfb\u52a0\u81ea\u5df1\u7684\u7406\u89e3\u5199\u6210\u7684\u3002\u82f1\u8bed\u4e0d\u9519\u7684\u8bdd\u8fd8\u662f\u63a8\u8350\u770b\u539f\u5185\u5bb9\uff0c\u4f5c\u8005\u63d0\u4f9b\u7684\u975e\u5e38\u4e30\u5bcc\u7684\u5f62\u5f0f\u6765\u8fdb\u884c\u5c55\u793a\uff08colab\uff0cYoutube\uff09\u3002</p>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/#pytorch","title":"pytorch\u7b80\u4ecb\u00b6","text":"<p>pytorch\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u57fa\u4e8epython\u6765\u5904\u7406\u6570\u636e\u548c\u6784\u5efa\u673a\u5668\u5b66\u4e60\u7684\u7b97\u6cd5\u3002pytorch\u975e\u5e38\u4f18\u79c0\uff0c\u5305\u62ec\u5fae\u8f6f\uff0cTesla\u548cOpenAI\u7b49\u591a\u4e2a\u5927\u7684\u56e2\u961f\u90fd\u662f\u7ed9\u4e88pytorch\u6765\u8fdb\u884c\u4ed6\u4eec\u7684\u7814\u7a76\u3002</p> <p>\u5b83\u7684\u53d1\u5c55\u4e5f\u975e\u5e38\u8fc5\u901f\uff0c\u57282022\u5e74\u5df2\u53d1\u8868\u6587\u732e\u7684\u4ee3\u7801\u7c7b\u578b\u4e2d\u5360\u6bd459%\uff0c\u5df2\u7ecf\u662f\u7814\u7a76\u8005\u6700\u7231\u4f7f\u7528\u7684\u4ee3\u7801\u3002pytorch\u5bf9\u4e8eGPU\u52a0\u901f\u7684\u652f\u6301\u975e\u5e38\u7684\u597d\uff0c\u6240\u4ee5\u4f60\u53ef\u4ee5\u4e13\u6ce8\u4e8e\u4f60\u7684\u6570\u636e\u548c\u7b97\u6cd5\u3002</p>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/#pytorch","title":"pytorch\u7684\u5b89\u88c5\u548c\u5bfc\u5165\u00b6","text":"<p>\u5728pytorch\u7684\u5b98\u7f51\u4e0a\uff0c\u53ef\u4ee5\u6839\u636e\u81ea\u5df1\u7684\u7cfb\u7edf\u7c7b\u578b\u7b49\u9700\u6c42\u5b89\u88c5\u5408\u9002\u7684torch\u7248\u672c\uff0c\u6211\u8fd9\u91cc\u662f\u5728linux\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7conda\u5b89\u88c5\u7684pytorch2.0\u7248\u672c\uff0cCUDA\u662f11.8\u3002\u547d\u4ee4\u5982\u4e0b<code>conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia</code></p> <p>2.0\u7248\u672c\u662f\u8fd1\u671f\u53d1\u5e03\u7684\uff082023.3\uff09\uff0c\u589e\u52a0\u4e86torch.compile\uff0c\u66f4\u5feb\u66f4\u5f3a\u3002</p>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/#tensor","title":"\u5f20\u91cf\uff08Tensor\uff09\u00b6","text":"<p>Tensor\u53ef\u4ee5\u7406\u89e3\u4e3a\u591a\u7ef4\u6570\u7ec4\uff0c\u5982\u6807\u91cf\u53ef\u4ee5\u770b\u4f5c0\u7ef4\u5f20\u91cf\uff0c\u5411\u91cf\u53ef\u4ee5\u770b\u4f5c1\u7ef4\u5f20\u91cf\uff0c\u77e9\u9635\u53ef\u4ee5\u770b\u4f5c2\u7ef4\u5f20\u91cf\u3002Tensor\u662f\u6784\u6210\u673a\u5668\u5b66\u4e60\u7684\u6700\u57fa\u672c\u7684\u6a21\u5757\u3002</p> <p>\u6bd4\u5982\u5bf9\u4e8e\u6709\u4e2a255x255\u7684RGB\u56fe\u50cf\uff0c\u5c06\u5176\u8868\u793a\u4e3a\u5f20\u91cf\uff0c\u5b83\u7684\u8868\u793a\u5f62\u5f0f\u662f[3,255,255],\u5206\u522b\u6307\u4ee3\u7684\u662f\u56fe\u50cf\u901a\u9053\uff0c\u957f\u548c\u5bbd\u3002</p> <p>\u63a5\u4e0b\u6765\u6211\u4eec\u901a\u8fc7pytorch\u6765\u5206\u522b\u6784\u5efa\u6807\u91cf\uff0c\u5411\u91cf\uff0c\u77e9\u9635\u548c\u5f20\u91cf\u3002</p>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/#tensor-operations","title":"\u64cd\u4f5c\u5f20\u91cf\uff08tensor operations\uff09\u00b6","text":""},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/","title":"\u5f20\u91cf\u8ba1\u7b97\u00b6","text":"<p>\u6211\u4eec\u5df2\u7ecf\u4e86\u89e3\u4e86\u5982\u4f55\u53bb\u6784\u5efa\u5f20\u91cf\uff0c\u800c\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u5bf9\u5f20\u91cf\u7684\u64cd\u4f5c\u624d\u662f\u6700\u4e3a\u91cd\u8981\u7684\u3002\u4e0d\u540c\u6a21\u578b\u7684\u67b6\u6784\u672c\u8d28\u4e0a\u662f\u4f7f\u7528\u4e0d\u540c\u7684\u8ba1\u7b97\u65b9\u5f0f\uff08\u7b56\u7565\uff09\u6765\u53bb\u5bf9\u5f20\u91cf\u8fdb\u884c\u5904\u7406\u3002\u5e38\u89c1\u7684\u8ba1\u7b97\u65b9\u5f0f\u662f\uff1a\u52a0\uff0c\u51cf\uff0c\u4e58\uff0c\u9664\u548c\u77e9\u9635\u4e58\u6cd5\u3002</p>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/","title":"\u5f20\u91cf\u7279\u5f81\u00b6","text":"<p>pytorch\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7684\u51fd\u6570\u7528\u6765\u8ba1\u7b97\u6700\u5927\u503c\uff0c\u6700\u5c0f\u503c\uff0c\u5e73\u5747\u503c\uff0c\u548c\u7b49\u7279\u5f81\uff0c\u800c\u4e14\u53ef\u4ee5\u53bb\u67e5\u627e\u76f8\u5e94\u6700\u5927\u503c\u6216\u8005\u6700\u5c0f\u503c\u7684\u4f4d\u7f6e</p>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/","title":"\u5f20\u91cf\u7684\u5f62\u72b6\u64cd\u4f5c\u00b6","text":"<p>\u6211\u4eec\u5df2\u7ecf\u4e86\u89e3\u5230\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u51e0\u4e4e\u5168\u662f\u77e9\u9635\u4e58\u6cd5\uff0c\u4f46\u662f\u77e9\u9635\u4e58\u6cd5\u5bf9\u4e8e\u4e24\u4e2a\u5f20\u91cf\u7684\u5f62\u72b6\u8981\u6c42\u975e\u5e38\u7684\u4e25\u683c\uff0c\u6240\u4ee5pytorch\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7684\u51fd\u6570\u6765\u53bb\u4fee\u6539\u5f20\u91cf\u7684\u5f62\u72b6\uff0c\u4ee5\u4fbf\u4e8e\u6211\u4eec\u80fd\u591f\u6b63\u786e\u7684\u8fdb\u884c\u77e9\u9635\u8fd0\u7b97\u3002\u53ef\u4f9b\u4f7f\u7528\u7684\u51fd\u6570\u6709<code>torch.reshape()</code>,<code>torch.Tensor.view()</code>,<code>torch.stack()</code>,<code>torch.squeeze()</code>,<code>torch.unsqueeze()</code>,<code>torch.permute()</code>:</p>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/","title":"\u5f20\u91cf\u7684\u7d22\u5f15\u00b6","text":"<p>\u83b7\u5f97Tensor\u7279\u5b9a\u4f4d\u7f6e\u7684\u6570\u636e\u7684\u884c\u4e3a\u662f\u7ecf\u5e38\u7528\u5230\u7684\u3002\u5f20\u91cf\u7684\u7d22\u5f15\u8ddfnumpy\u7684\u65b9\u5f0f\u975e\u5e38\u7684\u7c7b\u4f3c\uff0c\u8fd9\u91cc\u5c55\u793a\u4e24\u79cd\u65b9\u5f0f\uff08\u591a\u4e2a\u4e2d\u62ec\u53f7\u548c\u4e00\u4e2a\u4e2d\u62ec\u53f7\uff09\uff1a</p>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/#numpypytorch-tensor","title":"numpy\u4e0epytorch tensor\u00b6","text":"<p>\u6700\u5f00\u59cb\u7684\u673a\u5668\u5b66\u4e60\u597d\u50cf\u662f\u57fa\u4e8enumpy\u7684\uff0c\u56e0\u6b64\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u4f1a\u6709numpy\u4e0etensor\u7684\u8f6c\u6362\uff0cnumpy\u4e2d\u7684\u591a\u7ef4\u6570\u7ec4\u4e0etensor\u6027\u8d28\u662f\u76f8\u4f3c\u7684\u3002</p> <p>pytorch\u6709\u4e24\u4e2a\u51fd\u6570\u53ef\u4ee5\u7528\u6765\u4e4b\u95f4\u7684\u4e92\u76f8\u8f6c\u5316\uff1a<code>torch.from_numpy(ndarray)</code>\uff1andarray to tensor;<code>torch.Tensor.numpy()</code>:tensor to ndarray\u3002</p> <p>\u8fd8\u6709\u4e00\u4e2a\u8981\u5173\u6ce8\u7684\u70b9\u662fnumpy\u4e2d\u4f7f\u7528\u7684\u662ffloat64\u4f4d\uff0cpytorch\u662ffloat32\u4f4d\uff0c\u56e0\u6b64\u53ef\u80fd\u9700\u8981\u4e00\u4e9b\u6570\u636e\u7c7b\u578b\u7684\u8f6c\u6362\u3002</p>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/","title":"\u5176\u4ed6\u00b6","text":"<p>\u8fd9\u91cc\u8fd8\u6709\u4e00\u4e9b\u5176\u4ed6\u9700\u8981\u6ce8\u610f\u7684\u8981\u70b9\uff1a</p>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/#reproducibility","title":"\u590d\u73b0\uff08Reproducibility\uff09\u00b6","text":"<p>\u4e3a\u4ec0\u4e48\u9700\u8981\u8bbe\u7f6e\u968f\u673a\u6570\u5462\uff1f\u56e0\u4e3a\u6211\u4eec\u60f3\u8981\u590d\u73b0\u6bcf\u6b21\u7684\u7ed3\u679c\uff0c\u6709\u5229\u4e8e\u522b\u4eba\u4e5f\u6709\u76ca\u4e8e\u81ea\u5df1\uff08\u6bd4\u8f83\u4e0d\u540c\u8d85\u53c2\u6570\u7684\u5f71\u54cd\uff09\u3002pytorch\u901a\u8fc7\u4e86\u76f8\u5e94\u7684\u51fd\u6570\u6765\u8bbe\u7f6e\u968f\u673a\u6570\u79cd\u5b50\uff1a<code>torch.manual_seed()</code>\u3002seed\u53ef\u4ee5\u662f\u4efb\u610f\u7684\u6570\u5b57\uff0c\u8be5\u51fd\u6570\u53ef\u4ee5\u4fdd\u8bc1\u4e24\u6b21\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u751f\u6210\u7684\u7ed3\u679c\u662f\u4e00\u6837\u7684\uff0c\u4f46\u662f\u8981\u5728\u4e00\u6b21\u8fd0\u884c\u4e2d\u751f\u6210\u76f8\u540c\u7684\u968f\u673a\u6570\uff0c\u9700\u8981\u91cd\u65b0\u5728\u8bbe\u7f6e\u4e00\u6b21\uff0c\u5982\u4e0b\uff1a</p> <p><code>torch.manual_seed()</code>\u4e5f\u5f88\u5f3a\u5927\uff0c\u5b83\u4f1a\u4e3a\u6240\u6709\u7684\u8bbe\u5907\u8bbe\u7f6e\u968f\u673a\u6570\uff0c\u5305\u62eccpu\u548cgpu\u3002</p>"},{"location":"Learn/z2m-pytorch/00_pytorch_foundmental/","title":"\u8fd0\u884c\u8bbe\u5907\u00b6","text":"<p>\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u5305\u542b\u5927\u91cf\u7684\u77e9\u9635\u8fd0\u7b97\uff0c\u800cGPU\u5373\u663e\u5361\u7684\u77e9\u9635\u8fd0\u7b97\u80fd\u529b\u8981\u8fdc\u5f3a\u4e8eCPU\uff0c\u901a\u8fc7GPU\u53ef\u4ee5\u663e\u8457\u7684\u63d0\u5347\u8fd0\u7b97\u7684\u901f\u5ea6\u3002GPU\u7684\u8be6\u7ec6\u8d44\u6599\u5c31\u4e0d\u518d\u8fd9\u91cc\u6d89\u53ca\u4e86\uff0c\u65e5\u5e38\u4f7f\u7528\u4e2dNvidia\u7684GPU\u662f\u6700\u5e38\u89c1\u7684\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7<code>!nvidia-smi</code>\u547d\u4ee4\u6765\u67e5\u770b\u663e\u5361\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/","title":"01. PyTorch Workflow Fundamentals","text":"<p>For now, we'll use this workflow to predict a simple straight line but the workflow steps can be repeated and changed depending on the problem you're working on.</p> <p>Specifically, we're going to cover:</p> Topic Contents 1. Getting data ready Data can be almost anything but to get started we're going to create a simple straight line 2. Building a model Here we'll create a model to learn patterns in the data, we'll also choose a loss function, optimizer and build a training loop. 3. Fitting the model to data (training) We've got data and a model, now let's let the model (try to) find patterns in the (training) data. 4. Making predictions and evaluating a model (inference) Our model's found patterns in the data, let's compare its findings to the actual (testing) data. 5. Saving and loading a model You may want to use your model elsewhere, or come back to it later, here we'll cover that. 6. Putting it all together Let's take all of the above and combine it. In\u00a0[40]: Copied! <pre>what_were_covering = {1: \"data (prepare and load)\",\n    2: \"build model\",\n    3: \"fitting the model to data (training)\",\n    4: \"making predictions and evaluating a model (inference)\",\n    5: \"saving and loading a model\",\n    6: \"putting it all together\"\n}\n</pre> what_were_covering = {1: \"data (prepare and load)\",     2: \"build model\",     3: \"fitting the model to data (training)\",     4: \"making predictions and evaluating a model (inference)\",     5: \"saving and loading a model\",     6: \"putting it all together\" } <p>And now let's import what we'll need for this module.</p> <p>We're going to get <code>torch</code>, <code>torch.nn</code> (<code>nn</code> stands for neural network and this package contains the building blocks for creating neural networks in PyTorch) and <code>matplotlib</code>.</p> <p>\u8fd9\u91cc\u7684\u5bfc\u5165\u5305\u7684\u5f62\u5f0f\u662f\u6700\u5e38\u7528\u7684\uff0c\u4e0d\u8fc7\u6211\u8ba4\u4e3a\u53ea\u5bfc\u5165torch\u662f\u6700\u597d\u7684\uff0c\u8fd9\u6837\u5f88\u6709\u7406\u7531\u7406\u89e3pytorch\u7684\u6846\u67b6\uff0c\u77e5\u9053\u54ea\u4e2a\u529f\u80fd\u8d77\u7684\u4ec0\u4e48\u540d\u5b57\uff0c\u5728\u54ea\u91cc\uff08\u6bd4\u5982torch.nn\u6307\u7684\u5c31\u662ftorch\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\uff09</p> In\u00a0[41]: Copied! <pre>import torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n</pre> import torch from torch import nn # nn contains all of PyTorch's building blocks for neural networks import matplotlib.pyplot as plt  # Check PyTorch version torch.__version__ Out[41]: <pre>'2.0.0'</pre> <p>Sometimes one and two can be done at the same time.</p> <p>But what if you don't have data?</p> <p>Well, that's where we're at now.</p> <p>No data.</p> <p>But we can create some.</p> <p>Let's create our data as a straight line.</p> <p>We'll use linear regression to create the data with known parameters (things that can be learned by a model) and then we'll use PyTorch to see if we can build model to estimate these parameters using gradient descent.</p> <p>\u8fd9\u91cc\u5c31\u662f\u901a\u8fc7\u7b80\u5355\u7684\u7ebf\u6027\u56de\u5f52\u6765\u521b\u5efa\u6570\u636e\u3002\u4e0d\u8fc7\u8fd9\u91cc\u989d\u5916\u63d0\u51fa\u4e86\u4e00\u4e2a\u68af\u5ea6\u4e0b\u964d\uff08gradient descent\uff09\u7684\u6982\u5ff5\uff0c\u5b83\u662f\u6a21\u578b\u8bad\u7ec3\u4e2d\u975e\u5e38\u91cd\u8981\u7684\u7ec4\u6210\u90e8\u4efd\uff0c\u68af\u5ea6\u4e0b\u964d\uff08Gradient Descent\uff09\u662f\u4e00\u79cd\u4f18\u5316\u7b97\u6cd5\uff0c\u4e3b\u8981\u7528\u4e8e\u6700\u5c0f\u5316\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff08loss function\uff09\u3002\u5b83\u7684\u57fa\u672c\u601d\u60f3\u662f\u901a\u8fc7\u8fed\u4ee3\u7684\u65b9\u5f0f\uff0c\u5728\u6bcf\u4e00\u6b65\u4e2d\u671d\u7740\u5f53\u524d\u4f4d\u7f6e\u7684\u8d1f\u68af\u5ea6\u65b9\u5411\u79fb\u52a8\u4e00\u5b9a\u6b65\u957f\uff0c\u76f4\u5230\u8fbe\u5230\u51fd\u6570\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u6216\u5168\u5c40\u6700\u5c0f\u503c\u3002</p> <p>\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u68af\u5ea6\u4e0b\u964d\u5e38\u5e38\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u3002\u6a21\u578b\u7684\u53c2\u6570\u4f1a\u88ab\u521d\u59cb\u5316\u4e3a\u4e00\u4e9b\u968f\u673a\u503c\uff0c\u7136\u540e\u901a\u8fc7\u4e0d\u65ad\u5730\u8fed\u4ee3\uff0c\u6bcf\u6b21\u66f4\u65b0\u53c2\u6570\u7684\u503c\uff0c\u4f7f\u5f97\u635f\u5931\u51fd\u6570\u7684\u503c\u9010\u6e10\u51cf\u5c0f\uff0c\u76f4\u5230\u6536\u655b\u4e3a\u6b62\u3002\u5728\u6bcf\u6b21\u8fed\u4ee3\u65f6\uff0c\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f1a\u8ba1\u7b97\u51fa\u5f53\u524d\u4f4d\u7f6e\u7684\u68af\u5ea6\uff0c\u7136\u540e\u5c06\u53c2\u6570\u6cbf\u7740\u68af\u5ea6\u7684\u53cd\u65b9\u5411\u79fb\u52a8\u4e00\u5b9a\u6b65\u957f\uff0c\u4ece\u800c\u4f7f\u5f97\u635f\u5931\u51fd\u6570\u7684\u503c\u51cf\u5c0f\u3002\u68af\u5ea6\u4e0b\u964d\u6709\u975e\u5e38\u591a\u7684\u79cd\u7c7b\uff0c\u8fd9\u91cc\u5c31\u5148\u4e0d\u6d89\u53ca\u4e86\u3002</p> <p>Don't worry if the terms above don't mean much now, we'll see them in action and I'll put extra resources below where you can learn more.</p> In\u00a0[42]: Copied! <pre># Create *known* parameters\nweight = 0.7\nbias = 0.3\n\n# Create data\nstart = 0\nend = 1\nstep = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[:10], y[:10]\n</pre> # Create *known* parameters weight = 0.7 bias = 0.3  # Create data start = 0 end = 1 step = 0.02 X = torch.arange(start, end, step).unsqueeze(dim=1) y = weight * X + bias  X[:10], y[:10] Out[42]: <pre>(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))</pre> <p>Beautiful! Now we're going to move towards building a model that can learn the relationship between <code>X</code> (features) and <code>y</code> (labels).</p> In\u00a0[43]: Copied! <pre># Create train/test split\uff0c\u6570\u636e\u5206\u5272\u65b9\u5f0f\u4e5f\u5f88\u6709\u5b66\u95ee\uff0c\u968f\u673a\u5206\u5272\uff0c\u8fc7\u91c7\u6837\u548c\u964d\u91c7\u6837\uff0c\u9700\u8981\u6309\u7167\u95ee\u9898\u6765\u9009\u62e9\u5408\u9002\u7684\u89e3\u51b3\u601d\u8def\u3002\ntrain_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n#\u8fd9\u91cc\u7684\u5206\u5272\u65b9\u6cd5\u4e5f\u4e0d\u662f\u901a\u7528\u7684\u3002\n#\u8fd9\u91cc\u5c31\u662f\u662f\u524d80%\u7528\u6765\u8bad\u7ec3\uff0c\u540e20%\u7528\u6765\u6d4b\u8bd5\n</pre> # Create train/test split\uff0c\u6570\u636e\u5206\u5272\u65b9\u5f0f\u4e5f\u5f88\u6709\u5b66\u95ee\uff0c\u968f\u673a\u5206\u5272\uff0c\u8fc7\u91c7\u6837\u548c\u964d\u91c7\u6837\uff0c\u9700\u8981\u6309\u7167\u95ee\u9898\u6765\u9009\u62e9\u5408\u9002\u7684\u89e3\u51b3\u601d\u8def\u3002 train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing  X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]  len(X_train), len(y_train), len(X_test), len(y_test) #\u8fd9\u91cc\u7684\u5206\u5272\u65b9\u6cd5\u4e5f\u4e0d\u662f\u901a\u7528\u7684\u3002 #\u8fd9\u91cc\u5c31\u662f\u662f\u524d80%\u7528\u6765\u8bad\u7ec3\uff0c\u540e20%\u7528\u6765\u6d4b\u8bd5 Out[43]: <pre>(40, 40, 10, 10)</pre> <p>Wonderful, we've got 40 samples for training (<code>X_train</code> &amp; <code>y_train</code>) and 10 samples for testing (<code>X_test</code> &amp; <code>y_test</code>).</p> <p>The model we create is going to try and learn the relationship between <code>X_train</code> &amp; <code>y_train</code> and then we will evaluate what it learns on <code>X_test</code> and <code>y_test</code>.</p> <p>But right now our data is just numbers on a page.\uff08\u901a\u8fc7\u56fe\u50cf\u6765\u53ef\u89c6\u5316\u6570\u636e\u662f\u4e00\u4e2a\u975e\u5e38\u5173\u952e\u7684\u80fd\u529b\uff09</p> <p>Let's create a function to visualize it.</p> In\u00a0[44]: Copied! <pre>def plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=None):\n\"\"\"\n  Plots training data, test data and compares predictions.\n  \"\"\"\n  plt.figure(figsize=(10, 7))\n\n  # Plot training data in blue\n  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n  \n  # Plot test data in green\n  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n  if predictions is not None:\n    # Plot the predictions in red (predictions were made on the test data)\n    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n  # Show the legend\n  plt.legend(prop={\"size\": 14});\n</pre> def plot_predictions(train_data=X_train,                       train_labels=y_train,                       test_data=X_test,                       test_labels=y_test,                       predictions=None):   \"\"\"   Plots training data, test data and compares predictions.   \"\"\"   plt.figure(figsize=(10, 7))    # Plot training data in blue   plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")      # Plot test data in green   plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")    if predictions is not None:     # Plot the predictions in red (predictions were made on the test data)     plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")    # Show the legend   plt.legend(prop={\"size\": 14}); In\u00a0[45]: Copied! <pre>plot_predictions();\n</pre> plot_predictions(); <p>Epic!</p> <p>Now instead of just being numbers on a page, our data is a straight line.</p> <p>Note: Now's a good time to introduce you to the data explorer's motto... \"visualize, visualize, visualize!\"</p> <p>Think of this whenever you're working with data and turning it into numbers, if you can visualize something, it can do wonders for understanding.</p> <p>Machines love numbers and we humans like numbers too but we also like to look at things.</p> In\u00a0[46]: Copied! <pre># Create a Linear Regression model class\nclass LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n    def __init__(self):\n        super().__init__() \n        self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)\n                                                dtype=torch.float), # &lt;- PyTorch loves float32 by default\n                                   requires_grad=True) # &lt;- can we update this value with gradient descent?)\n\n        self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)\n                                            dtype=torch.float), # &lt;- PyTorch loves float32 by default\n                                requires_grad=True) # &lt;- can we update this value with gradient descent?))\n\n    # Forward defines the computation in the model\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)\n        return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b)\n#pytorch\u6784\u5efa\u6a21\u578b\u6700\u5173\u952e\u7684\u6709\u4e24\u4e2a\u70b9\u90fd\u5728\u8fd9\u91cc\u5c55\u793a\u4e86\uff0c\u5373\u91cd\u5199__init__\u548cforward.\n#-&gt; torch.Tensor\u8fd9\u91cc\u7684\u7bad\u5934\u4e0d\u662f\u8bed\u6cd5\u7684\u4e00\u90e8\u4efd\uff0c\u5b83\u5c5e\u4e8epython\u4e2d\u7684\u7c7b\u578b\u6ce8\u91ca\uff0c\u53ea\u662f\u8868\u660e\u6570\u636e\u7c7b\u578b\uff0c\u7c7b\u4f3c\uff08x:int),\u4fbf\u4e8e\u5f00\u53d1\u8005\u601d\u8003\n</pre> # Create a Linear Regression model class class LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)     def __init__(self):         super().__init__()          self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)                                                 dtype=torch.float), # &lt;- PyTorch loves float32 by default                                    requires_grad=True) # &lt;- can we update this value with gradient descent?)          self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)                                             dtype=torch.float), # &lt;- PyTorch loves float32 by default                                 requires_grad=True) # &lt;- can we update this value with gradient descent?))      # Forward defines the computation in the model     def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)         return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b) #pytorch\u6784\u5efa\u6a21\u578b\u6700\u5173\u952e\u7684\u6709\u4e24\u4e2a\u70b9\u90fd\u5728\u8fd9\u91cc\u5c55\u793a\u4e86\uff0c\u5373\u91cd\u5199__init__\u548cforward. #-&gt; torch.Tensor\u8fd9\u91cc\u7684\u7bad\u5934\u4e0d\u662f\u8bed\u6cd5\u7684\u4e00\u90e8\u4efd\uff0c\u5b83\u5c5e\u4e8epython\u4e2d\u7684\u7c7b\u578b\u6ce8\u91ca\uff0c\u53ea\u662f\u8868\u660e\u6570\u636e\u7c7b\u578b\uff0c\u7c7b\u4f3c\uff08x:int),\u4fbf\u4e8e\u5f00\u53d1\u8005\u601d\u8003 <p>Alright there's a fair bit going on above but let's break it down bit by bit.</p> <p>Resource: We'll be using Python classes to create bits and pieces for building neural networks. If you're unfamiliar with Python class notation, I'd recommend reading Real Python's Object Orientating programming in Python 3 guide a few times.</p> <p><code>torch.nn.Parameter</code>\u8fd9\u4e2a\u53c2\u6570\u6211\u6bd4\u8f83\u964c\u751f\uff0c\u5728\u6784\u5efa\u6a21\u578b\u4e2d\uff0c\u56e0\u4e3a\u7ecf\u5e38\u4f7f\u7528\u73b0\u6709\u7684\u6846\u67b6\uff0c\u6240\u4ee5\u4e0d\u9700\u8981\u81ea\u5df1\u5b9a\u4e49parameters\uff1f</p> In\u00a0[47]: Copied! <pre># Set manual seed since nn.Parameter are randomly initialzied\ntorch.manual_seed(42)\n\n# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\nmodel_0 = LinearRegressionModel()\n\n# Check the nn.Parameter(s) within the nn.Module subclass we created\nlist(model_0.parameters())\n</pre> # Set manual seed since nn.Parameter are randomly initialzied torch.manual_seed(42)  # Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s)) model_0 = LinearRegressionModel()  # Check the nn.Parameter(s) within the nn.Module subclass we created list(model_0.parameters()) Out[47]: <pre>[Parameter containing:\n tensor([0.3367], requires_grad=True),\n Parameter containing:\n tensor([0.1288], requires_grad=True)]</pre> <p>We can also get the state (what the model contains) of the model using <code>.state_dict()</code>. \u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\u67e5\u770b\u6240\u5305\u542b\u7684\u6240\u6709\u4e1c\u897f\u7684\u72b6\u6001\u3002</p> In\u00a0[48]: Copied! <pre># List named parameters \nmodel_0.state_dict()\n</pre> # List named parameters  model_0.state_dict() Out[48]: <pre>OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])</pre> <p>Notice how the values for <code>weights</code> and <code>bias</code> from <code>model_0.state_dict()</code> come out as random float tensors?</p> <p>This is becuase we initialized them above using <code>torch.randn()</code>.</p> <p>Essentially we want to start from random parameters and get the model to update them towards parameters that fit our data best (the hardcoded <code>weight</code> and <code>bias</code> values we set when creating our straight line data).</p> <p>Exercise: Try changing the <code>torch.manual_seed()</code> value two cells above, see what happens to the weights and bias values.</p> <p>Because our model starts with random values, right now it'll have poor predictive power.</p> In\u00a0[49]: Copied! <pre># Make predictions with model\nwith torch.inference_mode(): \n    y_preds = model_0(X_test)\n\n# Note: in older PyTorch code you might also see torch.no_grad()\n# with torch.no_grad():\n#   y_preds = model_0(X_test)\n</pre> # Make predictions with model with torch.inference_mode():      y_preds = model_0(X_test)  # Note: in older PyTorch code you might also see torch.no_grad() # with torch.no_grad(): #   y_preds = model_0(X_test) <p>Hmm?</p> <p>You probably noticed we used <code>torch.inference_mode()</code> as a context manager (that's what the <code>with torch.inference_mode():</code> is) to make the predictions.</p> <p>As the name suggests, <code>torch.inference_mode()</code> is used when using a model for inference (making predictions).</p> <p><code>torch.inference_mode()</code> turns off a bunch of things (like gradient tracking, which is necessary for training but not for inference) to make forward-passes (data going through the <code>forward()</code> method) faster.</p> <p>Note: In older PyTorch code, you may also see <code>torch.no_grad()</code> being used for inference. While <code>torch.inference_mode()</code> and <code>torch.no_grad()</code> do similar things, <code>torch.inference_mode()</code> is newer, potentially faster and preferred. See this Tweet from PyTorch for more.</p> <p>We've made some predictions, let's see what they look like.</p> <p>torch.inference_mode() \u662f PyTorch 1.7.0 \u5f15\u5165\u7684\u4e00\u4e2a\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff08Context Manager\uff09\uff0c\u7528\u4e8e\u5728\u8fd0\u884c\u65f6\u5f00\u542f\u6216\u5173\u95ed\u63a8\u7406\u6a21\u5f0f\u3002\u63a8\u7406\u6a21\u5f0f\u662f\u4e00\u79cd\u4f18\u5316\u6280\u672f\uff0c\u5b83\u53ef\u4ee5\u5728\u63a8\u7406\u65f6\u52a0\u901f\u6a21\u578b\u7684\u6267\u884c\uff0c\u4f46\u5728\u8bad\u7ec3\u65f6\u4f1a\u964d\u4f4e\u6a21\u578b\u7684\u6027\u80fd\u3002</p> <p>\u5728\u63a8\u7406\u6a21\u5f0f\u4e0b\uff0cPyTorch \u4f1a\u5c3d\u53ef\u80fd\u5730\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684\u7b97\u6cd5\u548c\u6570\u636e\u7ed3\u6784\u6765\u6267\u884c\u6a21\u578b\uff0c\u4ee5\u6700\u5c0f\u5316\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002\u4f8b\u5982\uff0c\u5728\u63a8\u7406\u6a21\u5f0f\u4e0b\uff0cPyTorch \u4f1a\u5173\u95ed\u68af\u5ea6\u8ba1\u7b97\u3001\u81ea\u52a8\u5fae\u5206\u548c\u81ea\u52a8\u6c42\u5bfc\u7b49\u673a\u5236\uff0c\u540c\u65f6\u4f7f\u7528 FP16 \u7cbe\u5ea6\u7684\u8ba1\u7b97\u548c\u6279\u91cf\u5f52\u4e00\u5316\uff08Batch Normalization\uff09\u7684\u4e0d\u540c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u6267\u884c\u901f\u5ea6\u3002</p> <p>\u4f7f\u7528 torch.inference_mode() \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u53ef\u4ee5\u65b9\u4fbf\u5730\u5207\u6362\u63a8\u7406\u6a21\u5f0f\u548c\u8bad\u7ec3\u6a21\u5f0f\u3002\u4f8b\u5982\uff0c\u5728\u63a8\u7406\u65f6\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u8fdb\u5165\u63a8\u7406\u6a21\u5f0f\uff1a</p> <pre><code>with torch.inference_mode():\n    # run inference code here\n</code></pre> <p>\u5728\u9000\u51fa\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u4e4b\u540e\uff0cPyTorch \u5c06\u81ea\u52a8\u6062\u590d\u5230\u8bad\u7ec3\u6a21\u5f0f</p> In\u00a0[50]: Copied! <pre># Check the predictions\nprint(f\"Number of testing samples: {len(X_test)}\") \nprint(f\"Number of predictions made: {len(y_preds)}\")\nprint(f\"Predicted values:\\n{y_preds}\")\n</pre> # Check the predictions print(f\"Number of testing samples: {len(X_test)}\")  print(f\"Number of predictions made: {len(y_preds)}\") print(f\"Predicted values:\\n{y_preds}\") <pre>Number of testing samples: 10\nNumber of predictions made: 10\nPredicted values:\ntensor([[0.3982],\n        [0.4049],\n        [0.4116],\n        [0.4184],\n        [0.4251],\n        [0.4318],\n        [0.4386],\n        [0.4453],\n        [0.4520],\n        [0.4588]])\n</pre> <p>Notice how there's one prediction value per testing sample.</p> <p>This is because of the kind of data we're using. For our straight line, one <code>X</code> value maps to one <code>y</code> value.</p> <p>However, machine learning models are very flexible. You could have 100 <code>X</code> values mapping to one, two, three or 10 <code>y</code> values. It all depends on what you're working on.(\u7279\u5f81\u4e0elabel\u4e4b\u95f4\u6ca1\u6709\u8be6\u7ec6\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u770b\u5177\u4f53\u7684\u95ee\u9898\uff09</p> <p>Our predictions are still numbers on a page, let's visualize them with our <code>plot_predictions()</code> function we created above.</p> In\u00a0[51]: Copied! <pre>plot_predictions(predictions=y_preds)\n</pre> plot_predictions(predictions=y_preds) In\u00a0[52]: Copied! <pre>y_test - y_preds#\u8ba1\u7b97\u5dee\u503c\u6709\u591a\u5c11\uff0c\u7136\u540e\u901a\u8fc7\u635f\u5931\u51fd\u6570\uff0c\u5916\u52a0\u53cd\u5411\u4f20\u64ad\u6765\u4f18\u5316\u635f\u5931\u51fd\u6570\uff1a\u5373\u8bad\u7ec3\u8fc7\u7a0b\u3002\n</pre> y_test - y_preds#\u8ba1\u7b97\u5dee\u503c\u6709\u591a\u5c11\uff0c\u7136\u540e\u901a\u8fc7\u635f\u5931\u51fd\u6570\uff0c\u5916\u52a0\u53cd\u5411\u4f20\u64ad\u6765\u4f18\u5316\u635f\u5931\u51fd\u6570\uff1a\u5373\u8bad\u7ec3\u8fc7\u7a0b\u3002 Out[52]: <pre>tensor([[0.4618],\n        [0.4691],\n        [0.4764],\n        [0.4836],\n        [0.4909],\n        [0.4982],\n        [0.5054],\n        [0.5127],\n        [0.5200],\n        [0.5272]])</pre> <p>Woah! Those predictions look pretty bad...</p> <p>This make sense though when you remember our model is just using random parameter values to make predictions.</p> <p>It hasn't even looked at the blue dots to try to predict the green dots.</p> <p>Time to change that.\uff08\u6700\u5f00\u59cb\u7684\u521d\u59cb\u5316\u503c\u80af\u5b9a\u662f\u4e0d\u597d\u7684\uff0c\u63a5\u4e0b\u6765\u5c31\u8fdb\u884c\u4f18\u5316\uff09</p> <p>However, there are some common values, that are known to work well such as the SGD (stochastic gradient descent) or Adam optimizer. And the MAE (mean absolute error) loss function for regression problems (predicting a number) or binary cross entropy loss function for classification problems (predicting one thing or another).</p> <p>For our problem, since we're predicting a number, let's use MAE (which is under <code>torch.nn.L1Loss()</code>) in PyTorch as our loss function.</p> <p> Mean absolute error (MAE, in PyTorch: <code>torch.nn.L1Loss</code>) measures the absolute difference between two points (predictions and labels) and then takes the mean across all examples.</p> <p>And we'll use SGD, <code>torch.optim.SGD(params, lr)</code> where:</p> <ul> <li><code>params</code> is the target model parameters you'd like to optimize (e.g. the <code>weights</code> and <code>bias</code> values we randomly set before).</li> <li><code>lr</code> is the learning rate you'd like the optimizer to update the parameters at, higher means the optimizer will try larger updates (these can sometimes be too large and the optimizer will fail to work), lower means the optimizer will try smaller updates (these can sometimes be too small and the optimizer will take too long to find the ideal values). The learning rate is considered a hyperparameter (because it's set by a machine learning engineer). Common starting values for the learning rate are <code>0.01</code>, <code>0.001</code>, <code>0.0001</code>, however, these can also be adjusted over time (this is called learning rate scheduling).</li> </ul> <p>Woah, that's a lot, let's see it in code.</p> <p>\u5b66\u4e60\u7387\u662f\u4e00\u4e2a\u8d85\u53c2\u6570\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u7684\u65f6\u5019\u9700\u8981\u6ce8\u610f\uff0c\u592a\u9ad8\u548c\u592a\u4f4e\u7684\u5b66\u4e60\u7387\u4e0d\u80fd\u5f97\u5230\u597d\u7684\u7ed3\u679c\uff0c\u5b66\u4e60\u7387\u4e5f\u53ef\u4ee5\u8fdb\u884c\u52a8\u6001\u8bbe\u7f6e\uff0c\u6bd4\u5982\u6700\u5f00\u59cb\u91c7\u7528\u8f83\u5927\u5b66\u4e60\u4e86\uff0c\u5728\u8bad\u7ec3\u4e00\u5b9a\u8f6e\u6b21\u4e4b\u540e\u5c06\u5b66\u4e60\u7387\u964d\u4f4e\u3002</p> In\u00a0[53]: Copied! <pre># Create the loss function\nloss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n\n# Create the optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))\n</pre> # Create the loss function loss_fn = nn.L1Loss() # MAE loss is same as L1Loss  # Create the optimizer optimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize                             lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time)) <p>optimizer.zero_grad()\u662f\u5728\u6bcf\u4e2a\u8fed\u4ee3\uff08epoch\uff09\u5f00\u59cb\u65f6\uff0c\u6e05\u9664\u5df2\u7ecf\u8ba1\u7b97\u7684\u68af\u5ea6\uff0c\u4ee5\u4fbf\u91cd\u65b0\u8ba1\u7b97\u5f53\u524d\u8fed\u4ee3\u7684\u68af\u5ea6\u3002</p> <p>\u4ece\u4e0b\u9762\u7684\u4ee3\u7801\u6765\u770b\uff0c\u5c31\u975e\u5e38\u7684\u76f4\u89c2\uff0c\u77e5\u9053\u6bcf\u4e00\u6b65\u505a\u4e86\u4e9b\u4ec0\u4e48\uff0c\u8863\u670d\u90fd\u8131\u7684\u7f8e\u611f\u3002\u800ctensorflow\u5c31\u662f\u5c01\u88c5\u597d\u7684\u51fd\u6570\uff0c\u4e0d\u4e86\u89e3\u5b83\u5185\u90e8\u90fd\u5728\u505a\u4e9b\u4ec0\u4e48\u4e1c\u897f\u3002</p> In\u00a0[54]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs (how many times the model will pass over the training data)\nepochs = 100\n\n# Create empty loss lists to track values\ntrain_loss_values = []\ntest_loss_values = []\nepoch_count = []\n\nfor epoch in range(epochs):\n    ### Training\n\n    # Put model in training mode (this is the default state of a model) #\u6253\u5f00\u8bad\u7ec3\u5f00\u5173\n    model_0.train()\n\n    # 1. Forward pass on train data using the forward() method inside \n    y_pred = model_0(X_train)\n    # print(y_pred)\n\n    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad of the optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Progress the optimizer\n    optimizer.step()\n\n    ### Testing\n\n    # Put the model in evaluation mode #\u6253\u5f00\u8bc4\u4f30\u5f00\u5173\uff0c\u8fd9\u65f6\u4f1a\u5173\u95ed\u8bad\u7ec3\u76f8\u5173\u7684\u5185\u5bb9\u3002\n    model_0.eval()\n\n    with torch.inference_mode(): #\u63a8\u7406\u6a21\u5f0f\n      # 1. Forward pass on test data\n      test_pred = model_0(X_test)\n\n      # 2. Caculate loss on test data\n      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n\n      # Print out what's happening\n      if epoch % 10 == 0:\n            epoch_count.append(epoch)\n            train_loss_values.append(loss.detach().numpy()) #detach() \u65b9\u6cd5\u5c06\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u8be5\u5f20\u91cf\u4e0e\u539f\u59cb\u5f20\u91cf\u5171\u4eab\u76f8\u540c\u7684\u5e95\u5c42\u6570\u636e\u5b58\u50a8\uff0c\n                                                            #\u5e76\u4e14\u4e0d\u4f1a\u518d\u88ab\u8ddf\u8e2a\u68af\u5ea6\u3002\u56e0\u6b64\uff0c\u5206\u79bb\u51fa\u6765\u7684\u5f20\u91cf\u4e0d\u518d\u5177\u6709\u68af\u5ea6\u4fe1\u606f\uff0c\u5e76\u4e14\u5728\u6267\u884c\u53cd\u5411\u4f20\u64ad\u65f6\u4e0d\u4f1a\u88ab\u66f4\u65b0\u3002\n            test_loss_values.append(test_loss.detach().numpy())\n            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n</pre> torch.manual_seed(42)  # Set the number of epochs (how many times the model will pass over the training data) epochs = 100  # Create empty loss lists to track values train_loss_values = [] test_loss_values = [] epoch_count = []  for epoch in range(epochs):     ### Training      # Put model in training mode (this is the default state of a model) #\u6253\u5f00\u8bad\u7ec3\u5f00\u5173     model_0.train()      # 1. Forward pass on train data using the forward() method inside      y_pred = model_0(X_train)     # print(y_pred)      # 2. Calculate the loss (how different are our models predictions to the ground truth)     loss = loss_fn(y_pred, y_train)      # 3. Zero grad of the optimizer     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Progress the optimizer     optimizer.step()      ### Testing      # Put the model in evaluation mode #\u6253\u5f00\u8bc4\u4f30\u5f00\u5173\uff0c\u8fd9\u65f6\u4f1a\u5173\u95ed\u8bad\u7ec3\u76f8\u5173\u7684\u5185\u5bb9\u3002     model_0.eval()      with torch.inference_mode(): #\u63a8\u7406\u6a21\u5f0f       # 1. Forward pass on test data       test_pred = model_0(X_test)        # 2. Caculate loss on test data       test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type        # Print out what's happening       if epoch % 10 == 0:             epoch_count.append(epoch)             train_loss_values.append(loss.detach().numpy()) #detach() \u65b9\u6cd5\u5c06\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u8be5\u5f20\u91cf\u4e0e\u539f\u59cb\u5f20\u91cf\u5171\u4eab\u76f8\u540c\u7684\u5e95\u5c42\u6570\u636e\u5b58\u50a8\uff0c                                                             #\u5e76\u4e14\u4e0d\u4f1a\u518d\u88ab\u8ddf\u8e2a\u68af\u5ea6\u3002\u56e0\u6b64\uff0c\u5206\u79bb\u51fa\u6765\u7684\u5f20\u91cf\u4e0d\u518d\u5177\u6709\u68af\u5ea6\u4fe1\u606f\uff0c\u5e76\u4e14\u5728\u6267\u884c\u53cd\u5411\u4f20\u64ad\u65f6\u4e0d\u4f1a\u88ab\u66f4\u65b0\u3002             test_loss_values.append(test_loss.detach().numpy())             print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \") <pre>Epoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.48106518387794495 \nEpoch: 10 | MAE Train Loss: 0.1976713240146637 | MAE Test Loss: 0.3463551998138428 \nEpoch: 20 | MAE Train Loss: 0.08908725529909134 | MAE Test Loss: 0.21729660034179688 \nEpoch: 30 | MAE Train Loss: 0.053148526698350906 | MAE Test Loss: 0.14464017748832703 \nEpoch: 40 | MAE Train Loss: 0.04543796554207802 | MAE Test Loss: 0.11360953003168106 \nEpoch: 50 | MAE Train Loss: 0.04167863354086876 | MAE Test Loss: 0.09919948130846024 \nEpoch: 60 | MAE Train Loss: 0.03818932920694351 | MAE Test Loss: 0.08886633068323135 \nEpoch: 70 | MAE Train Loss: 0.03476089984178543 | MAE Test Loss: 0.0805937647819519 \nEpoch: 80 | MAE Train Loss: 0.03132382780313492 | MAE Test Loss: 0.07232122868299484 \nEpoch: 90 | MAE Train Loss: 0.02788739837706089 | MAE Test Loss: 0.06473556160926819 \n</pre> <p>Oh would you look at that! Looks like our loss is going down with every epoch, let's plot it to find out.</p> In\u00a0[55]: Copied! <pre># Plot the loss curves\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n</pre> # Plot the loss curves plt.plot(epoch_count, train_loss_values, label=\"Train loss\") plt.plot(epoch_count, test_loss_values, label=\"Test loss\") plt.title(\"Training and test loss curves\") plt.ylabel(\"Loss\") plt.xlabel(\"Epochs\") plt.legend(); <p>Nice! The loss curves show the loss going down over time. Remember, loss is the measure of how wrong your model is, so the lower the better.</p> <p>But why did the loss go down?</p> <p>Well, thanks to our loss function and optimizer, the model's internal parameters (<code>weights</code> and <code>bias</code>) were updated to better reflect the underlying patterns in the data.</p> <p>Let's inspect our model's <code>.state_dict()</code> to see see how close our model gets to the original values we set for weights and bias.</p> In\u00a0[56]: Copied! <pre># Find our model's learned parameters\nprint(\"The model learned the following values for weights and bias:\")\nprint(model_0.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n</pre> # Find our model's learned parameters print(\"The model learned the following values for weights and bias:\") print(model_0.state_dict()) print(\"\\nAnd the original values for weights and bias are:\") print(f\"weights: {weight}, bias: {bias}\") <pre>The model learned the following values for weights and bias:\nOrderedDict([('weights', tensor([0.5784])), ('bias', tensor([0.3513]))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n</pre> <p>Wow! How cool is that?</p> <p>Our model got very close to calculate the exact original values for <code>weight</code> and <code>bias</code> (and it would probably get even closer if we trained it for longer).</p> <p>Exercise: Try changing the <code>epochs</code> value above to 200, what happens to the loss curves and the weights and bias parameter values of the model?</p> <p>It'd likely never guess them perfectly (especially when using more complicated datasets) but that's okay, often you can do very cool things with a close approximation.</p> <p>This is the whole idea of machine learning and deep learning, there are some ideal values that describe our data and rather than figuring them out by hand, we can train a model to figure them out programmatically.</p> <p>\u4e0a\u9762\u5c31\u5df2\u7ecf\u80fd\u591f\u5c55\u793a\u4e86\u4e00\u4e2a\u6a21\u578b\u8bad\u7ec3\u7684\u5168\u90e8\u8fc7\u7a0b\uff0c\u7b80\u5355\u603b\u7ed3\u4e00\u4e0b\uff1a</p> <ol> <li>\u9996\u5148\u662f\u6784\u5efa\u6570\u636e\uff0c\u5982\u4f55\u5c06\u4ed6\u4eec\u5206\u4e3a\u4e0d\u540c\u7684\u96c6\u5408\u7528\u4e8e\u8bad\u7ec3\uff0c\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u3002pytorch\u6709\u4e13\u95e8\u7684\u51fd\u6570\uff08Dataset\uff09\u6765\u5b58\u50a8\u548c\u5904\u7406\u6570\u636e\uff0c\u4e0d\u8fc7\u672c\u6587\u4e2d\u6ca1\u6709\u6d89\u53ca\u3002</li> <li>\u7b2c\u4e8c\u90e8\u4efd\u5219\u662f\u6784\u5efa\u6a21\u578b\uff1a\u6240\u6709\u7684\u6a21\u578b\u90fd\u662f<code>torch.nn.Module</code>\u7684\u5b50\u7c7b\uff0c\u5728\u6784\u5efa\u6a21\u578b\u65f6\u9700\u8981\u91cd\u5199__init__\u548cforward\u4e24\u4e2a\u51fd\u6570\uff0c\u4e00\u822c\u60c5\u51b5\u4e0b\uff0cinit\u5b9a\u4e49\u6a21\u578b\u7684\u6846\u67b6\uff0c\u800cforward\u5b9a\u4e49\u524d\u5411\u8ba1\u7b97\u8fc7\u7a0b\u3002</li> <li>\u6a21\u578b\u6784\u5efa\u597d\u4e86\u5c31\u8fdb\u5165\u4e86\u8bad\u7ec3\u6b65\u9aa4\uff0c\u5728\u8bad\u7ec3\u4e4b\u524d\u6211\u4eec\u9700\u8981\u6839\u636e\u95ee\u9898\u9009\u62e9\u5408\u9002\u7684\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\uff0c\u5bf9\u4e8e\u4f18\u5316\u5668\u6765\u8bf4\uff0c\u5b66\u4e60\u7387\u662f\u4e00\u4e2a\u5f88\u91cd\u8981\u7684\u8d85\u53c2\u6570\u3002</li> <li>\u8bad\u7ec3\u6b65\u9aa4\u7684\u5199\u6cd5\u4e00\u822c\u662f\u56fa\u5b9a\u7684\uff0c\u6211\u4eec\u8981\u6784\u9020\u4e00\u4e2a\u5faa\u73af\uff0c\u5728\u6bcf\u4e2a\u5faa\u73af\u4e2d\u90fd\u66f4\u65b0\u6a21\u578b\u7684\u53c2\u6570\u5e76\u67e5\u770b\u8868\u73b0\u6548\u679c\uff0c\u4e00\u4e2a\u5faa\u73af\u5206\u4e3a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4e24\u4e2a\u6b65\u9aa4\uff1a\uff08\u4e0a\u9762\u7684\u4ee3\u7801\u5c31\u662f\u5f88\u597d\u7684\u4f8b\u5b50\uff0c\u8fd9\u91cc\u6211\u4eec\u7528\u6587\u5b57\u6765\u8bf4\u4e00\u904d\uff09\uff1a<ul> <li>\u6a21\u578b\u8fdb\u5165\u8bad\u7ec3\u6a21\u5f0f\u540e\uff0c\u5148\u524d\u5411\u4f20\u64ad\uff0c\u7136\u540e\u8ba1\u7b97loss</li> <li>\u6e05\u9664\u6389\u4f18\u5316\u5668\u7684\u68af\u5ea6\u4e4b\u540e\uff0c\u901a\u8fc7loss\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u68af\u5ea6</li> <li>\u7136\u540e\u5728\u901a\u8fc7optimizer.step()\u6765\u66f4\u65b0\u6a21\u578b\u7684\u53c2\u6570</li> <li>\u7136\u540e\u58f0\u660e\u8fdb\u5165\u6d4b\u8bd5\u6a21\u5f0f\uff0c\u5728\u63a8\u7406\u6a21\u5f0f\u4e0b\u83b7\u5f97loss\u6216\u8005\u5176\u4ed6\u7684\u8bc4\u4ef7\u6307\u6807</li> <li>\u8f93\u51fa\u6307\u6807</li> </ul> </li> </ol> In\u00a0[57]: Copied! <pre># 1. Set the model in evaluation mode\nmodel_0.eval()\n\n# 2. Setup the inference mode context manager\nwith torch.inference_mode():\n  # 3. Make sure the calculations are done with the model and data on the same device\n  # in our case, we haven't setup device-agnostic code yet so our data and model are\n  # on the CPU by default.\n  # model_0.to(device)\n  # X_test = X_test.to(device)\n  y_preds = model_0(X_test)\ny_preds\n</pre> # 1. Set the model in evaluation mode model_0.eval()  # 2. Setup the inference mode context manager with torch.inference_mode():   # 3. Make sure the calculations are done with the model and data on the same device   # in our case, we haven't setup device-agnostic code yet so our data and model are   # on the CPU by default.   # model_0.to(device)   # X_test = X_test.to(device)   y_preds = model_0(X_test) y_preds Out[57]: <pre>tensor([[0.8141],\n        [0.8256],\n        [0.8372],\n        [0.8488],\n        [0.8603],\n        [0.8719],\n        [0.8835],\n        [0.8950],\n        [0.9066],\n        [0.9182]])</pre> <p>Nice! We've made some predictions with our trained model, now how do they look?</p> In\u00a0[58]: Copied! <pre>plot_predictions(predictions=y_preds)\n</pre> plot_predictions(predictions=y_preds) <p>Woohoo! Those red dots are looking far closer than they were before!</p> <p>Let's get onto saving an reloading a model in PyTorch.</p> <p>chatgpt\u7684\u63a8\u8350\u65b9\u5f0f\uff1a</p> <p>\u8981\u4fdd\u5b58\u548c\u8bfb\u53d6\u8bad\u7ec3\u597d\u7684PyTorch\u6a21\u578b\uff0c\u53ef\u4ee5\u4f7f\u7528PyTorch\u63d0\u4f9b\u7684\u5185\u7f6e\u51fd\u6570torch.save()\u548ctorch.load()\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u53ef\u4ee5\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c\uff1a</p> <ol> <li><p>\u4fdd\u5b58\u6a21\u578b\uff1a\u53ef\u4ee5\u4f7f\u7528torch.save()\u51fd\u6570\u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4fdd\u5b58\u5230\u672c\u5730\u6587\u4ef6\u4e2d\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u6309\u7167\u5982\u4e0b\u65b9\u5f0f\u4fdd\u5b58\u6a21\u578b\uff1a</p> <pre><code>torch.save(model.state_dict(), 'model.pth')\n</code></pre> <p>\u8fd9\u91cc\uff0cmodel\u662f\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0cstate_dict()\u65b9\u6cd5\u8fd4\u56de\u6a21\u578b\u7684\u53c2\u6570\u5b57\u5178\uff0c'model.pth'\u662f\u4fdd\u5b58\u6a21\u578b\u7684\u6587\u4ef6\u540d\u3002</p> </li> <li><p>\u8bfb\u53d6\u6a21\u578b\uff1a\u53ef\u4ee5\u4f7f\u7528torch.load()\u51fd\u6570\u4ece\u672c\u5730\u6587\u4ef6\u4e2d\u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u6309\u7167\u5982\u4e0b\u65b9\u5f0f\u52a0\u8f7d\u6a21\u578b\uff1a</p> <pre><code>model.load_state_dict(torch.load('model.pth'))\n</code></pre> <p>\u8fd9\u91cc\uff0c'model.pth'\u662f\u4fdd\u5b58\u6a21\u578b\u7684\u6587\u4ef6\u540d\uff0cmodel\u662f\u8981\u52a0\u8f7d\u6a21\u578b\u7684\u5bf9\u8c61\uff0cload_state_dict()\u65b9\u6cd5\u7528\u4e8e\u52a0\u8f7d\u6a21\u578b\u7684\u53c2\u6570\u5b57\u5178\u3002</p> </li> </ol> <p>\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728\u52a0\u8f7d\u6a21\u578b\u4e4b\u524d\uff0c\u9700\u8981\u5148\u521b\u5efa\u4e00\u4e2a\u4e0e\u539f\u6a21\u578b\u7ed3\u6784\u76f8\u540c\u7684\u65b0\u6a21\u578b\uff0c\u7136\u540e\u518d\u5c06\u8bfb\u53d6\u7684\u53c2\u6570\u8d4b\u503c\u7ed9\u65b0\u6a21\u578b\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u4ee3\u7801\u5b9e\u73b0\uff1a</p> <pre><code># \u5b9a\u4e49\u65b0\u6a21\u578b\nnew_model = MyModelClass()\n\n# \u52a0\u8f7d\u6a21\u578b\u53c2\u6570\nnew_model.load_state_dict(torch.load('model.pth'))\n</code></pre> <p>\u5176\u4e2d\uff0cMyModelClass()\u662f\u539f\u6a21\u578b\u7684\u7c7b\u540d\uff0cnew_model\u662f\u65b0\u6a21\u578b\u7684\u5bf9\u8c61\u540d\u3002\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u786e\u4fdd\u65b0\u6a21\u578b\u4e0e\u539f\u6a21\u578b\u7684\u7ed3\u6784\u76f8\u540c\uff0c\u4ece\u800c\u53ef\u4ee5\u6b63\u786e\u52a0\u8f7d\u53c2\u6570\u3002</p> In\u00a0[59]: Copied! <pre># from pathlib import Path\n\n# # 1. Create models directory \n# MODEL_PATH = Path(\"models\")\n# MODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# # 2. Create model save path \n# MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n# MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# # 3. Save the model state dict \n# print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n# torch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n#            f=MODEL_SAVE_PATH)\n</pre> # from pathlib import Path  # # 1. Create models directory  # MODEL_PATH = Path(\"models\") # MODEL_PATH.mkdir(parents=True, exist_ok=True)  # # 2. Create model save path  # MODEL_NAME = \"01_pytorch_workflow_model_0.pth\" # MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # # 3. Save the model state dict  # print(f\"Saving model to: {MODEL_SAVE_PATH}\") # torch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters #            f=MODEL_SAVE_PATH)  In\u00a0[60]: Copied! <pre># # Check the saved file path\n# !ls -l models/01_pytorch_workflow_model_0.pth\n</pre> # # Check the saved file path # !ls -l models/01_pytorch_workflow_model_0.pth In\u00a0[61]: Copied! <pre># # Instantiate a new instance of our model (this will be instantiated with random weights)\n# loaded_model_0 = LinearRegressionModel()\n\n# # Load the state_dict of our saved model (this will update the new instance of our model with trained weights)\n# loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n</pre> # # Instantiate a new instance of our model (this will be instantiated with random weights) # loaded_model_0 = LinearRegressionModel()  # # Load the state_dict of our saved model (this will update the new instance of our model with trained weights) # loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH)) <p>Excellent! It looks like things matched up.</p> <p>Now to test our loaded model, let's perform inference with it (make predictions) on the test data.</p> <p>Remember the rules for performing inference with PyTorch models?</p> <p>If not, here's a refresher:</p> PyTorch inference rules <ol> <li> Set the model in evaluation mode (<code>model.eval()</code>). </li> <li> Make the predictions using the inference mode context manager (<code>with torch.inference_mode(): ...</code>). </li> <li> All predictions should be made with objects on the same device (e.g. data and model on GPU only or data and model on CPU only).</li> </ol> In\u00a0[62]: Copied! <pre># # 1. Put the loaded model into evaluation mode\n# loaded_model_0.eval()\n\n# # 2. Use the inference mode context manager to make predictions\n# with torch.inference_mode():\n#     loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model\n</pre> # # 1. Put the loaded model into evaluation mode # loaded_model_0.eval()  # # 2. Use the inference mode context manager to make predictions # with torch.inference_mode(): #     loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model <p>Now we've made some predictions with the loaded model, let's see if they're the same as the previous predictions.</p> In\u00a0[63]: Copied! <pre># Compare previous model predictions with loaded model predictions (these should be the same)\n# y_preds == loaded_model_preds\n</pre> # Compare previous model predictions with loaded model predictions (these should be the same) # y_preds == loaded_model_preds <p>Nice!</p> <p>It looks like the loaded model predictions are the same as the previous model predictions (predictions made prior to saving). This indicates our model is saving and loading as expected.</p> <p>Note: There are more methods to save and load PyTorch models but I'll leave these for extra-curriculum and further reading. See the PyTorch guide for saving and loading models for more.</p> In\u00a0[64]: Copied! <pre># Import PyTorch and matplotlib\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n</pre> # Import PyTorch and matplotlib import torch from torch import nn # nn contains all of PyTorch's building blocks for neural networks import matplotlib.pyplot as plt  # Check PyTorch version torch.__version__ Out[64]: <pre>'2.0.0'</pre> <p>Now let's start making our code device agnostic by setting <code>device=\"cuda\"</code> if it's available, otherwise it'll default to <code>device=\"cpu\"</code>.</p> In\u00a0[65]: Copied! <pre># Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n</pre> # Setup device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Using device: {device}\") <pre>Using device: cuda\n</pre> <p>If you've got access to a GPU, the above should've printed out:</p> <pre><code>Using device: cuda\n</code></pre> <p>Otherwise, you'll be using a CPU for the following computations. This is fine for our small dataset but it will take longer for larger datasets.</p> In\u00a0[66]: Copied! <pre># Create weight and bias\nweight = 0.7\nbias = 0.3\n\n# Create range values\nstart = 0\nend = 1\nstep = 0.02\n\n# Create X and y (features and labels)\nX = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\ny = weight * X + bias \nX[:10], y[:10]\n</pre> # Create weight and bias weight = 0.7 bias = 0.3  # Create range values start = 0 end = 1 step = 0.02  # Create X and y (features and labels) X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers) y = weight * X + bias  X[:10], y[:10] Out[66]: <pre>(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))</pre> <p>Wonderful!</p> <p>Now we've got some data, let's split it into training and test sets.</p> <p>We'll use an 80/20 split with 80% training data and 20% testing data.</p> In\u00a0[67]: Copied! <pre># Split data\ntrain_split = int(0.8 * len(X))\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Split data train_split = int(0.8 * len(X)) X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]  len(X_train), len(y_train), len(X_test), len(y_test) Out[67]: <pre>(40, 40, 10, 10)</pre> <p>Excellent, let's visualize them to make sure they look okay.</p> In\u00a0[68]: Copied! <pre># Note: If you've reset your runtime, this function won't work, \n# you'll have to rerun the cell above where it's instantiated.\nplot_predictions(X_train, y_train, X_test, y_test)\n</pre> # Note: If you've reset your runtime, this function won't work,  # you'll have to rerun the cell above where it's instantiated. plot_predictions(X_train, y_train, X_test, y_test) In\u00a0[69]: Copied! <pre># Subclass nn.Module to make our model\nclass LinearRegressionModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use nn.Linear() for creating the model parameters\n        self.linear_layer = nn.Linear(in_features=1, \n                                      out_features=1)\n    \n    # Define the forward computation (input data x flows through nn.Linear())\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear_layer(x)\n\n# Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens)\ntorch.manual_seed(42)\nmodel_1 = LinearRegressionModelV2()\nmodel_1, model_1.state_dict()\n</pre> # Subclass nn.Module to make our model class LinearRegressionModelV2(nn.Module):     def __init__(self):         super().__init__()         # Use nn.Linear() for creating the model parameters         self.linear_layer = nn.Linear(in_features=1,                                        out_features=1)          # Define the forward computation (input data x flows through nn.Linear())     def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         return self.linear_layer(x)  # Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens) torch.manual_seed(42) model_1 = LinearRegressionModelV2() model_1, model_1.state_dict() Out[69]: <pre>(LinearRegressionModelV2(\n   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n ),\n OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n              ('linear_layer.bias', tensor([0.8300]))]))</pre> <p>Notice the outputs of <code>model_1.state_dict()</code>, the <code>nn.Linear()</code> layer created a random <code>weight</code> and <code>bias</code> parameter for us.</p> <p>Now let's put our model on the GPU (if it's available).</p> <p>We can change the device our PyTorch objects are on using <code>.to(device)</code>.</p> <p>First let's check the model's current device.</p> In\u00a0[70]: Copied! <pre># Check model device\nnext(model_1.parameters()).device\n</pre> # Check model device next(model_1.parameters()).device Out[70]: <pre>device(type='cpu')</pre> <p>Wonderful, looks like the model's on the CPU by default.</p> <p>Let's change it to be on the GPU (if it's available).</p> In\u00a0[71]: Copied! <pre># Set model to GPU if it's availalble, otherwise it'll default to CPU\nmodel_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not\nnext(model_1.parameters()).device\n</pre> # Set model to GPU if it's availalble, otherwise it'll default to CPU model_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not next(model_1.parameters()).device Out[71]: <pre>device(type='cuda', index=0)</pre> <p>Nice! Because of our device agnostic code, the above cell will work regardless of whether a GPU is available or not.</p> <p>If you do have access to a CUDA-enabled GPU, you should see an output of something like:</p> <pre><code>device(type='cuda', index=0)\n</code></pre> <p>Time to build a training and testing loop.</p> <p>First we'll need a loss function and an optimizer.</p> <p>Let's use the same functions we used earlier, <code>nn.L1Loss()</code> and <code>torch.optim.SGD()</code>.</p> <p>We'll have to pass the new model's parameters (<code>model.parameters()</code>) to the optimizer for it to adjust them during training.</p> <p>The learning rate of <code>0.1</code> worked well before too so let's use that again.</p> In\u00a0[72]: Copied! <pre># Create loss function\nloss_fn = nn.L1Loss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n                            lr=0.01)\n</pre> # Create loss function loss_fn = nn.L1Loss()  # Create optimizer optimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters                             lr=0.01) <p>Beautiful, loss function and optimizer ready, now let's train and evaluate our model using a training and testing loop.</p> <p>The only different thing we'll be doing in this step compared to the previous training loop is putting the data on the target <code>device</code>.</p> <p>We've already put our model on the target <code>device</code> using <code>model_1.to(device)</code>.</p> <p>And we can do the same with the data.</p> <p>That way if the model is on the GPU, the data is on the GPU (and vice versa).</p> <p>Let's step things up a notch this time and set <code>epochs=1000</code>.</p> <p>If you need a reminder of the PyTorch training loop steps, see below.</p> PyTorch training loop steps <ol> <li>Forward pass - The model goes through all of the training data once, performing its             <code>forward()</code> function             calculations (<code>model(x_train)</code>).         </li> <li>Calculate the loss - The model's outputs (predictions) are compared to the ground truth and evaluated             to see how             wrong they are (<code>loss = loss_fn(y_pred, y_train</code>).</li> <li>Zero gradients - The optimizers gradients are set to zero (they are accumulated by default) so they             can be             recalculated for the specific training step (<code>optimizer.zero_grad()</code>).</li> <li>Perform backpropagation on the loss - Computes the gradient of the loss with respect for every model             parameter to             be updated (each parameter             with <code>requires_grad=True</code>). This is known as backpropagation, hence \"backwards\"             (<code>loss.backward()</code>).</li> <li>Step the optimizer (gradient descent) - Update the parameters with <code>requires_grad=True</code>             with respect to the loss             gradients in order to improve them (<code>optimizer.step()</code>).</li> </ol> In\u00a0[73]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs \nepochs = 1000 \n\n# Put data on the available device\n# Without this, error will happen (not all model/data on device)\nX_train = X_train.to(device)\nX_test = X_test.to(device)\ny_train = y_train.to(device)\ny_test = y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_1.train() # train mode is on by default after construction\n\n    # 1. Forward pass\n    y_pred = model_1(X_train)\n\n    # 2. Calculate loss\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Step the optimizer\n    optimizer.step()\n\n    ### Testing\n    model_1.eval() # put the model in evaluation mode for testing (inference)\n    # 1. Forward pass\n    with torch.inference_mode():\n        test_pred = model_1(X_test)\n    \n        # 2. Calculate the loss\n        test_loss = loss_fn(test_pred, y_test)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n</pre> torch.manual_seed(42)  # Set the number of epochs  epochs = 1000   # Put data on the available device # Without this, error will happen (not all model/data on device) X_train = X_train.to(device) X_test = X_test.to(device) y_train = y_train.to(device) y_test = y_test.to(device)  for epoch in range(epochs):     ### Training     model_1.train() # train mode is on by default after construction      # 1. Forward pass     y_pred = model_1(X_train)      # 2. Calculate loss     loss = loss_fn(y_pred, y_train)      # 3. Zero grad optimizer     optimizer.zero_grad()      # 4. Loss backward     loss.backward()      # 5. Step the optimizer     optimizer.step()      ### Testing     model_1.eval() # put the model in evaluation mode for testing (inference)     # 1. Forward pass     with torch.inference_mode():         test_pred = model_1(X_test)              # 2. Calculate the loss         test_loss = loss_fn(test_pred, y_test)      if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\") <pre>Epoch: 0 | Train loss: 0.5551779866218567 | Test loss: 0.5739762187004089\nEpoch: 100 | Train loss: 0.006215683650225401 | Test loss: 0.014086711220443249\nEpoch: 200 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 300 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 400 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 500 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 600 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 700 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 800 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 900 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\n</pre> <p>Note: Due to the random nature of machine learning, you will likely get slightly different results (different loss and prediction values) depending on whether your model was trained on CPU or GPU. This is true even if you use the same random seed on either device. If the difference is large, you may want to look for errors, however, if it is small (ideally it is), you can ignore it.</p> <p>Nice! That loss looks pretty low.</p> <p>Let's check the parameters our model has learned and compare them to the original parameters we hard-coded.</p> In\u00a0[74]: Copied! <pre># Find our model's learned parameters\nfrom pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \nprint(\"The model learned the following values for weights and bias:\")\npprint(model_1.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n</pre> # Find our model's learned parameters from pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html  print(\"The model learned the following values for weights and bias:\") pprint(model_1.state_dict()) print(\"\\nAnd the original values for weights and bias are:\") print(f\"weights: {weight}, bias: {bias}\") <pre>The model learned the following values for weights and bias:\nOrderedDict([('linear_layer.weight', tensor([[0.6968]], device='cuda:0')),\n             ('linear_layer.bias', tensor([0.3025], device='cuda:0'))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n</pre> <p>Ho ho! Now that's pretty darn close to a perfect model.</p> <p>Remember though, in practice, it's rare that you'll know the perfect parameters ahead of time.</p> <p>And if you knew the parameters your model had to learn ahead of time, what would be the fun of machine learning?</p> <p>Plus, in many real-world machine learning problems, the number of parameters can well exceed tens of millions.</p> <p>I don't know about you but I'd rather write code for a computer to figure those out rather than doing it by hand.</p> In\u00a0[75]: Copied! <pre># Turn model into evaluation mode\nmodel_1.eval()\n\n# Make predictions on the test data\nwith torch.inference_mode():\n    y_preds = model_1(X_test)\ny_preds\n</pre> # Turn model into evaluation mode model_1.eval()  # Make predictions on the test data with torch.inference_mode():     y_preds = model_1(X_test) y_preds Out[75]: <pre>tensor([[0.8600],\n        [0.8739],\n        [0.8878],\n        [0.9018],\n        [0.9157],\n        [0.9296],\n        [0.9436],\n        [0.9575],\n        [0.9714],\n        [0.9854]], device='cuda:0')</pre> <p>If you're making predictions with data on the GPU, you might notice the output of the above has <code>device='cuda:0'</code> towards the end. That means the data is on CUDA device 0 (the first GPU your system has access to due to zero-indexing), if you end up using multiple GPUs in the future, this number may be higher.</p> <p>Now let's plot our model's predictions.</p> <p>Note: Many data science libraries such as pandas, matplotlib and NumPy aren't capable of using data that is stored on GPU. So you might run into some issues when trying to use a function from one of these libraries with tensor data not stored on the CPU. To fix this, you can call <code>.cpu()</code> on your target tensor to return a copy of your target tensor on the CPU.</p> In\u00a0[76]: Copied! <pre># plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU\n\n# Put data on the CPU and plot it\nplot_predictions(predictions=y_preds.cpu())\n</pre> # plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU  # Put data on the CPU and plot it plot_predictions(predictions=y_preds.cpu()) <p>Woah! Look at those red dots, they line up almost perfectly with the green dots. I guess the extra epochs helped.</p> In\u00a0[77]: Copied! <pre># from pathlib import Path\n\n# # 1. Create models directory \n# MODEL_PATH = Path(\"models\")\n# MODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# # 2. Create model save path \n# MODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\n# MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# # 3. Save the model state dict \n# print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n# torch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n#            f=MODEL_SAVE_PATH)\n</pre> # from pathlib import Path  # # 1. Create models directory  # MODEL_PATH = Path(\"models\") # MODEL_PATH.mkdir(parents=True, exist_ok=True)  # # 2. Create model save path  # MODEL_NAME = \"01_pytorch_workflow_model_1.pth\" # MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # # 3. Save the model state dict  # print(f\"Saving model to: {MODEL_SAVE_PATH}\") # torch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters #            f=MODEL_SAVE_PATH)  <p>And just to make sure everything worked well, let's load it back in.</p> <p>We'll:</p> <ul> <li>Create a new instance of the <code>LinearRegressionModelV2()</code> class</li> <li>Load in the model state dict using <code>torch.nn.Module.load_state_dict()</code></li> <li>Send the new instance of the model to the target device (to ensure our code is device-agnostic)</li> </ul> In\u00a0[78]: Copied! <pre># # Instantiate a fresh instance of LinearRegressionModelV2\n# loaded_model_1 = LinearRegressionModelV2()\n\n# # Load model state dict \n# loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\n# # Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\n# loaded_model_1.to(device)\n\n# print(f\"Loaded model:\\n{loaded_model_1}\")\n# print(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")\n</pre> # # Instantiate a fresh instance of LinearRegressionModelV2 # loaded_model_1 = LinearRegressionModelV2()  # # Load model state dict  # loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))  # # Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions) # loaded_model_1.to(device)  # print(f\"Loaded model:\\n{loaded_model_1}\") # print(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\") <p>Now we can evaluate the loaded model to see if its predictions line up with the predictions made prior to saving.</p> In\u00a0[79]: Copied! <pre># # Evaluate loaded model\n# loaded_model_1.eval()\n# with torch.inference_mode():\n#     loaded_model_1_preds = loaded_model_1(X_test)\n# y_preds == loaded_model_1_preds\n</pre> # # Evaluate loaded model # loaded_model_1.eval() # with torch.inference_mode(): #     loaded_model_1_preds = loaded_model_1(X_test) # y_preds == loaded_model_1_preds <p>Everything adds up! Nice!</p> <p>Well, we've come a long way. You've now built and trained your first two neural network models in PyTorch!</p> <p>Time to practice your skills.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#01-pytorch-workflow-fundamentals","title":"01. PyTorch Workflow Fundamentals\u00b6","text":""},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/","title":"\u524d\u8a00\u00b6","text":"<p>\u5728\u5199\u5b8c\u7b2c\u4e00\u7ae0\u4e4b\u540e\u6211\u53d1\u73b0\uff0c\u7ecf\u8fc7\u81ea\u5df1\u7684\u6574\u7406\u4e4b\u540e\u7ed3\u679c\u5e76\u4e0d\u591f\u597d\uff08\u8fc7\u6ee4\u4e00\u904d\u4e4b\u540e\uff0c\u77e5\u8bc6\u53d8\u5c11\u4e86\uff1f\uff09\uff0c\u6240\u4ee5\u9700\u8981\u6362\u79cd\u65b9\u5f0f\u3002\u5c1d\u8bd5\u4e00\u4e0b\u6279\u6ce8\u7684\u5f62\u5f0f\uff1a\u5728\u522b\u4eba\u5199\u597d\u7684\u5185\u5bb9\u4e0b\u6dfb\u52a0\u81ea\u5df1\u7684\u89c1\u89e3\u3002</p> <p>The essence of machine learning and deep learning is to take some data from the past, build an algorithm (like a neural network) to discover patterns in it and use the discoverd patterns to predict the future.\uff08\u8fd9\u91cc\u6982\u62ec\u6765\u8bf4\u5c31\u662f\uff1a\u901a\u8fc7\u73b0\u6709\u7684\u6570\u636e\u5b66\u4e60\u5176\u4e2d\u7684\u7279\u5f81\uff0c\u7136\u540e\u5e94\u7528\u5230\u65b0\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u6d4b\uff09</p> <p>There are many ways to do this and many new ways are being discovered all the time.\uff08\u4e3b\u8981\u662f\u7b97\u6cd5\u89d2\u5ea6\u7684\u66f4\u65b0\uff0c\u4e0d\u65ad\u6709\u65b0\u7684\u6a21\u578b\u63d0\u51fa\u6765\uff09</p> <p>But let's start small.</p> <p>How about we start with a straight line?\uff08\u5982\u4f55\u62df\u5408\u4e00\u4e2a\u76f4\u7ebf\uff1f\uff0c\u8fd9\u5e94\u8be5\u662f\u975e\u5e38\u7b80\u5355\u7684\uff0c\u56e0\u4e3a\u76f4\u7ebf\u5df2\u7ecf\u6709\u5b8c\u7f8e\u7684\u516c\u5f0f\u53ef\u4ee5\u62df\u5408\u51fa\u6765\uff09</p> <p>And we see if we can build a PyTorch model that learns the pattern of the straight line and matches it.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>In this module we're going to cover a standard PyTorch workflow (it can be chopped and changed as necessary but it covers the main outline of steps).</p> <p>\u5173\u952e\u7684\u6b65\u9aa4\u53ef\u4ee5\u5206\u4e3a\uff1a1.\u6570\u636e\uff0c2.\u6a21\u578b\uff0c3.\u4f18\u5316\uff0c4.\u8bc4\u4ef7\uff0c\u6bcf\u4e00\u6b65\u90fd\u5f88\u91cd\u8981\uff0c\u6570\u636e\u65f6\u6700\u57fa\u7840\u7684\uff0c\u6ca1\u6709\u6570\u636e\u6216\u8005\u6570\u636e\u5219\u4e0d\u80fd\u6784\u5efa\u6709\u7528\u7684\u6a21\u578b\uff0cchatgpt\u7684\u5f3a\u5927\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u662f\u56e0\u4e3a\u5b83\u4f7f\u7528\u4e86\u8db3\u591f\u591a\u7684\u8bad\u7ec3\u6570\u636e\u3002\u6a21\u578b\u4e5f\u5f88\u91cd\u8981\uff0c\u4e0d\u540c\u7684\u6a21\u578b\u53ef\u80fd\u9002\u7528\u7740\u4e0d\u540c\u7684\u95ee\u9898\uff0c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5c31\u662f\u80fd\u591f\u5728\u56fe\u7247\u5904\u7406\u4e0a\u4f18\u4e8e\u5176\u4ed6\u7684\u6846\u67b6\uff0c\u800c\u6700\u8fd1\u5f88\u706b\u7684Transformer\u5728\u5404\u79cd\u95ee\u9898\u4e0a\u90fd\u62e5\u6709\u8005\u5f88\u5f3a\u7684\u5b66\u4e60\u80fd\u529b\u3002\u4f18\u5316\u548c\u8bc4\u4ef7\u662f\u8f83\u5f31\u7684\u4e24\u9879\uff0c\u4e0d\u8fc7\u8fd9\u5bf9\u53d1\u6587\u7ae0\u5f88\u91cd\u8981\uff0c\u56e0\u4e3a\u4f60\u8981\u8bc1\u660e\u5728\u76f8\u540c\u7684\u8bc4\u4ef7\u6807\u51c6\u4e0b\uff0c\u4f60\u7684\u6a21\u578b\u8981\u597d\u4e8e\u5176\u4ed6\u7684\u5de5\u5177\u624d\u80fd\u5f97\u5230\u522b\u4eba\u7684\u8ba4\u53ef\u3002</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#where-can-can-you-get-help","title":"Where can can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>And if you run into trouble, you can ask a question on the Discussions page there too.</p> <p>There's also the PyTorch developer forums, a very helpful place for all things PyTorch.</p> <p>Let's start by putting what we're covering into a dictionary to reference later.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#1-data-preparing-and-loading","title":"1. Data (preparing and loading)\u00b6","text":"<p>I want to stress that \"data\" in machine learning can be almost anything you can imagine. A table of numbers (like a big Excel spreadsheet), images of any kind, videos (YouTube has lots of data!), audio files like songs or podcasts, protein structures, text and more.</p> <p></p> <p>Machine learning is a game of two parts:</p> <ol> <li>Turn your data, whatever it is, into numbers (a representation).</li> <li>Pick or build a model to learn the representation as best as possible.</li> </ol>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#split-data-into-training-and-test-sets","title":"Split data into training and test sets\u00b6","text":"<p>We've got some data.</p> <p>But before we build a model we need to split it up.</p> <p>One of most important steps in a machine learning project is creating a training and test set (and when required, a validation set).</p> <p>Each split of the dataset serves a specific purpose:</p> Split Purpose Amount of total data How often is it used? Training set The model learns from this data (like the course materials you study during the semester). ~60-80% Always Validation set The model gets tuned on this data (like the practice exam you take before the final exam). ~10-20% Often but not always Testing set The model gets evaluated on this data to test what it has learned (like the final exam you take at the end of the semester). ~10-20% Always <p>\u8fd9\u91cc\u5c31\u5f15\u5165\u4e86\u4e09\u4e2a\u6982\u5ff5\uff0c\u8bad\u7ec3\u96c6\uff08training dataset\uff09\u3001\u9a8c\u8bc1\u96c6\uff08validation dataset\uff09\u548c\u6d4b\u8bd5\u96c6\uff08testing dataset\uff09\u3002\u6709\u65f6\u5019\u4f1a\u6df7\u6dc6\u540e\u4e24\u4e2a\u6982\u5ff5\uff08\u5305\u62ec\u53d1\u8868\u7684\u6587\u7ae0\u4e2d\uff09\uff0c\u6240\u4ee5\u8fd9\u91cc\u5c31\u660e\u786e\u6309\u7167\u4e0a\u9762\u7684\u63cf\u8ff0\u6765\uff0c\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u8bad\u7ec3\u7684\u6a21\u578b\u4f1a\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8c03\u4f18\uff0c\u6240\u4ee5\u5b83\u4e0d\u662f\u72ec\u7acb\u7684\uff0c\u6a21\u578b\u5728\u5b83\u4e0a\u9762\u8868\u73b0\u597d\u4e0d\u4e00\u5b9a\u662f\u771f\u6b63\u7684\u8868\u73b0\u597d\u3002test\u4e0a\u8981\u662f\u5b8c\u5168\u72ec\u7acb\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u6837\u624d\u80fd\u8bc1\u660e\u6a21\u578b\u6700\u7ec8\u7684\u8868\u73b0\u6548\u679c\u3002</p> <p>For now, we'll just use a training and test set, this means we'll have a dataset for our model to learn on as well as be evaluated on.</p> <p>We can create them by splitting our <code>X</code> and <code>y</code> tensors.</p> <p>Note: When dealing with real-world data, this step is typically done right at the start of a project (the test set should always be kept separate from all other data). We want our model to learn on training data and then evaluate it on test data to get an indication of how well it generalizes to unseen examples.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#2-build-model","title":"2. Build model\u00b6","text":"<p>Now we've got some data, let's build a model to use the blue dots to predict the green dots.</p> <p>We're going to jump right in.</p> <p>We'll write the code first and then explain everything.</p> <p>Let's replicate a standard linear regression model using pure PyTorch.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#pytorch-model-building-essentials","title":"PyTorch model building essentials\u00b6","text":"<p>PyTorch has four (give or take) essential modules you can use to create almost any kind of neural network you can imagine.</p> <p>They are <code>torch.nn</code>, <code>torch.optim</code>, <code>torch.utils.data.Dataset</code> and <code>torch.utils.data.DataLoader</code>. For now, we'll focus on the first two and get to the other two later (though you may be able to guess what they do).</p> PyTorch module What does it do? <code>torch.nn</code> Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way). <code>torch.nn.Parameter</code> Stores tensors that can be used with <code>nn.Module</code>. If <code>requires_grad=True</code> gradients (used for updating model parameters via gradient descent)  are calculated automatically, this is often referred to as \"autograd\". <code>torch.nn.Module</code> The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass <code>nn.Module</code>. Requires a <code>forward()</code> method be implemented. <code>torch.optim</code> Contains various optimization algorithms (these tell the model parameters stored in <code>nn.Parameter</code> how to best change to improve gradient descent and in turn reduce the loss). <code>def forward()</code> All <code>nn.Module</code> subclasses require a <code>forward()</code> method, this defines the computation that will take place on the data passed to the particular <code>nn.Module</code> (e.g. the linear regression formula above). <p>If the above sounds complex, think of like this, almost everything in a PyTorch neural network comes from <code>torch.nn</code>,</p> <ul> <li><code>nn.Module</code> contains the larger building blocks (layers)</li> <li><code>nn.Parameter</code> contains the smaller parameters like weights and biases (put these together to make <code>nn.Module</code>(s))</li> <li><code>forward()</code> tells the larger blocks how to make calculations on inputs (tensors full of data) within  <code>nn.Module</code>(s)</li> <li><code>torch.optim</code> contains optimization methods on how to improve the parameters within <code>nn.Parameter</code> to better represent input data</li> </ul> <p> Basic building blocks of creating a PyTorch model by subclassing <code>nn.Module</code>. For objects that subclass <code>nn.Module</code>, the <code>forward()</code> method must be defined.</p> <p>Resource: See more of these essential modules and their uses cases in the PyTorch Cheat Sheet.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#checking-the-contents-of-a-pytorch-model","title":"Checking the contents of a PyTorch model\u00b6","text":"<p>Now we've got these out of the way, let's create a model instance with the class we've made and check its parameters using <code>.parameters()</code>.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#making-predictions-using-torchinference_mode","title":"Making predictions using <code>torch.inference_mode()</code>\u00b6","text":"<p>To check this we can pass it the test data <code>X_test</code> to see how closely it predicts <code>y_test</code>.</p> <p>When we pass data to our model, it'll go through the model's <code>forward()</code> method and produce a result using the computation we've defined.</p> <p>Let's make some predictions.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#3-train-model","title":"3. Train model\u00b6","text":"<p>Right now our model is making predictions using random parameters to make calculations, it's basically guessing (randomly).</p> <p>To fix that, we can update its internal parameters (I also refer to parameters as patterns), the <code>weights</code> and <code>bias</code> values we set randomly using <code>nn.Parameter()</code> and <code>torch.randn()</code> to be something that better represents the data.</p> <p>We could hard code this (since we know the default values <code>weight=0.7</code> and <code>bias=0.3</code>) but where's the fun in that?</p> <p>Much of the time you won't know what the ideal parameters are for a model.</p> <p>Instead, it's much more fun to write code to see if the model can try and figure them out itself.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#creating-a-loss-function-and-optimizer-in-pytorch","title":"Creating a loss function and optimizer in PyTorch\u00b6","text":"<p>For our model to update its parameters on its own, we'll need to add a few more things to our recipe.</p> <p>And that's a loss function as well as an optimizer.</p> <p>The rolls of these are:</p> Function What does it do? Where does it live in PyTorch? Common values Loss function Measures how wrong your models predictions (e.g. <code>y_preds</code>) are compared to the truth labels (e.g. <code>y_test</code>). Lower the better. PyTorch has plenty of built-in loss functions in <code>torch.nn</code>. Mean absolute error (MAE) for regression problems (<code>torch.nn.L1Loss()</code>). Binary cross entropy for binary classification problems (<code>torch.nn.BCELoss()</code>). Optimizer Tells your model how to update its internal parameters to best lower the loss. You can find various optimization function implementations in <code>torch.optim</code>. Stochastic gradient descent (<code>torch.optim.SGD()</code>). Adam optimizer (<code>torch.optim.Adam()</code>). <p>Let's create a loss function and an optimizer we can use to help improve our model.</p> <p>Depending on what kind of problem you're working on will depend on what loss function and what optimizer you use.</p> <p>\u635f\u5931\u51fd\u6570\u8ba1\u7b97\u503c\u4e0e\u9519\u8bef\u503c\u4e4b\u95f4\u7684\u5dee\u8ddd\u6709\u591a\u5c11\uff0c\u800c\u4f18\u5316\u5668\u8003\u8651\u91c7\u7528\u4ec0\u4e48\u6837\u7684\u7b56\u7565\u6765\u964d\u4f4e\u635f\u5931\u51fd\u6570\uff0c\u5728pytorch\u4e2d\u6709\u5f88\u591a\u7684\u7c7b\u522b\u6765\u9002\u5e94\u4e0d\u7528\u7684\u95ee\u9898\u3002</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#creating-an-optimization-loop-in-pytorch","title":"Creating an optimization loop in PyTorch\u00b6","text":"<p>Woohoo! Now we've got a loss function and an optimizer, it's now time to create a training loop (and testing loop).</p> <p>The training loop involves the model going through the training data and learning the relationships between the <code>features</code> and <code>labels</code>.</p> <p>The testing loop involves going through the testing data and evaluating how good the patterns are that the model learned on the training data (the model never see's the testing data during training).</p> <p>Each of these is called a \"loop\" because we want our model to look (loop through) at each sample in each dataset.</p> <p>To create these we're going to write a Python <code>for</code> loop in the theme of the unofficial PyTorch optimization loop song (there's a video version too).</p> <p> The unoffical PyTorch optimization loops song, a fun way to remember the steps in a PyTorch training (and testing) loop.</p> <p>There will be a fair bit of code but nothing we can't handle.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#pytorch-training-loop","title":"PyTorch training loop\u00b6","text":"<p>For the training loop, we'll build the following steps:</p> Number Step name What does it do? Code example 1 Forward pass The model goes through all of the training data once, performing its <code>forward()</code> function calculations. <code>model(x_train)</code> 2 Calculate the loss The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. <code>loss = loss_fn(y_pred, y_train)</code> 3 Zero gradients The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step. <code>optimizer.zero_grad()</code> 4 Perform backpropagation on the loss Computes the gradient of the loss with respect for every model parameter to be updated  (each parameter with <code>requires_grad=True</code>). This is known as backpropagation, hence \"backwards\". <code>loss.backward()</code> 5 Update the optimizer (gradient descent) Update the parameters with <code>requires_grad=True</code> with respect to the loss gradients in order to improve them. <code>optimizer.step()</code> <p></p> <p>Note: The above is just one example of how the steps could be ordered or described. With experience you'll find making PyTorch training loops can be quite flexible.</p> <p>And on the ordering of things, the above is a good default order but you may see slightly different orders. Some rules of thumb:</p> <ul> <li>Calculate the loss (<code>loss = ...</code>) before performing backpropagation on it (<code>loss.backward()</code>).</li> <li>Zero gradients (<code>optimizer.zero_grad()</code>) before stepping them (<code>optimizer.step()</code>).</li> <li>Step the optimizer (<code>optimizer.step()</code>) after performing backpropagation on the loss (<code>loss.backward()</code>).</li> </ul> <p>For resources to help understand what's happening behind the scenes with backpropagation and gradient descent, see the extra-curriculum section.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#pytorch-testing-loop","title":"PyTorch testing loop\u00b6","text":"<p>As for the testing loop (evaluating our model), the typical steps include:</p> Number Step name What does it do? Code example 1 Forward pass The model goes through all of the training data once, performing its <code>forward()</code> function calculations. <code>model(x_test)</code> 2 Calculate the loss The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. <code>loss = loss_fn(y_pred, y_test)</code> 3 Calulate evaluation metrics (optional) Alongisde the loss value you may want to calculate other evaluation metrics such as accuracy on the test set. Custom functions <p>Notice the testing loop doesn't contain performing backpropagation (<code>loss.backward()</code>) or stepping the optimizer (<code>optimizer.step()</code>), this is because no parameters in the model are being changed during testing, they've already been calculated. For testing, we're only interested in the output of the forward pass through the model.</p> <p></p> <p>Let's put all of the above together and train our model for 100 epochs (forward passes through the data) and we'll evaluate it every 10 epochs.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#4-making-predictions-with-a-trained-pytorch-model-inference","title":"4. Making predictions with a trained PyTorch model (inference)\u00b6","text":"<p>Once you've trained a model, you'll likely want to make predictions with it.</p> <p>We've already seen a glimpse of this in the training and testing code above, the steps to do it outside of the training/testing loop are similar.</p> <p>There are three things to remember when making predictions (also called performing inference) with a PyTorch model:</p> <ol> <li>Set the model in evaluation mode (<code>model.eval()</code>).</li> <li>Make the predictions using the inference mode context manager (<code>with torch.inference_mode(): ...</code>).</li> <li>All predictions should be made with objects on the same device (e.g. data and model on GPU only or data and model on CPU only).</li> </ol> <p>The first two items make sure all helpful calculations and settings PyTorch uses behind the scenes during training but aren't necessary for inference are turned off (this results in faster computation). And the third ensures that you won't run into cross-device errors.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#5-saving-and-loading-a-pytorch-model","title":"5. Saving and loading a PyTorch model\u00b6","text":"<p>\u4fdd\u5b58\u6a21\u578b\u662f\u633a\u91cd\u8981\u7684\u4e00\u6b65\uff0c \u4e0d\u8fc7\u5e94\u8be5\u5e76\u4e0d\u96be\u3002</p> <p>If you've trained a PyTorch model, chances are you'll want to save it and export it somewhere.</p> <p>As in, you might train it on Google Colab or your local machine with a GPU but you'd like to now export it to some sort of application where others can use it.</p> <p>Or maybe you'd like to save your progress on a model and come back and load it back later.</p> <p>For saving and loading models in PyTorch, there are three main methods you should be aware of (all of below have been taken from the PyTorch saving and loading models guide):</p> PyTorch method What does it do? <code>torch.save</code> Saves a serialzed object to disk using Python's <code>pickle</code> utility. Models, tensors and various other Python objects like dictionaries can be saved using <code>torch.save</code>. <code>torch.load</code> Uses <code>pickle</code>'s unpickling features to deserialize and load pickled Python object files (like models, tensors or dictionaries) into memory. You can also set which device to load the object to (CPU, GPU etc). <code>torch.nn.Module.load_state_dict</code> Loads a model's parameter dictionary (<code>model.state_dict()</code>) using a saved <code>state_dict()</code> object. <p>Note: As stated in Python's <code>pickle</code> documentation, the <code>pickle</code> module is not secure. That means you should only ever unpickle (load) data you trust. That goes for loading PyTorch models as well. Only ever use saved PyTorch models from sources you trust.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#saving-a-pytorch-models-state_dict","title":"Saving a PyTorch model's <code>state_dict()</code>\u00b6","text":"<p>The recommended way for saving and loading a model for inference (making predictions) is by saving and loading a model's <code>state_dict()</code>.</p> <p>Let's see how we can do that in a few steps:</p> <ol> <li>We'll create a directory for saving models to called <code>models</code> using Python's <code>pathlib</code> module.</li> <li>We'll create a file path to save the model to.</li> <li>We'll call <code>torch.save(obj, f)</code> where <code>obj</code> is the target model's <code>state_dict()</code> and <code>f</code> is the filename of where to save the model.</li> </ol> <p>Note: It's common convention for PyTorch saved models or objects to end with <code>.pt</code> or <code>.pth</code>, like <code>saved_model_01.pth</code>.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#loading-a-saved-pytorch-models-state_dict","title":"Loading a saved PyTorch model's <code>state_dict()</code>\u00b6","text":"<p>Since we've now got a saved model <code>state_dict()</code> at <code>models/01_pytorch_workflow_model_0.pth</code> we can now load it in using <code>torch.nn.Module.load_state_dict(torch.load(f))</code> where <code>f</code> is the filepath of our saved model <code>state_dict()</code>.</p> <p>Why call <code>torch.load()</code> inside <code>torch.nn.Module.load_state_dict()</code>?</p> <p>Because we only saved the model's <code>state_dict()</code> which is a dictionary of learned parameters and not the entire model, we first have to load the <code>state_dict()</code> with <code>torch.load()</code> and then pass that <code>state_dict()</code> to a new instance of our model (which is a subclass of <code>nn.Module</code>).</p> <p>Why not save the entire model?</p> <p>Saving the entire model rather than just the <code>state_dict()</code> is more intuitive, however, to quote the PyTorch documentation (italics mine):</p> <p>The disadvantage of this approach (saving the whole model) is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved...</p> <p>Because of this, your code can break in various ways when used in other projects or after refactors.</p> <p>So instead, we're using the flexible method of saving and loading just the <code>state_dict()</code>, which again is basically a dictionary of model parameters.</p> <p>Let's test it out by created another instance of <code>LinearRegressionModel()</code>, which is a subclass of <code>torch.nn.Module</code> and will hence have the in-built method <code>load_state_dit()</code>.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#6-putting-it-all-together","title":"6. Putting it all together\u00b6","text":"<p>\u4e0b\u9762\u7684\u5185\u5bb9\u5c31\u662f\u5bf9\u4e0a\u9762\u7684\u4e00\u4e2a\u6c47\u603b\uff0c\u5e76\u4e14\u5c06epoch\u589e\u52a0\u52301000\u4e4b\u540e\uff0c\u53d1\u73b0\u6a21\u578b\u6700\u7ec8\u80fd\u591f\u975e\u5e38\u597d\u7684\u62df\u5408\u5230\u5f00\u59cb\u7684weigth\u548cbias\u3002 We've covered a fair bit of ground so far.</p> <p>But once you've had some practice, you'll be performing the above steps like dancing down the street.</p> <p>Speaking of practice, let's put everything we've done so far together.</p> <p>Except this time we'll make our code device agnostic (so if there's a GPU available, it'll use it and if not, it will default to the CPU).</p> <p>There'll be far less commentary in this section than above since what we're going to go through has already been covered.</p> <p>We'll start by importing the standard libraries we need.</p> <p>Note: If you're using Google Colab, to setup a GPU, go to Runtime -&gt; Change runtime type -&gt; Hardware acceleration -&gt; GPU. If you do this, it will reset the Colab runtime and you will lose saved variables.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#61-data","title":"6.1 Data\u00b6","text":"<p>Let's create some data just like before.</p> <p>First, we'll hard-code some <code>weight</code> and <code>bias</code> values.</p> <p>Then we'll make a range of numbers between 0 and 1, these will be our <code>X</code> values.</p> <p>Finally, we'll use the <code>X</code> values, as well as the <code>weight</code> and <code>bias</code> values to create <code>y</code> using the linear regression formula (<code>y = weight * X + bias</code>).</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#62-building-a-pytorch-linear-model","title":"6.2 Building a PyTorch linear model\u00b6","text":"<p>We've got some data, now it's time to make a model.</p> <p>We'll create the same style of model as before except this time, instead of defining the weight and bias parameters of our model manually using <code>nn.Parameter()</code>, we'll use <code>nn.Linear(in_features, out_features)</code> to do it for us.</p> <p>Where <code>in_features</code> is the number of dimensions your input data has and <code>out_features</code> is the number of dimensions you'd like it to be output to.</p> <p>In our case, both of these are <code>1</code> since our data has <code>1</code> input feature (<code>X</code>) per label (<code>y</code>).</p> <p> Creating a linear regression model using <code>nn.Parameter</code> versus using <code>nn.Linear</code>. There are plenty more examples of where the <code>torch.nn</code> module has pre-built computations, including many popular and useful neural network layers.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#63-training","title":"6.3 Training\u00b6","text":""},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#64-making-predictions","title":"6.4 Making predictions\u00b6","text":"<p>Now we've got a trained model, let's turn on it's evaluation mode and make some predictions.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#65-saving-and-loading-a-model","title":"6.5 Saving and loading a model\u00b6","text":"<p>We're happy with our models predictions, so let's save it to file so it can be used later.</p> <p>\u4e0b\u9762\u90e8\u4efd\u5c31\u662f\u5199\u5165\u5230\u8def\u5f84\uff0c\u7ed9\u6ce8\u91ca\u6389\u4e86\u3002</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#exercises","title":"Exercises\u00b6","text":"<p>All exercises have been inspired from code throughout the notebook.</p> <p>There is one exercise per major section.</p> <p>You should be able to complete them by referencing their specific section.</p> <p>Note: For all exercises, your code should be device agnostic (meaning it could run on CPU or GPU if it's available).</p> <ol> <li>Create a straight line dataset using the linear regression formula (<code>weight * X + bias</code>).</li> </ol> <ul> <li>Set <code>weight=0.3</code> and <code>bias=0.9</code> there should be at least 100 datapoints total.</li> <li>Split the data into 80% training, 20% testing.</li> <li>Plot the training and testing data so it becomes visual.</li> </ul> <ol> <li>Build a PyTorch model by subclassing <code>nn.Module</code>.</li> </ol> <ul> <li>Inside should be a randomly initialized <code>nn.Parameter()</code> with <code>requires_grad=True</code>, one for <code>weights</code> and one for <code>bias</code>.</li> <li>Implement the <code>forward()</code> method to compute the linear regression function you used to create the dataset in 1.</li> <li>Once you've constructed the model, make an instance of it and check its <code>state_dict()</code>.</li> <li>Note: If you'd like to use <code>nn.Linear()</code> instead of <code>nn.Parameter()</code> you can.</li> </ul> <ol> <li>Create a loss function and optimizer using <code>nn.L1Loss()</code> and <code>torch.optim.SGD(params, lr)</code> respectively.</li> </ol> <ul> <li>Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2.</li> <li>Write a training loop to perform the appropriate training steps for 300 epochs.</li> <li>The training loop should test the model on the test dataset every 20 epochs.</li> </ul> <ol> <li>Make predictions with the trained model on the test data.</li> </ol> <ul> <li>Visualize these predictions against the original training and testing data (note: you may need to make sure the predictions are not on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot).</li> </ul> <ol> <li>Save your trained model's <code>state_dict()</code> to file.</li> </ol> <ul> <li>Create a new instance of your model class you made in 2. and load in the <code>state_dict()</code> you just saved to it.</li> <li>Perform predictions on your test data with the loaded model and confirm they match the original model predictions from 4.</li> </ul> <p>Resource: See the exercises notebooks templates and solutions on the course GitHub.</p>"},{"location":"Learn/z2m-pytorch/01_pytorch_workflow/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Listen to The Unofficial PyTorch Optimization Loop Song (to help remember the steps in a PyTorch training/testing loop).</li> <li>Read What is <code>torch.nn</code>, really? by Jeremy Howard for a deeper understanding of how one of the most important modules in PyTorch works.</li> <li>Spend 10-minutes scrolling through and checking out the PyTorch documentation cheatsheet for all of the different PyTorch modules you might come across.</li> <li>Spend 10-minutes reading the loading and saving documentation on the PyTorch website to become more familiar with the different saving and loading options in PyTorch.</li> <li>Spend 1-2 hours read/watching the following for an overview of the internals of gradient descent and backpropagation, the two main algorithms that have been working in the background to help our model learn.</li> <li>Wikipedia page for gradient descent</li> <li>Gradient Descent Algorithm \u2014 a deep dive by Robert Kwiatkowski</li> <li>Gradient descent, how neural networks learn video by 3Blue1Brown</li> <li>What is backpropagation really doing? video by 3Blue1Brown</li> <li>Backpropagation Wikipedia Page</li> </ul>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/","title":"02. PyTorch Neural Network Classification","text":"<p>\u770b\u5b8c\u4e4b\u540e\uff0c\u4e0a\u9762\u7684\u5185\u5bb9\u7ed9\u6211\u5e26\u6765\u65b0\u5947\u611f\u7684\u662f\u4e3a\u6a21\u578b\u589e\u52a0\u975e\u7ebf\u6027\uff08\u6fc0\u6d3b\u51fd\u6570\uff09\uff0c\u5728\u4e0d\u5b58\u5728\u6fc0\u6d3b\u51fd\u6570\u7684\u6761\u4ef6\u4e0b\uff0c\u5373\u4f7f\u662f\u6df1\u5ea6\u5b66\u4e60\u7684\u7f51\u7edc\uff0c\u4e5f\u53ea\u80fd\u53bb\u62df\u5408\u7ebf\u6027\u95ee\u9898\uff08\u4e3a\u4ec0\u4e48\u5462\uff1f\uff09\uff0c\u589e\u52a0\u4e86\u6fc0\u6d3b\u51fd\u6570\u7684\u4e4b\u540e\uff08\u79cd\u7c7b\u5e76\u6ca1\u6709\u4e25\u683c\u7684\u9650\u5236\uff09\uff0c\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6a21\u578b\u53ef\u4ee5\u53bb\u62df\u5408\u590d\u6742\u6570\u636e\u3002</p> <p>Chatgpt\u56de\u7b54\uff1a \u5728\u6ca1\u6709\u6fc0\u6d3b\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u5b9e\u9645\u4e0a\u53ea\u662f\u5355\u4e2a\u7ebf\u6027\u5c42\u7684\u7ec4\u5408\uff0c\u8fd9\u5c31\u610f\u5473\u7740\u7f51\u7edc\u65e0\u8bba\u6709\u591a\u5c11\u5c42\uff0c\u5176\u6574\u4e2a\u6a21\u578b\u90fd\u53ea\u80fd\u8868\u793a\u7ebf\u6027\u51fd\u6570\u3002</p> <p>\u8003\u8651\u4e00\u4e2a\u7b80\u5355\u7684\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u5176\u4e2d\u7b2c\u4e00\u5c42\u662f\u8f93\u5165\u5c42\uff0c\u7b2c\u4e8c\u5c42\u662f\u8f93\u51fa\u5c42\uff0c\u6ca1\u6709\u6fc0\u6d3b\u51fd\u6570\uff0c\u6743\u91cd\u77e9\u9635\u4e3a$W_1$\u548c$W_2$\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u7f51\u7edc\u7684\u8f93\u51fa\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a</p> <p>$y = W_2W_1x$</p> <p>\u53ef\u4ee5\u770b\u5230\uff0c\u8f93\u51fa$y$\u662f\u8f93\u5165$x$\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u800c\u8fd9\u5c31\u662f\u7ebf\u6027\u51fd\u6570\u3002\u5982\u679c\u6211\u4eec\u5bf9\u8fd9\u4e2a\u7f51\u7edc\u6dfb\u52a0\u6fc0\u6d3b\u51fd\u6570\uff0c\u5c31\u53ef\u4ee5\u6253\u7834\u8fd9\u79cd\u7ebf\u6027\u5173\u7cfb\uff0c\u4f7f\u5176\u80fd\u591f\u62df\u5408\u975e\u7ebf\u6027\u51fd\u6570\u3002</p> <p>\u6fc0\u6d3b\u51fd\u6570\u7684\u4f5c\u7528\u662f\u5f15\u5165\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u5c06\u7ebf\u6027\u8f93\u51fa\u8f6c\u6362\u4e3a\u975e\u7ebf\u6027\u8f93\u51fa\u3002\u8fd9\u4f7f\u5f97\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u62df\u5408\u66f4\u590d\u6742\u7684\u51fd\u6570\uff0c\u4ece\u800c\u6269\u5c55\u5176\u8868\u8fbe\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u6fc0\u6d3b\u51fd\u6570\u662f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u975e\u5e38\u91cd\u8981\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u6ca1\u6709\u6fc0\u6d3b\u51fd\u6570\u7684\u6a21\u578b\u53ea\u80fd\u89e3\u51b3\u7ebf\u6027\u95ee\u9898\u3002</p> <p>\u6dfb\u52a0\u6fc0\u6d3b\u51fd\u6570(relu)\u4e4b\u540e\u7684\u6570\u636e\u8868\u793a\u53ef\u4ee5\u4e3a\uff1a</p> <p>$y = relu(W_2W_1x)$</p> In\u00a0[22]: Copied! <pre>#!pip install -U scikit-learn -i https://pypi.mirrors.ustc.edu.cn/simple #\u53ef\u80fd\u9700\u8981\u4e0b\u8f7d\n</pre> #!pip install -U scikit-learn -i https://pypi.mirrors.ustc.edu.cn/simple #\u53ef\u80fd\u9700\u8981\u4e0b\u8f7d In\u00a0[23]: Copied! <pre>from sklearn.datasets import make_circles\n\n\n# Make 1000 samples \nn_samples = 1000\n\n# Create circles\nX, y = make_circles(n_samples,\n                    noise=0.03, # a little bit of noise to the dots\n                    random_state=42) # keep random state so we get the same values\n</pre> from sklearn.datasets import make_circles   # Make 1000 samples  n_samples = 1000  # Create circles X, y = make_circles(n_samples,                     noise=0.03, # a little bit of noise to the dots                     random_state=42) # keep random state so we get the same values  <p>Alright, now let's view the first 5 <code>X</code> and <code>y</code> values.</p> In\u00a0[24]: Copied! <pre>print(f\"First 5 X features:\\n{X[:5]}\")\nprint(f\"\\nFirst 5 y labels:\\n{y[:5]}\")\n</pre> print(f\"First 5 X features:\\n{X[:5]}\") print(f\"\\nFirst 5 y labels:\\n{y[:5]}\") <pre>First 5 X features:\n[[ 0.75424625  0.23148074]\n [-0.75615888  0.15325888]\n [-0.81539193  0.17328203]\n [-0.39373073  0.69288277]\n [ 0.44220765 -0.89672343]]\n\nFirst 5 y labels:\n[1 1 1 1 0]\n</pre> <p>Looks like there's two <code>X</code> values per one <code>y</code> value.</p> <p>Let's keep following the data explorer's motto of visualize, visualize, visualize and put them into a pandas DataFrame.</p> In\u00a0[25]: Copied! <pre># Make DataFrame of circle data\nimport pandas as pd\ncircles = pd.DataFrame({\"X1\": X[:, 0],\n    \"X2\": X[:, 1],\n    \"label\": y\n})\ncircles.head(10)\n</pre> # Make DataFrame of circle data import pandas as pd circles = pd.DataFrame({\"X1\": X[:, 0],     \"X2\": X[:, 1],     \"label\": y }) circles.head(10) Out[25]: X1 X2 label 0 0.754246 0.231481 1 1 -0.756159 0.153259 1 2 -0.815392 0.173282 1 3 -0.393731 0.692883 1 4 0.442208 -0.896723 0 5 -0.479646 0.676435 1 6 -0.013648 0.803349 1 7 0.771513 0.147760 1 8 -0.169322 -0.793456 1 9 -0.121486 1.021509 0 <p>It looks like each pair of <code>X</code> features (<code>X1</code> and <code>X2</code>) has a label (<code>y</code>) value of either 0 or 1.</p> <p>This tells us that our problem is binary classification since there's only two options (0 or 1).</p> <p>How many values of each class is there?</p> In\u00a0[83]: Copied! <pre># Check different labels\ncircles.label.value_counts()#\u67e5\u770b\u7c7b\u522b\u6570\n#\u6ca1\u60f3\u5230\u8fd8\u53ef\u4ee5\u8fd9\u4e48\u4f7f\u7528\u5217\u7d22\u5f15\uff0c\u6211\u4e00\u822c\u8fd9\u4e48\u5199\ncircles[\"label\"].value_counts()\n</pre> # Check different labels circles.label.value_counts()#\u67e5\u770b\u7c7b\u522b\u6570 #\u6ca1\u60f3\u5230\u8fd8\u53ef\u4ee5\u8fd9\u4e48\u4f7f\u7528\u5217\u7d22\u5f15\uff0c\u6211\u4e00\u822c\u8fd9\u4e48\u5199 circles[\"label\"].value_counts() Out[83]: <pre>1    500\n0    500\nName: label, dtype: int64</pre> <p>500 each, nice and balanced.</p> <p>Let's plot them.</p> In\u00a0[27]: Copied! <pre># Visualize with a plot\nimport matplotlib.pyplot as plt\nplt.scatter(x=X[:, 0], \n            y=X[:, 1], \n            c=y, \n            cmap=plt.cm.RdYlBu)\n</pre> # Visualize with a plot import matplotlib.pyplot as plt plt.scatter(x=X[:, 0],              y=X[:, 1],              c=y,              cmap=plt.cm.RdYlBu) Out[27]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f773c2ac4f0&gt;</pre> <p>Alrighty, looks like we've got a problem to solve.</p> <p>Let's find out how we could build a PyTorch neural network to classify dots into red (0) or blue (1). \u4ece\u56fe\u4e2d\u53ef\u6709\u770b\u51fa\u660e\u663e\u7684\u533a\u5206\uff0c\u56e0\u6b64\u5728\u901a\u8fc7\u6a21\u578b\u4e5f\u4f1a\u8f83\u5bb9\u6613\u7684\u533a\u5206\u8fd9\u4e2a\u6570\u636e\u3002</p> <p>Note: This dataset is often what's considered a toy problem (a problem that's used to try and test things out on) in machine learning.</p> <p>But it represents the major key of classification, you have some kind of data represented as numerical values and you'd like to build a model that's able to classify it, in our case, separate it into red or blue dots.</p> In\u00a0[28]: Copied! <pre># Check the shapes of our features and labels\nX.shape, y.shape\n</pre> # Check the shapes of our features and labels X.shape, y.shape Out[28]: <pre>((1000, 2), (1000,))</pre> <p>\u8fd9\u91cc\u5c31\u662f\u7406\u89e3\u4e3a\uff0c\u603b\u5171\u7684\u6570\u636e\u4e2d\uff0c\u6211\u4eec\u662f\u67091000\u4e2a\u6837\u672c\u7684\uff0c\u5bf9\u4e8eX\uff0c\u6211\u4eec\u6bcf\u4e2a\u6837\u672c\u6709\u4e24\u4e2a\u503c\uff0c\u5bf9\u4e8ey\u6211\u4eec\u7684\u6bcf\u4e2a\u6837\u672c\u53ea\u6709\u4e00\u4e2a\u503c\uff08\u4e00\u4e2a\u6807\u7b7e\uff09\u3002</p> <p>Looks like we've got a match on the first dimension of each.</p> <p>There's 1000 <code>X</code> and 1000 <code>y</code>.</p> <p>But what's the second dimension on <code>X</code>?</p> <p>It often helps to view the values and shapes of a single sample (features and labels).</p> <p>Doing so will help you understand what input and output shapes you'd be expecting from your model.</p> In\u00a0[29]: Copied! <pre># View the first example of features and labels\nX_sample = X[0]\ny_sample = y[0]\nprint(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\nprint(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\")\n</pre> # View the first example of features and labels X_sample = X[0] y_sample = y[0] print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\") print(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\") <pre>Values for one sample of X: [0.75424625 0.23148074] and the same for y: 1\nShapes for one sample of X: (2,) and the same for y: ()\n</pre> <p>This tells us the second dimension for <code>X</code> means it has two features (vector) where as <code>y</code> has a single feature (scalar).</p> <p>We have two inputs for one output.</p> In\u00a0[30]: Copied! <pre># Turn data into tensors\n# Otherwise this causes issues with computations later on\nimport torch\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n# View the first five samples\nX[:5], y[:5]\n</pre> # Turn data into tensors # Otherwise this causes issues with computations later on import torch X = torch.from_numpy(X).type(torch.float) y = torch.from_numpy(y).type(torch.float)  # View the first five samples X[:5], y[:5] Out[30]: <pre>(tensor([[ 0.7542,  0.2315],\n         [-0.7562,  0.1533],\n         [-0.8154,  0.1733],\n         [-0.3937,  0.6929],\n         [ 0.4422, -0.8967]]),\n tensor([1., 1., 1., 1., 0.]))</pre> <p>Now our data is in tensor format, let's split it into training and test sets.</p> <p>To do so, let's use the helpful function <code>train_test_split()</code> from Scikit-Learn. \u8fd8\u662f\u4f7f\u7528\u7684sklearn\u7684\u51fd\u6570\uff0c\u4f46\u662fpytorch\u4e2d\u4e5f\u6709\u76f8\u5173\u7684\u51fd\u6570\u7684\uff1atorch.utils.data.random_split\u3002\u4f46\u662f\u8fd9\u4e2a\u662f\u57fa\u4e8epytorh\u7684Dataset\u5bf9\u8c61\u7684\uff0c\u597d\u60f3\u6ca1\u6cd5\u76f4\u63a5\u7528\u4e8etensor\u3002</p> <p>We'll use <code>test_size=0.2</code> (80% training, 20% testing) and because the split happens randomly across the data, let's use <code>random_state=42</code> so the split is reproducible.</p> In\u00a0[31]: Copied! <pre># Split data into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2, # 20% test, 80% train\n                                                    random_state=42) # make the random split reproducible\n\nlen(X_train), len(X_test), len(y_train), len(y_test)\n</pre> # Split data into train and test sets from sklearn.model_selection import train_test_split  X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                      test_size=0.2, # 20% test, 80% train                                                     random_state=42) # make the random split reproducible  len(X_train), len(X_test), len(y_train), len(y_test) Out[31]: <pre>(800, 200, 800, 200)</pre> <p>Nice! Looks like we've now got 800 training samples and 200 testing samples.</p> In\u00a0[32]: Copied! <pre># Standard PyTorch imports\nimport torch\nfrom torch import nn\n\n# Make device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Standard PyTorch imports import torch from torch import nn  # Make device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[32]: <pre>'cuda'</pre> <p>Excellent, now <code>device</code> is setup, we can use it for any data or models we create and PyTorch will handle it on the CPU (default) or GPU if it's available.</p> <p>How about we create a model?</p> <p>We'll want a model capable of handling our <code>X</code> data as inputs and producing something in the shape of our <code>y</code> data as ouputs.</p> <p>In other words, given <code>X</code> (features) we want our model to predict <code>y</code> (label).</p> <p>This setup where you have features and labels is referred to as supervised learning. Because your data is telling your model what the outputs should be given a certain input.</p> <p>To create such a model it'll need to handle the input and output shapes of <code>X</code> and <code>y</code>.</p> <p>Remember how I said input and output shapes are important? Here we'll see why.</p> <p>Let's create a model class that:</p> <ol> <li>Subclasses <code>nn.Module</code> (almost all PyTorch models are subclasses of <code>nn.Module</code>). \u6240\u6709\u7684pytorch\u6a21\u578b\u90fd\u662ftorch.nn.Module\u7684\u5b50\u7c7b</li> <li>Creates 2 <code>nn.Linear</code> layers in the constructor capable of handling the input and output shapes of <code>X</code> and <code>y</code>. \u91cd\u5199 __init__\u51fd\u6570</li> <li>Defines a <code>forward()</code> method containing the forward pass computation of the model.\u91cd\u5199\u524d\u5411\u4f20\u64ad\u51fd\u6570</li> <li>Instantiates the model class and sends it to the target <code>device</code>. \u5b9e\u4f8b\u5316\u6a21\u578b\u5e76\u5c06\u5176\u79fb\u52a8\u5230\u5bf9\u5e94\u7684\u8bbe\u5907\u4e2d to(device)\u51fd\u6570</li> </ol> <p>\u4e3a\u4ec0\u4e48\u8981\u8fd9\u4e48\u5199\u5462\uff1f</p> <ol> <li><code>torch.nn.Module</code>\u4e2d\u5df2\u7ecf\u5b9e\u73b0\u4e86\u540e\u9762\u6a21\u578b\u6240\u9700\u8981\u7684\u529f\u80fd\uff0c\u6240\u4ee5\u6211\u4eec\u76f4\u63a5\u4f7f\u7528\u5b83\u7684\u5b50\u7c7b\u5373\u53ef\uff0c\u53ea\u9700\u8981\u91cd\u5199<code>init()</code>\u548c<code>forward()</code></li> <li>init\u662fpython\u4e2d\u7684\u7c7b\u6784\u9020\u51fd\u6570\uff0c\u5f53\u6211\u4eec\u5b9e\u4f8b\u5316\u4e00\u4e2a\u7c7b\u7684\u65f6\u5019\uff0c\u5728init\u4e2d\u5b9a\u4e49\u597d\u7684\u4e1c\u897f\u4f1a\u88ab\u76f4\u63a5\u786e\u5b9a\u3002\u7528\u6765\u5b9a\u4e49\u6a21\u578b\u67b6\u6784\u3002</li> <li>forward\u662f\u5b9e\u73b0\u6a21\u578b\u7684\u524d\u5411\u4f20\u64ad\u7b97\u6cd5\uff0c\u5373\u6a21\u578b\u7684\u8ba1\u7b97\u65b9\u5f0f\u3002\u5411\u91cf\u62fc\u63a5\u4e00\u822c\u5728\u8fd9\u91cc\u5b9e\u73b0\u3002</li> </ol> <p>\u4e3e\u4f8b\u4e00\u4e2a\u6bd4\u8f83\u590d\u6742\u7684\u6a21\u578b\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        # \u62c6\u5206\u8f93\u5165x\u4e3a\u4e09\u4e2a\u901a\u9053\n        red_channel = x[:, 0, :, :].unsqueeze(1)\n        green_channel = x[:, 1, :, :].unsqueeze(1)\n        blue_channel = x[:, 2, :, :].unsqueeze(1)\n\n        # \u5c06\u4e09\u4e2a\u901a\u9053\u62fc\u63a5\u6210\u4e00\u4e2aTensor\n        x = torch.cat([red_channel, green_channel, blue_channel], dim=1)\n\n        # \u8fdb\u884c\u5377\u79ef\u548c\u5168\u8fde\u63a5\u64cd\u4f5c\n        x = nn.functional.relu(self.conv1(x))\n        x = nn.functional.max_pool2d(x, 2)\n        x = nn.functional.relu(self.conv2(x))\n        x = nn.functional.max_pool2d(x, 2)\n        x = x.view(-1, 128 * 8 * 8)\n        x = nn.functional.relu(self.fc1(x))\n        x = self.fc2(x)\n\n        return x\n</code></pre> In\u00a0[33]: Copied! <pre># 1. Construct a model class that subclasses nn.Module\nclass CircleModelV0(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes\n        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n    \n    # 3. Define a forward method containing the forward pass computation\n    def forward(self, x):\n        # Return the output of layer_2, a single feature, the same shape as y\n        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n\n# 4. Create an instance of the model and send it to target device\nmodel_0 = CircleModelV0().to(device)\nmodel_0\n</pre> # 1. Construct a model class that subclasses nn.Module class CircleModelV0(nn.Module):     def __init__(self):         super().__init__()         # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes         self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features         self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)          # 3. Define a forward method containing the forward pass computation     def forward(self, x):         # Return the output of layer_2, a single feature, the same shape as y         return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2  # 4. Create an instance of the model and send it to target device model_0 = CircleModelV0().to(device) model_0 Out[33]: <pre>CircleModelV0(\n  (layer_1): Linear(in_features=2, out_features=5, bias=True)\n  (layer_2): Linear(in_features=5, out_features=1, bias=True)\n)</pre> <p>What's going on here?</p> <p>We've seen a few of these steps before.</p> <p>The only major change is what's happening between <code>self.layer_1</code> and <code>self.layer_2</code>.</p> <p><code>self.layer_1</code> takes 2 input features <code>in_features=2</code> and produces 5 output features <code>out_features=5</code>.</p> <p>This is known as having 5 hidden units or neurons.</p> <p>This layer turns the input data from having 2 features to 5 features.</p> <p>Why do this? (\u8fd9\u79cd\u4e8b\u5c5e\u4e8e\u6a21\u578b\u5982\u4f55\u6784\u5efa\u7684\u601d\u8def\uff09</p> <p>This allows the model to learn patterns from 5 numbers rather than just 2 numbers, potentially leading to better outputs.</p> <p>I say potentially because sometimes it doesn't work.</p> <p>The number of hidden units you can use in neural network layers is a hyperparameter (a value you can set yourself) and there's no set in stone value you have to use.</p> <p>Generally more is better but there's also such a thing as too much. The amount you choose will depend on your model type and dataset you're working with.</p> <p>Since our dataset is small and simple, we'll keep it small.</p> <p>The only rule with hidden units is that the next layer, in our case, <code>self.layer_2</code> has to take the same <code>in_features</code> as the previous layer <code>out_features</code>.</p> <p>That's why <code>self.layer_2</code> has <code>in_features=5</code>, it takes the <code>out_features=5</code> from <code>self.layer_1</code> and performs a linear computation on them, turning them into <code>out_features=1</code> (the same shape as <code>y</code>).</p> <p> A visual example of what a similar classificiation neural network to the one we've just built looks like. Try create one of your own on the TensorFlow Playground website. \u54c7\uff0c\u8fd9\u4e2a\u53ef\u89c6\u5316\u7684\u7f51\u7ad9\u592a\u5f3a\u4e86\u3002\u5f88\u76f4\u89c2\u3002</p> <p>You can also do the same as above using <code>nn.Sequential</code>.</p> <p><code>nn.Sequential</code> performs a forward pass computation of the input data through the layers in the order they appear.</p> In\u00a0[34]: Copied! <pre># Replicate CircleModelV0 with nn.Sequential\nmodel_0 = nn.Sequential(\n    nn.Linear(in_features=2, out_features=5),\n    nn.Linear(in_features=5, out_features=1)\n).to(device)\n\nmodel_0\n</pre> # Replicate CircleModelV0 with nn.Sequential model_0 = nn.Sequential(     nn.Linear(in_features=2, out_features=5),     nn.Linear(in_features=5, out_features=1) ).to(device)  model_0 Out[34]: <pre>Sequential(\n  (0): Linear(in_features=2, out_features=5, bias=True)\n  (1): Linear(in_features=5, out_features=1, bias=True)\n)</pre> <p>Woah, that looks much simpler than subclassing <code>nn.Module</code>, why not just always use <code>nn.Sequential</code>?</p> <p><code>nn.Sequential</code> is fantastic for straight-forward computations, however, as the namespace says, it always runs in sequential order.</p> <p>So if you'd something else to happen (rather than just straight-forward sequential computation) you'll want to define your own custom <code>nn.Module</code> subclass.</p> <p>Now we've got a model, let's see what happens when we pass some data through it.</p> <p><code>nn.Sequential</code>\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\uff0c\u6bd4\u8f83\u7c7b\u4f3c\u4e8e\u4e00\u4e2a\u5feb\u6377\u65b9\u5f0f\u3002</p> In\u00a0[35]: Copied! <pre># Make predictions with the model\nuntrained_preds = model_0(X_test.to(device))\nprint(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\nprint(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\nprint(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\nprint(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")\n</pre> # Make predictions with the model untrained_preds = model_0(X_test.to(device)) print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\") print(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\") print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\") print(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\") <pre>Length of predictions: 200, Shape: torch.Size([200, 1])\nLength of test samples: 200, Shape: torch.Size([200])\n\nFirst 10 predictions:\ntensor([[0.5718],\n        [0.3631],\n        [1.0974],\n        [0.4230],\n        [1.0634],\n        [0.9362],\n        [0.3756],\n        [0.5097],\n        [1.0980],\n        [0.3488]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nFirst 10 test labels:\ntensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])\n</pre> <p>Hmm, it seems there's the same amount of predictions as there is test labels but the predictions don't look like they're in the same form or shape as the test labels.</p> <p>We've got a couple steps we can do to fix this, we'll see these later on.</p> <p>\u901a\u8fc7\u672a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u67e5\u770b\u4e86\u9884\u6d4b\u7684\u7ed3\u679c\uff0c\u53d1\u73b0\u6548\u679c\u5f88\u5dee\u3002</p> In\u00a0[36]: Copied! <pre># Create a loss function\n# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\nloss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n\n# Create an optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), \n                            lr=0.1)\n</pre> # Create a loss function # loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in  # Create an optimizer optimizer = torch.optim.SGD(params=model_0.parameters(),                              lr=0.1) <p>Now let's also create an evaluation metric. \u6a21\u578b\u7684\u8bc4\u4ef7\u6807\u51c6\uff0c\u8fd9\u6837\u66f4\u76f4\u89c2\u7684\u770b\u5230\u6a21\u578b\u7684\u53d8\u5316\uff0c\u4e5f\u6709\u5229\u4e8e\u540e\u7eed\u7684\u8c03\u4f18\uff08\u5305\u62ecROC\uff0cACC\u7b49\uff09</p> <p>An evaluation metric can be used to offer another perspective on how your model is going.</p> <p>If a loss function measures how wrong your model is, I like to think of evaluation metrics as measuring how right it is.\uff08\u8fd9\u53e5\u8bdd\u5f88\u7cbe\u8f9f\u54ce\uff09</p> <p>Of course, you could argue both of these are doing the same thing but evaluation metrics offer a different perspective.</p> <p>After all, when evaluating your models it's good to look at things from multiple points of view.</p> <p>There are several evaluation metrics that can be used for classification problems but let's start out with accuracy.</p> <p>Accuracy can be measured by dividing the total number of correct predictions over the total number of predictions.</p> <p>For example, a model that makes 99 correct predictions out of 100 will have an accuracy of 99%.</p> <p>Let's write a function to do so.</p> In\u00a0[37]: Copied! <pre># Calculate accuracy (a classification metric)\ndef accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n    acc = (correct / len(y_pred)) * 100 \n    return acc\n</pre> # Calculate accuracy (a classification metric) def accuracy_fn(y_true, y_pred):     correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal     acc = (correct / len(y_pred)) * 100      return acc <p><code>torch.eq(y_true, y_pred).sum().item()</code>\u505a\u7684\u662f\uff0c\u8ba1\u7b97\u4e24\u4e2atensor\u662f\u5426\u76f8\u7b49\uff08\u8fd4\u56de\u5e03\u5c14\u6570\u503c\uff09\uff0c\u7136\u540e\u7d2f\u52a0\uff0c\u518d\u7528item\u8fd4\u56detensor\u7684\u6570\u503c\u3002</p> <p>Excellent! We can now use this function whilst training our model to measure it's performance alongside the loss.</p> <p>\u8fd9\u4e00\u90e8\u5206\u6982\u8ff0\u4e86\uff0c\u5982\u4f55\u5c06\u6211\u4eec\u7684\u9884\u6d4b\u503c\u8f6c\u5316\u4e3alabel\uff0c\u6a21\u578b\u7684\u8f93\u51fa\u662f\u6570\u503c\uff0c\u6211\u4eec\u901a\u8fc7sigmoid\u51fd\u6570\u8f6c\u5316\u4e3a0-1\u4e4b\u95f4\u7684\u6570\u503c\uff0c\u6839\u636e0.5\u4e3acutoff\uff0c\u6765\u83b7\u5f97\u9884\u6d4b\u503c\u7684label\u3002</p> In\u00a0[38]: Copied! <pre># View the frist 5 outputs of the forward pass on the test data\ny_logits = model_0(X_test.to(device))[:5]\ny_logits\n</pre> # View the frist 5 outputs of the forward pass on the test data y_logits = model_0(X_test.to(device))[:5] y_logits Out[38]: <pre>tensor([[0.5718],\n        [0.3631],\n        [1.0974],\n        [0.4230],\n        [1.0634]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)</pre> <p>Since our model hasn't been trained, these outputs are basically random.</p> <p>But what are they?</p> <p>They're the output of our <code>forward()</code> method.</p> <p>Which implements two layers of <code>nn.Linear()</code> which internally calls the following equation:</p> <p>$$ \\mathbf{y} = x \\cdot \\mathbf{Weights}^T  + \\mathbf{bias} $$</p> <p>The raw outputs (unmodified) of this equation ($\\mathbf{y}$) and in turn, the raw outputs of our model are often referred to as logits.</p> <p>That's what our model is outputing above when it takes in the input data ($x$ in the equation or <code>X_test</code> in the code), logits.</p> <p>However, these numbers are hard to interpret.</p> <p>We'd like some numbers that are comparable to our truth labels.</p> <p>To get our model's raw outputs (logits) into such a form, we can use the sigmoid activation function.</p> <p>Let's try it out.</p> In\u00a0[39]: Copied! <pre># Use sigmoid on model logits\ny_pred_probs = torch.sigmoid(y_logits)\ny_pred_probs\n</pre> # Use sigmoid on model logits y_pred_probs = torch.sigmoid(y_logits) y_pred_probs Out[39]: <pre>tensor([[0.6392],\n        [0.5898],\n        [0.7498],\n        [0.6042],\n        [0.7433]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)</pre> <p>Okay, it seems like the outputs now have some kind of consistency (even though they're still random).</p> <p>They're now in the form of prediction probabilities (I usually refer to these as <code>y_pred_probs</code>), in other words, the values are now how much the model thinks the data point belongs to one class or another.</p> <p>In our case, since we're dealing with binary classification, our ideal outputs are 0 or 1.</p> <p>So these values can be viewed as a decision boundary.</p> <p>The closer to 0, the more the model thinks the sample belongs to class 0, the closer to 1, the more the model thinks the sample belongs to class 1.</p> <p>More specificially:</p> <ul> <li>If <code>y_pred_probs</code> &gt;= 0.5, <code>y=1</code> (class 1)</li> <li>If <code>y_pred_probs</code> &lt; 0.5, <code>y=0</code> (class 0)</li> </ul> <p>To turn our prediction probabilities in prediction labels, we can round the outputs of the sigmoid activation function.</p> In\u00a0[40]: Copied! <pre># Find the predicted labels (round the prediction probabilities)\ny_preds = torch.round(y_pred_probs)\n\n# In full\ny_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n\n# Check for equality\nprint(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n\n# Get rid of extra dimension\ny_preds.squeeze()\n</pre> # Find the predicted labels (round the prediction probabilities) y_preds = torch.round(y_pred_probs)  # In full y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))  # Check for equality print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))  # Get rid of extra dimension y_preds.squeeze() <pre>tensor([True, True, True, True, True], device='cuda:0')\n</pre> Out[40]: <pre>tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=&lt;SqueezeBackward0&gt;)</pre> <p>Excellent! Now it looks like our model's predictions are in the same form as our truth labels (<code>y_test</code>).</p> In\u00a0[41]: Copied! <pre>y_test[:5]\n</pre> y_test[:5] Out[41]: <pre>tensor([1., 0., 1., 0., 1.])</pre> <p>This means we'll be able to compare our models predictions to the test labels to see how well it's going.</p> <p>To recap, we converted our model's raw outputs (logits) to predicition probabilities using a sigmoid activation function.</p> <p>And then converted the prediction probabilities to prediction labels by rounding them.</p> <p>Note: The use of the sigmoid activation function is often only for binary classification logits. For multi-class classification, we'll be looking at using the softmax activation function (this will come later on).</p> <p>And the use of the sigmoid activation function is not required when passing our model's raw outputs to the <code>nn.BCEWithLogitsLoss</code> (the \"logits\" in logits loss is because it works on the model's raw logits output), this is because it has a sigmoid function built-in.</p> In\u00a0[42]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs\nepochs = 100\n\n# Put data to target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\n# Build training and evaluation loop\nfor epoch in range(epochs):\n    ### Training\n    model_0.train() #\u8bad\u7ec3\u6a21\u5f0f\u6253\u5f00\n\n    # 1. Forward pass (model outputs raw logits)\u8fdb\u884c\u524d\u5411\u4f20\u64ad\n    y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -&gt; pred probs -&gt; pred labls\n  \n    # 2. Calculate loss/accuracy\n    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n    #                y_train) \n    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n                   y_train) \n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred) \n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad() #\u6e05\u9664\u68af\u5ea6\n\n    # 4. Loss backwards\n    loss.backward() #\u53cd\u5411\u4f20\u64ad\n\n    # 5. Optimizer step\n    optimizer.step() #\u66f4\u65b0\u53c2\u6570\n\n    ### Testing \n    model_0.eval() #\u8bc4\u4ef7\u6a21\u5f0f\u6253\u5f00\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_0(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. Caculate loss/accuracy\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # Print out what's happening every 10 epochs\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n</pre> torch.manual_seed(42)  # Set the number of epochs epochs = 100  # Put data to target device X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)  # Build training and evaluation loop for epoch in range(epochs):     ### Training     model_0.train() #\u8bad\u7ec3\u6a21\u5f0f\u6253\u5f00      # 1. Forward pass (model outputs raw logits)\u8fdb\u884c\u524d\u5411\u4f20\u64ad     y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device      y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -&gt; pred probs -&gt; pred labls        # 2. Calculate loss/accuracy     # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()     #                y_train)      loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits                    y_train)      acc = accuracy_fn(y_true=y_train,                        y_pred=y_pred)       # 3. Optimizer zero grad     optimizer.zero_grad() #\u6e05\u9664\u68af\u5ea6      # 4. Loss backwards     loss.backward() #\u53cd\u5411\u4f20\u64ad      # 5. Optimizer step     optimizer.step() #\u66f4\u65b0\u53c2\u6570      ### Testing      model_0.eval() #\u8bc4\u4ef7\u6a21\u5f0f\u6253\u5f00     with torch.inference_mode():         # 1. Forward pass         test_logits = model_0(X_test).squeeze()          test_pred = torch.round(torch.sigmoid(test_logits))         # 2. Caculate loss/accuracy         test_loss = loss_fn(test_logits,                             y_test)         test_acc = accuracy_fn(y_true=y_test,                                y_pred=test_pred)      # Print out what's happening every 10 epochs     if epoch % 10 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\") <pre>Epoch: 0 | Loss: 0.78769, Accuracy: 50.00% | Test loss: 0.75649, Test acc: 50.00%\nEpoch: 10 | Loss: 0.71917, Accuracy: 50.00% | Test loss: 0.70614, Test acc: 50.00%\nEpoch: 20 | Loss: 0.70067, Accuracy: 50.25% | Test loss: 0.69416, Test acc: 50.50%\nEpoch: 30 | Loss: 0.69562, Accuracy: 55.62% | Test loss: 0.69180, Test acc: 54.00%\nEpoch: 40 | Loss: 0.69418, Accuracy: 51.25% | Test loss: 0.69166, Test acc: 53.50%\nEpoch: 50 | Loss: 0.69371, Accuracy: 49.38% | Test loss: 0.69193, Test acc: 54.00%\nEpoch: 60 | Loss: 0.69352, Accuracy: 48.88% | Test loss: 0.69222, Test acc: 53.50%\nEpoch: 70 | Loss: 0.69341, Accuracy: 48.00% | Test loss: 0.69246, Test acc: 53.50%\nEpoch: 80 | Loss: 0.69333, Accuracy: 46.88% | Test loss: 0.69266, Test acc: 51.50%\nEpoch: 90 | Loss: 0.69327, Accuracy: 45.00% | Test loss: 0.69283, Test acc: 48.00%\n</pre> <p>Hmm, what do you notice about the performance of our model?</p> <p>It looks like it went through the training and testing steps fine but the results don't seem to have moved too much.</p> <p>The accuracy barely moves above 50% on each data split.</p> <p>And because we're working with a balanced binary classification problem, it means our model is performing as good as random guessing (with 500 samples of class 0 and class 1 a model predicting class 1 every single time would achieve 50% accuracy).</p> In\u00a0[43]: Copied! <pre>import requests\nfrom pathlib import Path \n\n# Download helper functions from Learn PyTorch repo (if not already downloaded)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n\nfrom helper_functions import plot_predictions, plot_decision_boundary\n</pre> import requests from pathlib import Path   # Download helper functions from Learn PyTorch repo (if not already downloaded) if Path(\"helper_functions.py\").is_file():   print(\"helper_functions.py already exists, skipping download\") else:   print(\"Downloading helper_functions.py\")   request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")   with open(\"helper_functions.py\", \"wb\") as f:     f.write(request.content)  from helper_functions import plot_predictions, plot_decision_boundary <pre>Downloading helper_functions.py\n</pre> In\u00a0[44]: Copied! <pre># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_0, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_0, X_test, y_test)\n</pre> # Plot decision boundaries for training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_0, X_train, y_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_0, X_test, y_test) <p>Oh wow, it seems like we've found the cause of model's performance issue.</p> <p>It's currently trying to split the red and blue dots using a straight line...</p> <p>That explains the 50% accuracy. Since our data is circular, drawing a straight line can at best cut it down the middle.</p> <p>In machine learning terms, our model is underfitting, meaning it's not learning predictive patterns from the data.</p> <p>How could we improve this?</p> <p>\u8fd9\u4e2a\u53ef\u89c6\u5316\u51fd\u6570\u633a\u6709\u610f\u601d\u7684\uff0c\u4f46\u662f\u597d\u50cf\u4e0d\u662f\u5728\u5f88\u591a\u95ee\u9898\u4e2d\u80fd\u591f\u901a\u7528\u7684\uff0c\u6bd4\u5982X\u5305\u542b\u5f88\u591a\u7684feature\u4e2d\uff0c\u53ef\u89c6\u5316\u5c31\u5f88\u96be\u3002 \u901a\u8fc7\u4e0a\u9762\u7684\u53ef\u89c6\u5316\u5c31\u80fd\u591f\u53d1\u73b0\u73b0\u6709\u7684\u6a21\u578b\u95ee\u9898\u51fa\u5728\u54ea\u91cc\uff0c\u63a5\u4e0b\u6765\u662f\u5bfb\u627e\u6709\u6ca1\u6709\u5408\u9002\u7684\u624b\u6bb5\u80fd\u591f\u4f18\u5316\u81ea\u5df1\u7684\u6a21\u578b\u5462\uff1f\u7b80\u5355\u7684\u6765\u8bf4\u5c31\u662f\u8c03\u8282\u8d85\u53c2\u6570\u3002</p> In\u00a0[45]: Copied! <pre>class CircleModelV1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        \n    def forward(self, x): # note: always make sure forward is spelt correctly!\n        # Creating a model like this is the same as below, though below\n        # generally benefits from speedups where possible.\n        # z = self.layer_1(x)\n        # z = self.layer_2(z)\n        # z = self.layer_3(z)\n        # return z\n        return self.layer_3(self.layer_2(self.layer_1(x)))\n\nmodel_1 = CircleModelV1().to(device)\nmodel_1\n</pre> class CircleModelV1(nn.Module):     def __init__(self):         super().__init__()         self.layer_1 = nn.Linear(in_features=2, out_features=10)         self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer         self.layer_3 = nn.Linear(in_features=10, out_features=1)              def forward(self, x): # note: always make sure forward is spelt correctly!         # Creating a model like this is the same as below, though below         # generally benefits from speedups where possible.         # z = self.layer_1(x)         # z = self.layer_2(z)         # z = self.layer_3(z)         # return z         return self.layer_3(self.layer_2(self.layer_1(x)))  model_1 = CircleModelV1().to(device) model_1 Out[45]: <pre>CircleModelV1(\n  (layer_1): Linear(in_features=2, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n)</pre> <p>Now we've got a model, we'll recreate a loss function and optimizer instance, using the same settings as before.</p> In\u00a0[46]: Copied! <pre># loss_fn = nn.BCELoss() # Requires sigmoid on input\nloss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input\noptimizer = torch.optim.SGD(model_1.parameters(), lr=0.1)\n</pre> # loss_fn = nn.BCELoss() # Requires sigmoid on input loss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input optimizer = torch.optim.SGD(model_1.parameters(), lr=0.1) <p>Beautiful, model, optimizer and loss function ready, let's make a training loop.</p> <p>This time we'll train for longer (<code>epochs=1000</code> vs <code>epochs=100</code>) and see if it improves our model.</p> In\u00a0[47]: Copied! <pre>torch.manual_seed(42)\n\nepochs = 1000 # Train for longer\n\n# Put data to target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    # 1. Forward pass\n    y_logits = model_1(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; predicition probabilities -&gt; prediction labels\n\n    # 2. Calculate loss/accuracy\n    loss = loss_fn(y_logits, y_train)\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_1.eval()\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_1(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. Caculate loss/accuracy\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # Print out what's happening every 10 epochs\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n</pre> torch.manual_seed(42)  epochs = 1000 # Train for longer  # Put data to target device X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)  for epoch in range(epochs):     ### Training     # 1. Forward pass     y_logits = model_1(X_train).squeeze()     y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; predicition probabilities -&gt; prediction labels      # 2. Calculate loss/accuracy     loss = loss_fn(y_logits, y_train)     acc = accuracy_fn(y_true=y_train,                        y_pred=y_pred)      # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_1.eval()     with torch.inference_mode():         # 1. Forward pass         test_logits = model_1(X_test).squeeze()          test_pred = torch.round(torch.sigmoid(test_logits))         # 2. Caculate loss/accuracy         test_loss = loss_fn(test_logits,                             y_test)         test_acc = accuracy_fn(y_true=y_test,                                y_pred=test_pred)      # Print out what's happening every 10 epochs     if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")  <pre>Epoch: 0 | Loss: 0.69396, Accuracy: 50.88% | Test loss: 0.69261, Test acc: 51.00%\nEpoch: 100 | Loss: 0.69305, Accuracy: 50.38% | Test loss: 0.69379, Test acc: 48.00%\nEpoch: 200 | Loss: 0.69299, Accuracy: 51.12% | Test loss: 0.69437, Test acc: 46.00%\nEpoch: 300 | Loss: 0.69298, Accuracy: 51.62% | Test loss: 0.69458, Test acc: 45.00%\nEpoch: 400 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69465, Test acc: 46.00%\nEpoch: 500 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69467, Test acc: 46.00%\nEpoch: 600 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 700 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 800 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 900 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\n</pre> <p>What? Our model trained for longer and with an extra layer but it still looks like it didn't learn any patterns better than random guessing.</p> <p>Let's visualize.</p> In\u00a0[48]: Copied! <pre># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_1, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_1, X_test, y_test)\n</pre> # Plot decision boundaries for training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_1, X_train, y_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_1, X_test, y_test) <p>Hmmm.</p> <p>Our model is still drawing a straight line between the red and blue dots.</p> <p>If our model is drawing a straight line, could it model linear data? Like we did in notebook 01?</p> In\u00a0[49]: Copied! <pre># Create some data (same as notebook 01)\nweight = 0.7\nbias = 0.3\nstart = 0\nend = 1\nstep = 0.01\n\n# Create data\nX_regression = torch.arange(start, end, step).unsqueeze(dim=1)\ny_regression = weight * X_regression + bias # linear regression formula\n\n# Check the data\nprint(len(X_regression))\nX_regression[:5], y_regression[:5]\n</pre> # Create some data (same as notebook 01) weight = 0.7 bias = 0.3 start = 0 end = 1 step = 0.01  # Create data X_regression = torch.arange(start, end, step).unsqueeze(dim=1) y_regression = weight * X_regression + bias # linear regression formula  # Check the data print(len(X_regression)) X_regression[:5], y_regression[:5] <pre>100\n</pre> Out[49]: <pre>(tensor([[0.0000],\n         [0.0100],\n         [0.0200],\n         [0.0300],\n         [0.0400]]),\n tensor([[0.3000],\n         [0.3070],\n         [0.3140],\n         [0.3210],\n         [0.3280]]))</pre> <p>Wonderful, now let's split our data into training and test sets.</p> In\u00a0[50]: Copied! <pre># Create train and test splits\ntrain_split = int(0.8 * len(X_regression)) # 80% of data used for training set\nX_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\nX_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n\n# Check the lengths of each split\nprint(len(X_train_regression), \n    len(y_train_regression), \n    len(X_test_regression), \n    len(y_test_regression))\n</pre> # Create train and test splits train_split = int(0.8 * len(X_regression)) # 80% of data used for training set X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split] X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]  # Check the lengths of each split print(len(X_train_regression),      len(y_train_regression),      len(X_test_regression),      len(y_test_regression)) <pre>80 80 20 20\n</pre> <p>Beautiful, let's see how the data looks.</p> <p>To do so, we'll use the <code>plot_predictions()</code> function we created in notebook 01.</p> <p>It's contained within the <code>helper_functions.py</code> script on the Learn PyTorch for Deep Learning repo which we downloaded above.</p> In\u00a0[51]: Copied! <pre>plot_predictions(train_data=X_train_regression,\n    train_labels=y_train_regression,\n    test_data=X_test_regression,\n    test_labels=y_test_regression\n);\n</pre> plot_predictions(train_data=X_train_regression,     train_labels=y_train_regression,     test_data=X_test_regression,     test_labels=y_test_regression ); In\u00a0[52]: Copied! <pre># Same architecture as model_1 (but using nn.Sequential)\nmodel_2 = nn.Sequential(\n    nn.Linear(in_features=1, out_features=10),\n    nn.Linear(in_features=10, out_features=10),\n    nn.Linear(in_features=10, out_features=1)\n).to(device)\n\nmodel_2\n</pre> # Same architecture as model_1 (but using nn.Sequential) model_2 = nn.Sequential(     nn.Linear(in_features=1, out_features=10),     nn.Linear(in_features=10, out_features=10),     nn.Linear(in_features=10, out_features=1) ).to(device)  model_2 Out[52]: <pre>Sequential(\n  (0): Linear(in_features=1, out_features=10, bias=True)\n  (1): Linear(in_features=10, out_features=10, bias=True)\n  (2): Linear(in_features=10, out_features=1, bias=True)\n)</pre> <p>We'll setup the loss function to be <code>nn.L1Loss()</code> (the same as mean absolute error) and the optimizer to be <code>torch.optim.SGD()</code>.</p> In\u00a0[53]: Copied! <pre># Loss and optimizer\nloss_fn = nn.L1Loss()\noptimizer = torch.optim.SGD(model_2.parameters(), lr=0.1)\n</pre> # Loss and optimizer loss_fn = nn.L1Loss() optimizer = torch.optim.SGD(model_2.parameters(), lr=0.1) <p>Now let's train the model using the regular training loop steps for <code>epochs=1000</code> (just like <code>model_1</code>).</p> <p>Note: We've been writing similar training loop code over and over again. I've made it that way on purpose though, to keep practicing. However, do you have ideas how we could functionize this? That would save a fair bit of coding in the future. Potentially there could be a function for training and a function for testing.</p> In\u00a0[54]: Copied! <pre># Train the model\ntorch.manual_seed(42)\n\n# Set the number of epochs\nepochs = 1000\n\n# Put data to target device\nX_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\nX_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n\nfor epoch in range(epochs):\n    ### Training \n    # 1. Forward pass\n    y_pred = model_2(X_train_regression)\n    \n    # 2. Calculate loss (no accuracy since it's a regression problem, not classification)\n    loss = loss_fn(y_pred, y_train_regression)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_2.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_pred = model_2(X_test_regression)\n      # 2. Calculate the loss \n      test_loss = loss_fn(test_pred, y_test_regression)\n\n    # Print out what's happening\n    if epoch % 100 == 0: \n        print(f\"Epoch: {epoch} | Train loss: {loss:.5f}, Test loss: {test_loss:.5f}\")\n</pre> # Train the model torch.manual_seed(42)  # Set the number of epochs epochs = 1000  # Put data to target device X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device) X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)  for epoch in range(epochs):     ### Training      # 1. Forward pass     y_pred = model_2(X_train_regression)          # 2. Calculate loss (no accuracy since it's a regression problem, not classification)     loss = loss_fn(y_pred, y_train_regression)      # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_2.eval()     with torch.inference_mode():       # 1. Forward pass       test_pred = model_2(X_test_regression)       # 2. Calculate the loss        test_loss = loss_fn(test_pred, y_test_regression)      # Print out what's happening     if epoch % 100 == 0:          print(f\"Epoch: {epoch} | Train loss: {loss:.5f}, Test loss: {test_loss:.5f}\") <pre>Epoch: 0 | Train loss: 0.75986, Test loss: 0.54143\nEpoch: 100 | Train loss: 0.09309, Test loss: 0.02901\nEpoch: 200 | Train loss: 0.07376, Test loss: 0.02850\nEpoch: 300 | Train loss: 0.06745, Test loss: 0.00615\nEpoch: 400 | Train loss: 0.06107, Test loss: 0.02004\nEpoch: 500 | Train loss: 0.05698, Test loss: 0.01061\nEpoch: 600 | Train loss: 0.04857, Test loss: 0.01326\nEpoch: 700 | Train loss: 0.06109, Test loss: 0.02127\nEpoch: 800 | Train loss: 0.05599, Test loss: 0.01426\nEpoch: 900 | Train loss: 0.05571, Test loss: 0.00603\n</pre> <p>Okay, unlike <code>model_1</code> on the classification data, it looks like <code>model_2</code>'s loss is actually going down.</p> <p>Let's plot its predictions to see if that's so.</p> <p>And remember, since our model and data are using the target <code>device</code>, and this device may be a GPU, however, our plotting function uses matplotlib and matplotlib can't handle data on the GPU.</p> <p>To handle that, we'll send all of our data to the CPU using <code>.cpu()</code> when we pass it to <code>plot_predictions()</code>.</p> In\u00a0[55]: Copied! <pre># Turn on evaluation mode\nmodel_2.eval()\n\n# Make predictions (inference)\nwith torch.inference_mode():\n    y_preds = model_2(X_test_regression)\n\n# Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU)\n# (try removing .cpu() from one of the below and see what happens)\nplot_predictions(train_data=X_train_regression.cpu(),\n                 train_labels=y_train_regression.cpu(),\n                 test_data=X_test_regression.cpu(),\n                 test_labels=y_test_regression.cpu(),\n                 predictions=y_preds.cpu());\n</pre> # Turn on evaluation mode model_2.eval()  # Make predictions (inference) with torch.inference_mode():     y_preds = model_2(X_test_regression)  # Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU) # (try removing .cpu() from one of the below and see what happens) plot_predictions(train_data=X_train_regression.cpu(),                  train_labels=y_train_regression.cpu(),                  test_data=X_test_regression.cpu(),                  test_labels=y_test_regression.cpu(),                  predictions=y_preds.cpu()); <p>Alright, it looks like our model is able to do far better than random guessing on straight lines.</p> <p>This is a good thing.</p> <p>It means our model at least has some capacity to learn.</p> <p>Note: A helpful troubleshooting step when building deep learning models is to start as small as possible to see if the model works before scaling it up.</p> <p>This could mean starting with a simple neural network (not many layers, not many hidden neurons) and a small dataset (like the one we've made) and then overfitting (making the model perform too well) on that small example before increasing the amount data or the model size/design to reduce overfitting.</p> <p>So what could it be?</p> <p>Let's find out.</p> In\u00a0[56]: Copied! <pre># Make and plot data\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_circles\n\nn_samples = 1000\n\nX, y = make_circles(n_samples=1000,\n    noise=0.03,\n    random_state=42,\n)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu);\n</pre> # Make and plot data import matplotlib.pyplot as plt from sklearn.datasets import make_circles  n_samples = 1000  X, y = make_circles(n_samples=1000,     noise=0.03,     random_state=42, )  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu); <p>Nice! Now let's split it into training and test sets using 80% of the data for training and 20% for testing.</p> In\u00a0[57]: Copied! <pre># Convert to tensors and split into train and test sets\nimport torch\nfrom sklearn.model_selection import train_test_split\n\n# Turn data into tensors\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2,\n                                                    random_state=42\n)\n\nX_train[:5], y_train[:5]\n</pre> # Convert to tensors and split into train and test sets import torch from sklearn.model_selection import train_test_split  # Turn data into tensors X = torch.from_numpy(X).type(torch.float) y = torch.from_numpy(y).type(torch.float)  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                      test_size=0.2,                                                     random_state=42 )  X_train[:5], y_train[:5] Out[57]: <pre>(tensor([[ 0.6579, -0.4651],\n         [ 0.6319, -0.7347],\n         [-1.0086, -0.1240],\n         [-0.9666, -0.2256],\n         [-0.1666,  0.7994]]),\n tensor([1., 0., 0., 0., 1.]))</pre> <p>\u6574\u4e2a\u5185\u5bb9\u597d\u884c\u90fd\u662f\u5728\u63cf\u8ff0\u6fc0\u6d3b\u51fd\u6570\u7684\u91cd\u8981\u6027\u3002</p> <p>\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u6fc0\u6d3b\u51fd\u6570\u88ab\u7528\u4e8e\u5728\u795e\u7ecf\u7f51\u7edc\u7684\u6bcf\u4e2a\u795e\u7ecf\u5143\u4e0a\u5f15\u5165\u975e\u7ebf\u6027\u3002\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u795e\u7ecf\u5143\u63a5\u6536\u4e00\u7ec4\u8f93\u5165\uff0c\u5bf9\u8fd9\u4e9b\u8f93\u5165\u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u5e76\u901a\u8fc7\u6fc0\u6d3b\u51fd\u6570\u8ba1\u7b97\u8f93\u51fa\u3002\u6fc0\u6d3b\u51fd\u6570\u7684\u4f5c\u7528\u662f\u5c06\u8fd9\u4e2a\u8f93\u51fa\u8f6c\u6362\u4e3a\u4e00\u4e2a\u975e\u7ebf\u6027\u7684\u5f62\u5f0f\uff0c\u4ee5\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u793a\u80fd\u529b\u3002</p> <p>\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570\u7684\u597d\u5904\u662f\u5b83\u53ef\u4ee5\u4f7f\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u66f4\u52a0\u7075\u6d3b\u3002\u5982\u679c\u6ca1\u6709\u6fc0\u6d3b\u51fd\u6570\uff0c\u795e\u7ecf\u7f51\u7edc\u5c31\u5c06\u53d8\u6210\u4e00\u4e2a\u7b80\u5355\u7684\u7ebf\u6027\u6a21\u578b\uff0c\u8fd9\u610f\u5473\u7740\u795e\u7ecf\u7f51\u7edc\u53ea\u80fd\u591f\u5b66\u4e60\u5230\u8f93\u5165\u6570\u636e\u7684\u7ebf\u6027\u5173\u7cfb\uff0c\u800c\u65e0\u6cd5\u5b66\u4e60\u5230\u66f4\u52a0\u590d\u6742\u7684\u6a21\u5f0f\u3002</p> <p>\u53e6\u5916\uff0c\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570\u8fd8\u53ef\u4ee5\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u3002\u56e0\u4e3a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u662f\u4e00\u4e2a\u9ad8\u5ea6\u975e\u7ebf\u6027\u7684\u51fd\u6570\uff0c\u5b83\u7684\u8bad\u7ec3\u8fc7\u7a0b\u9700\u8981\u4f7f\u7528\u975e\u7ebf\u6027\u7684\u4f18\u5316\u7b97\u6cd5\u3002\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570\u53ef\u4ee5\u4f7f\u5f97\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u8bef\u5dee\u51fd\u6570\u66f4\u52a0\u590d\u6742\uff0c\u4ece\u800c\u589e\u52a0\u4e86\u4f18\u5316\u95ee\u9898\u7684\u96be\u5ea6\u3002\u4f46\u540c\u65f6\uff0c\u8fd9\u4e5f\u4f7f\u5f97\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u80fd\u591f\u9002\u5e94\u66f4\u52a0\u590d\u6742\u7684\u6570\u636e\u5206\u5e03\uff0c\u5e76\u5177\u6709\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u3002</p> <p>\u4e00\u4e9b\u5e38\u7528\u7684\u6fc0\u6d3b\u51fd\u6570\u5305\u62ecSigmoid\u51fd\u6570\u3001Tanh\u51fd\u6570\u3001ReLU\u51fd\u6570\u3001LeakyReLU\u51fd\u6570\u3001ELU\u51fd\u6570\u7b49\u3002\u4e0d\u540c\u7684\u6fc0\u6d3b\u51fd\u6570\u5177\u6709\u4e0d\u540c\u7684\u4f18\u7f3a\u70b9\uff0c\u901a\u5e38\u9700\u8981\u6839\u636e\u5177\u4f53\u7684\u4efb\u52a1\u6765\u9009\u62e9\u9002\u5408\u7684\u6fc0\u6d3b\u51fd\u6570\u3002</p> In\u00a0[58]: Copied! <pre># Build model with non-linear activation function\nfrom torch import nn\nclass CircleModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        self.relu = nn.ReLU() # &lt;- add in ReLU activation function\n        # Can also put sigmoid in the model \n        # This would mean you don't need to use it on the predictions\n        # self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n      # Intersperse the ReLU activation function between layers\n       return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n\nmodel_3 = CircleModelV2().to(device)\nprint(model_3)\n</pre> # Build model with non-linear activation function from torch import nn class CircleModelV2(nn.Module):     def __init__(self):         super().__init__()         self.layer_1 = nn.Linear(in_features=2, out_features=10)         self.layer_2 = nn.Linear(in_features=10, out_features=10)         self.layer_3 = nn.Linear(in_features=10, out_features=1)         self.relu = nn.ReLU() # &lt;- add in ReLU activation function         # Can also put sigmoid in the model          # This would mean you don't need to use it on the predictions         # self.sigmoid = nn.Sigmoid()      def forward(self, x):       # Intersperse the ReLU activation function between layers        return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))  model_3 = CircleModelV2().to(device) print(model_3) <pre>CircleModelV2(\n  (layer_1): Linear(in_features=2, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n  (relu): ReLU()\n)\n</pre> <p> A visual example of what a similar classificiation neural network to the one we've just built (suing ReLU activation) looks like. Try create one of your own on the TensorFlow Playground website.</p> <p>Question: Where should I put the non-linear activation functions when constructing a neural network?</p> <p>A rule of thumb is to put them in between hidden layers and just after the output layer, however, there is no set in stone option. As you learn more about neural networks and deep learning you'll find a bunch of different ways of putting things together. In the meantine, best to experiment, experiment, experiment.</p> <p>Now we've got a model ready to go, let's create a binary classification loss function as well as an optimizer.</p> In\u00a0[59]: Copied! <pre># Setup loss and optimizer \nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model_3.parameters(), lr=0.1)\n</pre> # Setup loss and optimizer  loss_fn = nn.BCEWithLogitsLoss() optimizer = torch.optim.SGD(model_3.parameters(), lr=0.1) <p>Wonderful!</p> In\u00a0[60]: Copied! <pre># Fit the model\ntorch.manual_seed(42)\nepochs = 1000\n\n# Put all data on target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    # 1. Forward pass\n    y_logits = model_3(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels\n    \n    # 2. Calculate loss and accuracy\n    loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n    \n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_3.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_logits = model_3(X_test).squeeze()\n      test_pred = torch.round(torch.sigmoid(test_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels\n      # 2. Calcuate loss and accuracy\n      test_loss = loss_fn(test_logits, y_test)\n      test_acc = accuracy_fn(y_true=y_test,\n                             y_pred=test_pred)\n\n    # Print out what's happening\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n</pre> # Fit the model torch.manual_seed(42) epochs = 1000  # Put all data on target device X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)  for epoch in range(epochs):     # 1. Forward pass     y_logits = model_3(X_train).squeeze()     y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels          # 2. Calculate loss and accuracy     loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits     acc = accuracy_fn(y_true=y_train,                        y_pred=y_pred)          # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backward     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_3.eval()     with torch.inference_mode():       # 1. Forward pass       test_logits = model_3(X_test).squeeze()       test_pred = torch.round(torch.sigmoid(test_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels       # 2. Calcuate loss and accuracy       test_loss = loss_fn(test_logits, y_test)       test_acc = accuracy_fn(y_true=y_test,                              y_pred=test_pred)      # Print out what's happening     if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\") <pre>Epoch: 0 | Loss: 0.69295, Accuracy: 50.00% | Test Loss: 0.69319, Test Accuracy: 50.00%\nEpoch: 100 | Loss: 0.69115, Accuracy: 52.88% | Test Loss: 0.69102, Test Accuracy: 52.50%\nEpoch: 200 | Loss: 0.68977, Accuracy: 53.37% | Test Loss: 0.68940, Test Accuracy: 55.00%\nEpoch: 300 | Loss: 0.68795, Accuracy: 53.00% | Test Loss: 0.68723, Test Accuracy: 56.00%\nEpoch: 400 | Loss: 0.68517, Accuracy: 52.75% | Test Loss: 0.68411, Test Accuracy: 56.50%\nEpoch: 500 | Loss: 0.68102, Accuracy: 52.75% | Test Loss: 0.67941, Test Accuracy: 56.50%\nEpoch: 600 | Loss: 0.67515, Accuracy: 54.50% | Test Loss: 0.67285, Test Accuracy: 56.00%\nEpoch: 700 | Loss: 0.66659, Accuracy: 58.38% | Test Loss: 0.66322, Test Accuracy: 59.00%\nEpoch: 800 | Loss: 0.65160, Accuracy: 64.00% | Test Loss: 0.64757, Test Accuracy: 67.50%\nEpoch: 900 | Loss: 0.62362, Accuracy: 74.00% | Test Loss: 0.62145, Test Accuracy: 79.00%\n</pre> <p>Ho ho! That's looking far better!</p> In\u00a0[61]: Copied! <pre># Make predictions\nmodel_3.eval()\nwith torch.inference_mode():\n    y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\ny_preds[:10], y[:10] # want preds in same format as truth labels\n</pre> # Make predictions model_3.eval() with torch.inference_mode():     y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze() y_preds[:10], y[:10] # want preds in same format as truth labels Out[61]: <pre>(tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 0.], device='cuda:0'),\n tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0.]))</pre> In\u00a0[62]: Copied! <pre># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity\n</pre> # Plot decision boundaries for training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity <p>Nice! Not perfect but still far better than before.</p> <p>Potentially you could try a few tricks to improve the test accuracy of the model? (hint: head back to section 5 for tips on improving the model)</p> In\u00a0[63]: Copied! <pre># Create a toy tensor (similar to the data going into our model(s))\nA = torch.arange(-10, 10, 1, dtype=torch.float32)\nA\n</pre> # Create a toy tensor (similar to the data going into our model(s)) A = torch.arange(-10, 10, 1, dtype=torch.float32) A Out[63]: <pre>tensor([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,\n          2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.])</pre> <p>Wonderful, now let's plot it.</p> In\u00a0[64]: Copied! <pre># Visualize the toy tensor\nplt.plot(A);\n</pre> # Visualize the toy tensor plt.plot(A); <p>A straight line, nice.</p> <p>Now let's see how the ReLU activation function influences it.</p> <p>And instead of using PyTorch's ReLU (<code>torch.nn.ReLU</code>), we'll recreate it ourselves.</p> <p>The ReLU function turns all negatives to 0 and leaves the positive values as they are.</p> In\u00a0[65]: Copied! <pre># Create ReLU function by hand \ndef relu(x):\n  return torch.maximum(torch.tensor(0), x) # inputs must be tensors\n\n# Pass toy tensor through ReLU function\nrelu(A)\n</pre> # Create ReLU function by hand  def relu(x):   return torch.maximum(torch.tensor(0), x) # inputs must be tensors  # Pass toy tensor through ReLU function relu(A) Out[65]: <pre>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 7.,\n        8., 9.])</pre> <p>It looks like our ReLU function worked, all of the negative values are zeros.</p> <p>Let's plot them.</p> In\u00a0[66]: Copied! <pre># Plot ReLU activated toy tensor\nplt.plot(relu(A));\n</pre> # Plot ReLU activated toy tensor plt.plot(relu(A)); <p>Nice! That looks exactly like the shape of the ReLU function on the Wikipedia page for ReLU).</p> <p>How about we try the sigmoid function we've been using?</p> <p>The sigmoid function formula goes like so:</p> <p>$$ out_i = \\frac{1}{1+e^{-input_i}} $$</p> <p>Or using $x$ as input:</p> <p>$$ S(x) = \\frac{1}{1+e^{-x_i}} $$</p> <p>Where $S$ stands for sigmoid, $e$ stands for exponential (<code>torch.exp()</code>) and $i$ stands for a particular element in a tensor.</p> <p>Let's build a function to replicate the sigmoid function with PyTorch.</p> In\u00a0[67]: Copied! <pre># Create a custom sigmoid function\ndef sigmoid(x):\n  return 1 / (1 + torch.exp(-x))\n\n# Test custom sigmoid on toy tensor\nsigmoid(A)\n</pre> # Create a custom sigmoid function def sigmoid(x):   return 1 / (1 + torch.exp(-x))  # Test custom sigmoid on toy tensor sigmoid(A) Out[67]: <pre>tensor([4.5398e-05, 1.2339e-04, 3.3535e-04, 9.1105e-04, 2.4726e-03, 6.6929e-03,\n        1.7986e-02, 4.7426e-02, 1.1920e-01, 2.6894e-01, 5.0000e-01, 7.3106e-01,\n        8.8080e-01, 9.5257e-01, 9.8201e-01, 9.9331e-01, 9.9753e-01, 9.9909e-01,\n        9.9966e-01, 9.9988e-01])</pre> <p>Woah, those values look a lot like prediction probabilities we've seen earlier, let's see what they look like visualized.</p> In\u00a0[68]: Copied! <pre># Plot sigmoid activated toy tensor\nplt.plot(sigmoid(A));\n</pre> # Plot sigmoid activated toy tensor plt.plot(sigmoid(A)); <p>Looking good! We've gone from a straight line to a curved line.</p> <p>Now there's plenty more non-linear activation functions that exist in PyTorch that we haven't tried.</p> <p>But these two are two of the most common.</p> <p>And the point remains, what patterns could you draw using an unlimited amount of linear (straight) and non-linear (not straight) lines?</p> <p>Almost anything right?</p> <p>That's exactly what our model is doing when we combine linear and non-linear functions.</p> <p>Instead of telling our model what to do, we give it tools to figure out how to best discover patterns in the data.</p> <p>And those tools are linear and non-linear functions.</p> In\u00a0[69]: Copied! <pre># Import dependencies\nimport torch\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\n\n# Set the hyperparameters for data creation\nNUM_CLASSES = 4\nNUM_FEATURES = 2\nRANDOM_SEED = 42\n\n# 1. Create multi-class data\nX_blob, y_blob = make_blobs(n_samples=1000,\n    n_features=NUM_FEATURES, # X features\n    centers=NUM_CLASSES, # y labels \n    cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)\n    random_state=RANDOM_SEED\n)\n\n# 2. Turn data into tensors\nX_blob = torch.from_numpy(X_blob).type(torch.float)\ny_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\nprint(X_blob[:5], y_blob[:5])\n\n# 3. Split into train and test sets\nX_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,\n    y_blob,\n    test_size=0.2,\n    random_state=RANDOM_SEED\n)\n\n# 4. Plot data\nplt.figure(figsize=(10, 7))\nplt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);\n</pre> # Import dependencies import torch import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split  # Set the hyperparameters for data creation NUM_CLASSES = 4 NUM_FEATURES = 2 RANDOM_SEED = 42  # 1. Create multi-class data X_blob, y_blob = make_blobs(n_samples=1000,     n_features=NUM_FEATURES, # X features     centers=NUM_CLASSES, # y labels      cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)     random_state=RANDOM_SEED )  # 2. Turn data into tensors X_blob = torch.from_numpy(X_blob).type(torch.float) y_blob = torch.from_numpy(y_blob).type(torch.LongTensor) print(X_blob[:5], y_blob[:5])  # 3. Split into train and test sets X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,     y_blob,     test_size=0.2,     random_state=RANDOM_SEED )  # 4. Plot data plt.figure(figsize=(10, 7)) plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu); <pre>tensor([[-8.4134,  6.9352],\n        [-5.7665, -6.4312],\n        [-6.0421, -6.7661],\n        [ 3.9508,  0.6984],\n        [ 4.2505, -0.2815]]) tensor([3, 2, 2, 1, 1])\n</pre> <p>Nice! Looks like we've got some multi-class data ready to go.</p> <p>Let's build a model to separate the coloured blobs.</p> <p>Question: Does this dataset need non-linearity? Or could you draw a succession of straight lines to separate it?</p> In\u00a0[70]: Copied! <pre># Create device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Create device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[70]: <pre>'cuda'</pre> In\u00a0[71]: Copied! <pre>from torch import nn\n\n# Build model\nclass BlobModel(nn.Module):\n    def __init__(self, input_features, output_features, hidden_units=8):\n\"\"\"Initializes all required hyperparameters for a multi-class classification model.\n\n        Args:\n            input_features (int): Number of input features to the model.\n            out_features (int): Number of output features of the model\n              (how many classes there are).\n            hidden_units (int): Number of hidden units between layers, default 8.\n        \"\"\"\n        super().__init__()\n        self.linear_layer_stack = nn.Sequential(\n            nn.Linear(in_features=input_features, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n            nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?\n        )\n    \n    def forward(self, x):\n        return self.linear_layer_stack(x)\n\n# Create an instance of BlobModel and send it to the target device\nmodel_4 = BlobModel(input_features=NUM_FEATURES, \n                    output_features=NUM_CLASSES, \n                    hidden_units=8).to(device)\nmodel_4\n</pre> from torch import nn  # Build model class BlobModel(nn.Module):     def __init__(self, input_features, output_features, hidden_units=8):         \"\"\"Initializes all required hyperparameters for a multi-class classification model.          Args:             input_features (int): Number of input features to the model.             out_features (int): Number of output features of the model               (how many classes there are).             hidden_units (int): Number of hidden units between layers, default 8.         \"\"\"         super().__init__()         self.linear_layer_stack = nn.Sequential(             nn.Linear(in_features=input_features, out_features=hidden_units),             # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)             nn.Linear(in_features=hidden_units, out_features=hidden_units),             # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)             nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?         )          def forward(self, x):         return self.linear_layer_stack(x)  # Create an instance of BlobModel and send it to the target device model_4 = BlobModel(input_features=NUM_FEATURES,                      output_features=NUM_CLASSES,                      hidden_units=8).to(device) model_4 Out[71]: <pre>BlobModel(\n  (linear_layer_stack): Sequential(\n    (0): Linear(in_features=2, out_features=8, bias=True)\n    (1): Linear(in_features=8, out_features=8, bias=True)\n    (2): Linear(in_features=8, out_features=4, bias=True)\n  )\n)</pre> <p>Excellent! Our multi-class model is ready to go, let's create a loss function and optimizer for it.</p> In\u00a0[72]: Copied! <pre># Create loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model_4.parameters(), \n                            lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance\n</pre> # Create loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model_4.parameters(),                              lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance In\u00a0[73]: Copied! <pre># Perform a single forward pass on the data (we'll need to put it to the target device for it to work)\nmodel_4(X_blob_train.to(device))[:5]\n</pre> # Perform a single forward pass on the data (we'll need to put it to the target device for it to work) model_4(X_blob_train.to(device))[:5] Out[73]: <pre>tensor([[-1.2711, -0.6494, -1.4740, -0.7044],\n        [ 0.2210, -1.5439,  0.0420,  1.1531],\n        [ 2.8698,  0.9143,  3.3169,  1.4027],\n        [ 1.9576,  0.3125,  2.2244,  1.1324],\n        [ 0.5458, -1.2381,  0.4441,  1.1804]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)</pre> <p>What's coming out here?</p> <p>It looks like we get one value per feature of each sample.</p> <p>Let's check the shape to confirm.</p> In\u00a0[74]: Copied! <pre># How many elements in a single prediction sample?\nmodel_4(X_blob_train.to(device))[0].shape, NUM_CLASSES\n</pre> # How many elements in a single prediction sample? model_4(X_blob_train.to(device))[0].shape, NUM_CLASSES  Out[74]: <pre>(torch.Size([4]), 4)</pre> <p>Wonderful, our model is predicting one value for each class that we have.</p> <p>Do you remember what the raw outputs of our model are called?</p> <p>Hint: it rhymes with \"frog splits\" (no animals were harmed in the creation of these materials).</p> <p>If you guessed logits, you'd be correct.</p> <p>So right now our model is outputing logits but what if we wanted to figure out exactly which label is was giving the sample?</p> <p>As in, how do we go from <code>logits -&gt; prediction probabilities -&gt; prediction labels</code> just like we did with the binary classification problem?</p> <p>That's where the softmax activation function comes into play.</p> <p>The softmax function calculates the probability of each prediction class being the actual predicted class compared to all other possible classes.</p> <p>If this doesn't make sense, let's see in code.</p> In\u00a0[75]: Copied! <pre># Make prediction logits with model\ny_logits = model_4(X_test.to(device))\n\n# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\ny_pred_probs = torch.softmax(y_logits, dim=1) \nprint(y_logits[:5])\nprint(y_pred_probs[:5])\n</pre> # Make prediction logits with model y_logits = model_4(X_test.to(device))  # Perform softmax calculation on logits across dimension 1 to get prediction probabilities y_pred_probs = torch.softmax(y_logits, dim=1)  print(y_logits[:5]) print(y_pred_probs[:5]) <pre>tensor([[ 0.2341, -0.3357,  0.2307,  0.2534],\n        [ 0.1198, -0.3702,  0.0998,  0.1887],\n        [ 0.3790, -0.2037,  0.4095,  0.2689],\n        [ 0.1936, -0.3733,  0.1807,  0.2496],\n        [ 0.1338, -0.1378,  0.1487,  0.0247]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\ntensor([[0.2792, 0.1579, 0.2782, 0.2846],\n        [0.2729, 0.1672, 0.2675, 0.2924],\n        [0.2869, 0.1602, 0.2958, 0.2570],\n        [0.2769, 0.1571, 0.2733, 0.2928],\n        [0.2722, 0.2075, 0.2763, 0.2441]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\n</pre> <p>Hmm, what's happened here?</p> <p>It may still look like the outputs of the softmax function are jumbled numbers (and they are, since our model hasn't been trained and is predicting using random patterns) but there's a very specific thing different about each sample.</p> <p>After passing the logits through the softmax function, each individual sample now adds to 1 (or very close to).</p> <p>Let's check.</p> In\u00a0[76]: Copied! <pre># Sum the first sample output of the softmax activation function \ntorch.sum(y_pred_probs[0])\n</pre> # Sum the first sample output of the softmax activation function  torch.sum(y_pred_probs[0]) Out[76]: <pre>tensor(1., device='cuda:0', grad_fn=&lt;SumBackward0&gt;)</pre> <p>These prediction probablities are essentially saying how much the model thinks the target <code>X</code> sample (the input) maps to each class.</p> <p>Since there's one value for each class in <code>y_pred_probs</code>, the index of the highest value is the class the model thinks the specific data sample most belongs to.</p> <p>We can check which index has the highest value using <code>torch.argmax()</code>.</p> In\u00a0[77]: Copied! <pre># Which class does the model think is *most* likely at the index 0 sample?\nprint(y_pred_probs[0])\nprint(torch.argmax(y_pred_probs[0]))\n</pre> # Which class does the model think is *most* likely at the index 0 sample? print(y_pred_probs[0]) print(torch.argmax(y_pred_probs[0])) <pre>tensor([0.2792, 0.1579, 0.2782, 0.2846], device='cuda:0',\n       grad_fn=&lt;SelectBackward0&gt;)\ntensor(3, device='cuda:0')\n</pre> <p>You can see the output of <code>torch.argmax()</code> returns 3, so for the features (<code>X</code>) of the sample at index 0, the model is predicting that the most likely class value (<code>y</code>) is 3.</p> <p>Of course, right now this is just random guessing so it's got a 25% chance of being right (since there's four classes). But we can improve those chances by training the model.</p> <p>Note: To summarize the above, a model's raw output is referred to as logits.</p> <p>For a multi-class classification problem, to turn the logits into prediction probabilities, you use the softmax activation function (<code>torch.softmax</code>).</p> <p>The index of the value with the highest prediction probability is the class number the model thinks is most likely given the input features for that sample (although this is a prediction, it doesn't mean it will be correct).</p> In\u00a0[78]: Copied! <pre># Fit the model\ntorch.manual_seed(42)\n\n# Set number of epochs\nepochs = 100\n\n# Put data to target device\nX_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\nX_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_4.train()\n\n    # 1. Forward pass\n    y_logits = model_4(X_blob_train) # model outputs raw logits \n    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -&gt; prediction probabilities -&gt; prediction labels\n    # print(y_logits)\n    # 2. Calculate loss and accuracy\n    loss = loss_fn(y_logits, y_blob_train) \n    acc = accuracy_fn(y_true=y_blob_train,\n                      y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_4.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_logits = model_4(X_blob_test)\n      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n      # 2. Calculate test loss and accuracy\n      test_loss = loss_fn(test_logits, y_blob_test)\n      test_acc = accuracy_fn(y_true=y_blob_test,\n                             y_pred=test_pred)\n\n    # Print out what's happening\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")\n</pre> # Fit the model torch.manual_seed(42)  # Set number of epochs epochs = 100  # Put data to target device X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device) X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)  for epoch in range(epochs):     ### Training     model_4.train()      # 1. Forward pass     y_logits = model_4(X_blob_train) # model outputs raw logits      y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -&gt; prediction probabilities -&gt; prediction labels     # print(y_logits)     # 2. Calculate loss and accuracy     loss = loss_fn(y_logits, y_blob_train)      acc = accuracy_fn(y_true=y_blob_train,                       y_pred=y_pred)      # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_4.eval()     with torch.inference_mode():       # 1. Forward pass       test_logits = model_4(X_blob_test)       test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)       # 2. Calculate test loss and accuracy       test_loss = loss_fn(test_logits, y_blob_test)       test_acc = accuracy_fn(y_true=y_blob_test,                              y_pred=test_pred)      # Print out what's happening     if epoch % 10 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")  <pre>Epoch: 0 | Loss: 1.04324, Acc: 65.50% | Test Loss: 0.57861, Test Acc: 95.50%\nEpoch: 10 | Loss: 0.14398, Acc: 99.12% | Test Loss: 0.13037, Test Acc: 99.00%\nEpoch: 20 | Loss: 0.08062, Acc: 99.12% | Test Loss: 0.07216, Test Acc: 99.50%\nEpoch: 30 | Loss: 0.05924, Acc: 99.12% | Test Loss: 0.05133, Test Acc: 99.50%\nEpoch: 40 | Loss: 0.04892, Acc: 99.00% | Test Loss: 0.04098, Test Acc: 99.50%\nEpoch: 50 | Loss: 0.04295, Acc: 99.00% | Test Loss: 0.03486, Test Acc: 99.50%\nEpoch: 60 | Loss: 0.03910, Acc: 99.00% | Test Loss: 0.03083, Test Acc: 99.50%\nEpoch: 70 | Loss: 0.03643, Acc: 99.00% | Test Loss: 0.02799, Test Acc: 99.50%\nEpoch: 80 | Loss: 0.03448, Acc: 99.00% | Test Loss: 0.02587, Test Acc: 99.50%\nEpoch: 90 | Loss: 0.03300, Acc: 99.12% | Test Loss: 0.02423, Test Acc: 99.50%\n</pre> In\u00a0[79]: Copied! <pre># Make predictions\nmodel_4.eval()\nwith torch.inference_mode():\n    y_logits = model_4(X_blob_test)\n\n# View the first 10 predictions\ny_logits[:10]\n</pre> # Make predictions model_4.eval() with torch.inference_mode():     y_logits = model_4(X_blob_test)  # View the first 10 predictions y_logits[:10] Out[79]: <pre>tensor([[  4.3377,  10.3539, -14.8948,  -9.7642],\n        [  5.0142, -12.0371,   3.3860,  10.6699],\n        [ -5.5885, -13.3448,  20.9894,  12.7711],\n        [  1.8400,   7.5599,  -8.6016,  -6.9942],\n        [  8.0727,   3.2906, -14.5998,  -3.6186],\n        [  5.5844, -14.9521,   5.0168,  13.2890],\n        [ -5.9739, -10.1913,  18.8655,   9.9179],\n        [  7.0755,  -0.7601,  -9.5531,   0.1736],\n        [ -5.5918, -18.5990,  25.5309,  17.5799],\n        [  7.3142,   0.7197, -11.2017,  -1.2011]], device='cuda:0')</pre> <p>Alright, looks like our model's predictions are still in logit form.</p> <p>Though to evaluate them, they'll have to be in the same form as our labels (<code>y_blob_test</code>) which are in integer form.</p> <p>Let's convert our model's prediction logits to prediction probabilities (using <code>torch.softmax()</code>) then to prediction labels (by taking the <code>argmax()</code> of each sample).</p> <p>Note: It's possible to skip the <code>torch.softmax()</code> function and go straight from <code>predicted logits -&gt; predicted labels</code> by calling <code>torch.argmax()</code> directly on the logits.</p> <p>For example, <code>y_preds = torch.argmax(y_logits, dim=1)</code>, this saves a computation step (no <code>torch.softmax()</code>) but results in no prediction probabilities being available to use.</p> In\u00a0[80]: Copied! <pre># Turn predicted logits in prediction probabilities\ny_pred_probs = torch.softmax(y_logits, dim=1)\n\n# Turn prediction probabilities into prediction labels\ny_preds = y_pred_probs.argmax(dim=1)\n\n# Compare first 10 model preds and test labels\nprint(f\"Predictions: {y_preds[:10]}\\nLabels: {y_blob_test[:10]}\")\nprint(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\")\n</pre> # Turn predicted logits in prediction probabilities y_pred_probs = torch.softmax(y_logits, dim=1)  # Turn prediction probabilities into prediction labels y_preds = y_pred_probs.argmax(dim=1)  # Compare first 10 model preds and test labels print(f\"Predictions: {y_preds[:10]}\\nLabels: {y_blob_test[:10]}\") print(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\") <pre>Predictions: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')\nLabels: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')\nTest accuracy: 99.5%\n</pre> <p>Nice! Our model predictions are now in the same form as our test labels.</p> <p>Let's visualize them with <code>plot_decision_boundary()</code>, remember because our data is on the GPU, we'll have to move it to the CPU for use with matplotlib (<code>plot_decision_boundary()</code> does this automatically for us).</p> In\u00a0[81]: Copied! <pre>plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_4, X_blob_train, y_blob_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_4, X_blob_test, y_blob_test)\n</pre> plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_4, X_blob_train, y_blob_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_4, X_blob_test, y_blob_test) In\u00a0[82]: Copied! <pre>!pip -q install torchmetrics\n\nfrom torchmetrics import Accuracy\n\n# Setup metric and make sure it's on the target device\ntorchmetrics_accuracy = Accuracy().to(device)\n\n# Calculate accuracy\ntorchmetrics_accuracy(y_preds, y_blob_test)\n</pre> !pip -q install torchmetrics  from torchmetrics import Accuracy  # Setup metric and make sure it's on the target device torchmetrics_accuracy = Accuracy().to(device)  # Calculate accuracy torchmetrics_accuracy(y_preds, y_blob_test) <pre>ERROR: Exception:\nTraceback (most recent call last):\n  File \"/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/response.py\", line 425, in _error_catcher\n    yield\n  File \"/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/response.py\", line 507, in read\n    data = self._fp.read(amt) if not fp_closed else b\"\"\n  File \"/usr/share/python-wheels/CacheControl-0.12.6-py2.py3-none-any.whl/cachecontrol/filewrapper.py\", line 62, in read\n    data = self.__fp.read(amt)\n  File \"/usr/lib/python3.8/http/client.py\", line 459, in read\n    n = self.readinto(b)\n  File \"/usr/lib/python3.8/http/client.py\", line 503, in readinto\n    n = self.fp.readinto(b)\n  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\n  File \"/usr/lib/python3.8/ssl.py\", line 1241, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/usr/lib/python3.8/ssl.py\", line 1099, in read\n    return self._sslobj.read(len, buffer)\nsocket.timeout: The read operation timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/base_command.py\", line 186, in _main\n    status = self.run(options, args)\n  File \"/usr/lib/python3/dist-packages/pip/_internal/commands/install.py\", line 357, in run\n    resolver.resolve(requirement_set)\n  File \"/usr/lib/python3/dist-packages/pip/_internal/legacy_resolve.py\", line 177, in resolve\n    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n  File \"/usr/lib/python3/dist-packages/pip/_internal/legacy_resolve.py\", line 333, in _resolve_one\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\n  File \"/usr/lib/python3/dist-packages/pip/_internal/legacy_resolve.py\", line 282, in _get_abstract_dist_for\n    abstract_dist = self.preparer.prepare_linked_requirement(req)\n  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/prepare.py\", line 480, in prepare_linked_requirement\n    local_path = unpack_url(\n  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/prepare.py\", line 282, in unpack_url\n    return unpack_http_url(\n  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/prepare.py\", line 158, in unpack_http_url\n    from_path, content_type = _download_http_url(\n  File \"/usr/lib/python3/dist-packages/pip/_internal/operations/prepare.py\", line 303, in _download_http_url\n    for chunk in download.chunks:\n  File \"/usr/lib/python3/dist-packages/pip/_internal/network/utils.py\", line 15, in response_chunks\n    for chunk in response.raw.stream(\n  File \"/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/response.py\", line 564, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/response.py\", line 529, in read\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n  File \"/usr/lib/python3.8/contextlib.py\", line 131, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/response.py\", line 430, in _error_catcher\n    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\nurllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n</pre> <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[82], line 3\n      1 get_ipython().system('pip -q install torchmetrics')\n----&gt; 3 from torchmetrics import Accuracy\n      5 # Setup metric and make sure it's on the target device\n      6 torchmetrics_accuracy = Accuracy().to(device)\n\nModuleNotFoundError: No module named 'torchmetrics'</pre>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#02-pytorch-neural-network-classification","title":"02. PyTorch Neural Network Classification\u00b6","text":"<p>\u5728\u4e86\u89e3pytorch\u6700\u57fa\u7840\u7684\u77e5\u8bc6\u4e4b\u540e\uff0c\u63a5\u4e0b\u6765\u770b\u7684\u5c31\u662f\u5177\u4f53\u7684\u6df1\u5ea6\u5b66\u4e60\u95ee\u9898\uff0c\u9996\u5148\u4ecb\u7ecd\u7684\u662f\u5206\u7c7b\u95ee\u9898\uff1a\u7b80\u5355\u7684\u7406\u89e3\u5c31\u662f\u67d0\u4e2a\u6570\u636e\u6709\u4e00\u4e2a\uff08\u6216\u591a\u4e2a\uff09label\u6765\u5bf9\u5e94</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#what-is-a-classification-problem","title":"What is a classification problem?\u00b6","text":"<p>A classification problem involves predicting whether something is one thing or another.</p> <p>For example, you might want to:</p> Problem type What is it? Example Binary classification Target can be one of two options, e.g. yes or no Predict whether or not someone has heart disease based on their health parameters. Multi-class classification Target can be one of more than two options Decide whether a photo of is of food, a person or a dog. Multi-label classification Target can be assigned more than one option Predict what categories should be assigned to a Wikipedia article (e.g. mathematics, science &amp; philosohpy). <p></p> <p>Classification, along with regression (predicting a number) is one of the most common types of machine learning problems.</p> <p>In this notebook, we're going to work through a couple of different classification problems with PyTorch.</p> <p>In other words, taking a set of inputs and predicting what class those set of inputs belong to.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>In this notebook we're going to reiterate over the PyTorch workflow we coverd in notebook 01.</p> <p></p> <p>Except instead of trying to predict a straight line (predicting a number, also called a regression problem), we'll be working on a classification problem.</p> <p>Specifically, we're going to cover:</p> Topic Contents 0. Architecture of a classification neural network Neural networks can come in almost any shape or size, but they typically follow a similar floor plan. 1. Getting binary classification data ready Data can be almost anything but to get started we're going to create a simple binary classification dataset. 2. Building a PyTorch classification model Here we'll create a model to learn patterns in the data, we'll also choose a loss function, optimizer and build a training loop specific to classification. 3. Fitting the model to data (training) We've got data and a model, now let's let the model (try to) find patterns in the (training) data. 4. Making predictions and evaluating a model (inference) Our model's found patterns in the data, let's compare its findings to the actual (testing) data. 5. Improving a model (from a model perspective) We've trained an evaluated a model but it's not working, let's try a few things to improve it. 6. Non-linearity So far our model has only had the ability to model straight lines, what about non-linear (non-straight) lines? 7. Replicating non-linear functions We used non-linear functions to help model non-linear data, but what do these look like? 8. Putting it all together with multi-class classification Let's put everything we've done so far for binary classification together with a multi-class classification problem. <p>\u8fd9\u91cc\u8bb2\u4e86\u4e00\u4e0b\u672c\u7ae0\u7684\u5185\u5bb9\u4e0a\u90fd\u5305\u542b\u8fd9\u4ec0\u4e48\uff0c\u4e5f\u53ef\u4ee5\u5bf9\u6bd4\u7684\u770b\u4e00\u4e0b\u5206\u7c7b\u95ee\u9898\u7684\u67b6\u6784\u4e0e\u4e0a\u4e00\u4e2a\u9884\u6d4b\u76f4\u7ebf\u4e4b\u95f4\u6709\u4ec0\u4e48\u6837\u7684\u533a\u522b\u3002</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#where-can-can-you-get-help","title":"Where can can you get help?\u00b6","text":"<p>All of the materials for this course live on GitHub.</p> <p>And if you run into trouble, you can ask a question on the Discussions page there too.</p> <p>There's also the PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#0-architecture-of-a-classification-neural-network","title":"0. Architecture of a classification neural network\u00b6","text":"<p>\u8fd9\u4e00\u90e8\u5206\u7684\u5185\u5bb9\u662f\u4fbf\u603b\u7ed3\u5411\u7684\uff0c\u6709\u70b9\u592a\u8d85\u524d\u4e86\uff0c\u53ef\u4ee5\u56de\u8fc7\u5934\u6765\u518d\u770b\uff0c\u80fd\u66f4\u5bb9\u6613\u7406\u89e3</p> <p>Before we get into writing code, let's look at the general architecture of a classification neural network.</p> Hyperparameter Binary Classification Multiclass classification Input layer shape (<code>in_features</code>) Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) Same as binary classification Hidden layer(s) Problem specific, minimum = 1, maximum = unlimited Same as binary classification Neurons per hidden layer Problem specific, generally 10 to 512 Same as binary classification Output layer shape (<code>out_features</code>) 1 (one class or the other) 1 per class (e.g. 3 for food, person or dog photo) Hidden layer activation Usually ReLU (rectified linear unit) but can be many others Same as binary classification Output activation Sigmoid (<code>torch.sigmoid</code> in PyTorch) Softmax (<code>torch.softmax</code> in PyTorch) Loss function Binary crossentropy (<code>torch.nn.BCELoss</code> in PyTorch) Cross entropy (<code>torch.nn.CrossEntropyLoss</code> in PyTorch) Optimizer SGD (stochastic gradient descent), Adam (see <code>torch.optim</code> for more options) Same as binary classification <p>Of course, this ingredient list of classification neural network components will vary depending on the problem you're working on.</p> <p>But it's more than enough to get started.</p> <p>We're going to gets hands-on with this setup throughout this notebook.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#1-make-classification-data-and-get-it-ready","title":"1. Make classification data and get it ready\u00b6","text":"<p>Let's begin by making some data.</p> <p>We'll use the <code>make_circles()</code> method from Scikit-Learn to generate two circles with different coloured dots.</p> <p>\u8fd9\u91cc\u5176\u5b9e\u5c31\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u5f88\u91cd\u8981\u7684\u5e93<code>sklearn</code>\uff0c\u5b83\u63d0\u4f9b\u4e86\u5f88\u591a\u5f88\u6709\u7528\u7684\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u4f7f\u7528\u7684\u51fd\u6570\uff0c\u6bd4\u5982\u7ed8\u5236ROC\u7b49\u3002\u5728tensorflow\u4e2d\u8981\u642d\u914dsklearn\u7528\u624d\u80fd\u5b8c\u6210\u81ea\u5df1\u7684\u76ee\u7684\uff0c\u4f46\u597d\u50cfpytorch\u4e2d\u9700\u6c42\u6bd4\u8f83\u4f4e\u4e86\uff0c\u56e0\u4e3a\u5df2\u7ecf\u5185\u90e8\u6784\u5efa\u4e86\u5f88\u591a\u7684\u9700\u8981\u7684\u51fd\u6570\uff0c\u4f46\u662f\u8fd8\u662f\u5bf9\u5b83\u6709\u6240\u4e86\u89e3\u6bd4\u8f83\u597d\u3002</p> <p>make_circles \u751f\u6210\u7684\u6570\u636e\u96c6\u5305\u542b\u4e24\u4e2a\u7279\u5f81\u548c\u4e24\u79cd\u7c7b\u522b\uff0c\u5176\u4e2d\u4e24\u4e2a\u7c7b\u522b\u5206\u522b\u4f4d\u4e8e\u4e24\u4e2a\u5706\u73af\u5185\u3002\u8fd9\u4e2a\u6570\u636e\u96c6\u901a\u5e38\u88ab\u7528\u4e8e\u6d4b\u8bd5\u5206\u7c7b\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u975e\u7ebf\u6027\u5206\u7c7b\u7b97\u6cd5\u3002</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#11-input-and-output-shapes","title":"1.1 Input and output shapes\u00b6","text":"<p>One of the most common errors in deep learning is shape errors.</p> <p>Mismatching the shapes of tensors and tensor operations with result in errors in your models. \u56e0\u6b64\u5728\u5177\u4f53\u8fdb\u884c\u5b9e\u9645\u7684\u9879\u76ee\u65f6\uff0c\u9700\u8981\u5bf9\u4e8e\u81ea\u5df1\u7684\u8f93\u5165\uff0c\u6bcf\u4e00\u6b65\u7684\u8f93\u51fa\u6709\u4e00\u5b9a\u7684\u8ba4\u77e5\u3002\u5b66\u4e60\u4e86CNN\u4e4b\u540e\uff0c\u5bf9\u4e8e\u8fd9\u4e2a\u5f62\u72b6\u7684\u95ee\u9898\u53ef\u80fd\u4f1a\u6709\u8f83\u597d\u7684\u7406\u89e3\uff0c\u56e0\u4e3a\u6709\u5377\u79ef\u6838\u7684\u5b58\u5728\u3002</p> <p>We're going to see plenty(\u5bcc\u88d5\u7684\uff0c\u5927\u91cf\u7684\uff09 of these throughout the course.</p> <p>And there's no surefire way to making sure they won't happen, they will.</p> <p>What you can do instead is continaully familiarize yourself with the shape of the data you're working with.</p> <p>I like referring to it as input and output shapes.</p> <p>Ask yourself:</p> <p>\"What shapes are my inputs and what shapes are my outputs?\"</p> <p>Let's find out.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#12-turn-data-into-tensors-and-create-train-and-test-splits","title":"1.2 Turn data into tensors and create train and test splits\u00b6","text":"<p>We've investigated the input and output shapes of our data, now let's prepare it for being used with PyTorch and for modelling.</p> <p>Specifically, we'll need to:</p> <ol> <li>Turn our data into tensors (right now our data is in NumPy arrays and PyTorch prefers to work with PyTorch tensors).</li> <li>Split our data into training and test sets (we'll train a model on the training set to learn the patterns between <code>X</code> and <code>y</code> and then evaluate those learned patterns on the test dataset).</li> </ol>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#2-building-a-model","title":"2. Building a model\u00b6","text":"<p>We've got some data ready, now it's time to build a model.</p> <p>We'll break it down into a few parts.</p> <ol> <li>Setting up device agnostic code (so our model can run on CPU or GPU if it's available).</li> <li>Constructing a model by subclassing <code>nn.Module</code>.</li> <li>Defining a loss function and optimizer.</li> <li>Creating a training loop (this'll be in the next section).</li> </ol> <p>The good news is we've been through all of the above steps before in notebook 01.</p> <p>Except now we'll be adjusting them so they work with a classification dataset.</p> <p>Let's start by importing PyTorch and <code>torch.nn</code> as well as setting up device agnostic code.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#21-setup-loss-function-and-optimizer","title":"2.1 Setup loss function and optimizer\u00b6","text":"<p>We've setup a loss (also called a criterion or cost function) and optimizer before in notebook 01.</p> <p>But different problem types require different loss functions. \u5206\u7c7b\u95ee\u9898\uff0c\u56de\u5f52\u95ee\u9898\u7684\u635f\u5931\u51fd\u6570\u4e0d\u540c\u3002\u6709\u76d1\u7763\u548c\u65e0\u76d1\u7763\u7684\u4e5f\u4e0d\u4e00\u6837\u3002\u800c\u4e14\u5f88\u591a\u6587\u732e\u4e2d\u4e5f\u6709\u6839\u636e\u95ee\u9898\u8bbe\u8ba1\u7684\u81ea\u5df1\u7684\u635f\u5931\u51fd\u6570\u3002</p> <p>For example, for a regression problem (predicting a number) you might used mean absolute error (MAE) loss.</p> <p>And for a binary classification problem (like ours), you'll often use binary cross entropy as the loss function.</p> <p>However, the same optimizer function can often be used across different problem spaces.</p> <p>For example, the stochastic gradient descent optimizer (SGD, <code>torch.optim.SGD()</code>) can be used for a range of problems, so can too the Adam optimizer (<code>torch.optim.Adam()</code>).</p> Loss function/Optimizer Problem type PyTorch Code Stochastic Gradient Descent (SGD) optimizer Classification, regression, many others. <code>torch.optim.SGD()</code> Adam Optimizer Classification, regression, many others. <code>torch.optim.Adam()</code> Binary cross entropy loss Binary classification <code>torch.nn.BCELossWithLogits</code> or <code>torch.nn.BCELoss</code> Cross entropy loss Mutli-class classification <code>torch.nn.CrossEntropyLoss</code> Mean absolute error (MAE) or L1 Loss Regression <code>torch.nn.L1Loss</code> Mean squared error (MSE) or L2 Loss Regression <code>torch.nn.MSELoss</code> <p>Table of various loss functions and optimizers, there are more but these some common ones you'll see.</p> <p>Since we're working with a binary classification problem, let's use a binary cross entropy loss function.</p> <p>Note: Recall a loss function is what measures how wrong your model predictions are, the higher the loss, the worse your model.</p> <p>Also, PyTorch documentation often refers to loss functions as \"loss criterion\uff08\u5c3a\u5ea6\uff0c\u6807\u51c6\uff09\" or \"criterion\", these are all different ways of describing the same thing.</p> <p>PyTorch has two binary cross entropy implementations:</p> <ol> <li><code>torch.nn.BCELoss()</code> - Creates a loss function that measures the binary cross entropy between the target (label) and input (features).</li> <li><code>torch.nn.BCEWithLogitsLoss()</code> - This is the same as above except it has a sigmoid layer (<code>nn.Sigmoid</code>) built-in (we'll see what this means soon).</li> </ol> <p>Which one should you use?</p> <p>The documentation for <code>torch.nn.BCEWithLogitsLoss()</code> states that it's more numerically stable than using <code>torch.nn.BCELoss()</code> after a <code>nn.Sigmoid</code> layer.</p> <p>So generally, implementation 2 is a better option. However for advanced usage, you may want to separate the combination of <code>nn.Sigmoid</code> and <code>torch.nn.BCELoss()</code> but that is beyond the scope of this notebook.</p> <p>Knowing this, let's create a loss function and an optimizer.</p> <p>\u8fd9\u91cc\u8ba8\u8bba\u7684\u95ee\u9898\u5c31\u6709\u70b9\u8fc7\u4e8e\u6df1\u4e86\uff0cBCEWithLogitsLoss\u8981\u6bd4BCELoss+sigmoid\u66f4\u597d\uff0c\u66f4\u7a33\u5b9a\u3002</p> <p>For the optimizer we'll use <code>torch.optim.SGD()</code> to optimize the model parameters with learning rate 0.1.</p> <p>Note: There's a discussion on the PyTorch forums about the use of <code>nn.BCELoss</code> vs. <code>nn.BCEWithLogitsLoss</code>. It can be confusing at first but as with many things, it becomes easier with practice.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#3-train-model","title":"3. Train model\u00b6","text":"<p>Okay, now we've got a loss function and optimizer ready to go, let's train a model.</p> <p>Do you remember the steps in a PyTorch training loop?</p> <p>If not, here's a reminder.</p> <p>Steps in training:</p> PyTorch training loop steps <ol> <li>Forward pass - The model goes through all of the training data once, performing its             <code>forward()</code> function             calculations (<code>model(x_train)</code>).         </li> <li>Calculate the loss - The model's outputs (predictions) are compared to the ground truth and evaluated             to see how             wrong they are (<code>loss = loss_fn(y_pred, y_train</code>).</li> <li>Zero gradients - The optimizers gradients are set to zero (they are accumulated by default) so they             can be             recalculated for the specific training step (<code>optimizer.zero_grad()</code>).</li> <li>Perform backpropagation on the loss - Computes the gradient of the loss with respect for every model             parameter to             be updated (each parameter             with <code>requires_grad=True</code>). This is known as backpropagation, hence \"backwards\"             (<code>loss.backward()</code>).</li> <li>Step the optimizer (gradient descent) - Update the parameters with <code>requires_grad=True</code>             with respect to the loss             gradients in order to improve them (<code>optimizer.step()</code>).</li> </ol>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#31-going-from-raw-model-outputs-to-predicted-labels-logits-prediction-probabilities-prediction-labels","title":"3.1 Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels)\u00b6","text":"<p>Before we the training loop steps, let's see what comes out of our model during the forward pass (the forward pass is defined by the <code>foward()</code> method).</p> <p>To do so, let's pass the model some data.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#32-building-a-training-and-testing-loop","title":"3.2 Building a training and testing loop\u00b6","text":"<p>Alright, we've discussed how to take our raw model outputs and convert them to prediction labels, now let's build a training loop.</p> <p>Let's start by training for 100 epochs and outputing the model's progress every 10 epochs.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#4-make-predictions-and-evaluate-the-model","title":"4. Make predictions and evaluate the model\u00b6","text":"<p>From the metrics it looks like our model is random guessing.</p> <p>How could we investigate this further?</p> <p>I've got an idea.</p> <p>The data explorer's motto!</p> <p>\"Visualize, visualize, visualize!\"</p> <p>Let's make a plot of our model's predictions, the data it's trying to predict on and the decision boundary it's creating for whether something is class 0 or class 1.</p> <p>To do so, we'll write some code to download and import the <code>helper_functions.py</code> script from the Learn PyTorch for Deep Learning repo.</p> <p>It contains a helpful function called <code>plot_decision_boundary()</code> which creates a NumPy meshgrid to visually plot the different points where our model is predicting certain classes.</p> <p>We'll also import <code>plot_predictions()</code> which we wrote in notebook 01 to use later.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#5-improving-a-model-from-a-model-perspective","title":"5. Improving a model (from a model perspective)\u00b6","text":"<p>Let's try to fix our model's underfitting problem.</p> <p>Focusing specifically on the model (not the data), there are a few ways we could do this.</p> Model improvement technique* What does it do? Add more layers Each layer potentially increases the learning capabilities of the model with each layer being able to learn some kind of new pattern in the data, more layers is often referred to as making your neural network deeper. Add more hidden units Similar to the above, more hidden units per layer means a potential increase in learning capabilities of the model, more hidden units is often referred to as making your neural network wider. Fitting for longer (more epochs) Your model might learn more if it had more opportunities to look at the data. Changing the activation functions Some data just can't be fit with only straight lines (like what we've seen), using non-linear activation functions can help with this (hint, hint). Change the learning rate Less model specific, but still related, the learning rate of the optimizer decides how much a model should change its parameters each step, too much and the model overcorrects, too little and it doesn't learn enough. Change the loss function Again, less model specific but still important, different problems require different loss functions. For example, a binary cross entropy loss function won't work with a multi-class classification problem. Use transfer learning Take a pretrained model from a problem domain similar to yours and adjust it to your own problem. We cover transfer learning in notebook 06. <p>Note: *because you can adjust all of these by hand, they're referred to as hyperparameters.</p> <p>And this is also where machine learning's half art half science comes in, there's no real way to know here what the best combination of values is for your project, best to follow the data scientist's motto of \"experiment, experiment, experiment\".</p> <p>Let's see what happens if we add an extra layer to our model, fit for longer (<code>epochs=1000</code> instead of <code>epochs=100</code>) and increase the number of hidden units from <code>5</code> to <code>10</code>.</p> <p>We'll follow the same steps we did above but with a few changed hyperparameters.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#51-preparing-data-to-see-if-our-model-can-model-a-straight-line","title":"5.1 Preparing data to see if our model can model a straight line\u00b6","text":"<p>Let's create some linear data to see if our model's able to model it and we're not just using a model that can't learn anything.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#52-adjusting-model_1-to-fit-a-straight-line","title":"5.2 Adjusting <code>model_1</code> to fit a straight line\u00b6","text":"<p>Now we've got some data, let's recreate <code>model_1</code> but with a loss function suited to our regression data.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#6-the-missing-piece-non-linearity","title":"6. The missing piece: non-linearity\u00b6","text":"<p>We've seen our model can draw straight (linear) lines, thanks to its linear layers.</p> <p>But how about we give it the capacity to draw non-straight (non-linear) lines?</p> <p>How?</p> <p>Let's find out.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#61-recreating-non-linear-data-red-and-blue-circles","title":"6.1 Recreating non-linear data (red and blue circles)\u00b6","text":"<p>First, let's recreate the data to start off fresh. We'll use the same setup as before.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#62-building-a-model-with-non-linearity","title":"6.2 Building a model with non-linearity\u00b6","text":"<p>Now here comes the fun part.</p> <p>What kind of pattern do you think you could draw with unlimited straight (linear) and non-straight (non-linear) lines?</p> <p>I bet you could get pretty creative.</p> <p>So far our neural networks have only been using linear (straight) line functions.</p> <p>But the data we've been working with is non-linear (circles).</p> <p>What do you think will happen when we introduce the capability for our model to use non-linear actviation functions?</p> <p>Well let's see.</p> <p>PyTorch has a bunch of ready-made non-linear activation functions that do similiar but different things.</p> <p>One of the most common and best performing is ReLU (rectified linear-unit, <code>torch.nn.ReLU()</code>).</p> <p>Rather than talk about it, let's put it in our neural network between the hidden layers in the forward pass and see what happens.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#63-training-a-model-with-non-linearity","title":"6.3 Training a model with non-linearity\u00b6","text":"<p>You know the drill, model, loss function, optimizer ready to go, let's create a training and testing loop.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#64-evaluating-a-model-trained-with-non-linear-activation-functions","title":"6.4 Evaluating a model trained with non-linear activation functions\u00b6","text":"<p>Remember how our circle data is non-linear? Well, let's see how our models predictions look now the model's been trained with non-linear activation functions.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#7-replicating-non-linear-activation-functions","title":"7. Replicating non-linear activation functions\u00b6","text":"<p>We saw before how adding non-linear activation functions to our model can helped it to model non-linear data.</p> <p>Note: Much of the data you'll encounter in the wild is non-linear (or a combination of linear and non-linear). Right now we've been working with dots on a 2D plot. But imagine if you had images of plants you'd like to classify, there's a lot of different plant shapes. Or text from Wikipedia you'd like to summarize, there's lots of different ways words can be put together (linear and non-linear patterns).</p> <p>But what does a non-linear activation look like?</p> <p>How about we replicate some and what they do?</p> <p>Let's start by creating a small amount of data.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#8-putting-things-together-by-building-a-multi-class-pytorch-model","title":"8. Putting things together by building a multi-class PyTorch model\u00b6","text":"<p>\u591a\u5206\u7c7b\u7684\u95ee\u9898\uff0c\u4fee\u6539\u635f\u5931\u51fd\u6570\u548c\u6700\u540e\u7684\u8f93\u51fa\u5c42\u5373\u53ef</p> <p>We've covered a fair bit.</p> <p>But now let's put it all together using a multi-class classification problem.</p> <p>Recall a binary classification problem deals with classifying something as one of two options (e.g. a photo as a cat photo or a dog photo) where as a multi-class classification problem deals with classifying something from a list of more than two options (e.g. classifying a photo as a cat a dog or a chicken).</p> <p> Example of binary vs. multi-class classification. Binary deals with two classes (one thing or another), where as multi-class classification can deal with any number of classes over two, for example, the popular ImageNet-1k dataset is used as a computer vision benchmark and has 1000 classes.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#81-creating-mutli-class-classification-data","title":"8.1 Creating mutli-class classification data\u00b6","text":"<p>To begin a multi-class classification problem, let's create some multi-class data.</p> <p>To do so, we can leverage Scikit-Learn's <code>make_blobs()</code> method.</p> <p>This method will create however many classes (using the <code>centers</code> parameter) we want.</p> <p>Specifically, let's do the following:</p> <ol> <li>Create some multi-class data with <code>make_blobs()</code>.</li> <li>Turn the data into tensors (the default of <code>make_blobs()</code> is to use NumPy arrays).</li> <li>Split the data into training and test sets using <code>train_test_split()</code>.</li> <li>Visualize the data.</li> </ol>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#82-building-a-multi-class-classification-model-in-pytorch","title":"8.2 Building a multi-class classification model in PyTorch\u00b6","text":"<p>We've created a few models in PyTorch so far.</p> <p>You might also be starting to get an idea of how flexible neural networks are.</p> <p>How about we build one similar to <code>model_3</code> but this still capable of handling multi-class data?</p> <p>To do so, let's create a subclass of <code>nn.Module</code> that takes in three hyperparameters:</p> <ul> <li><code>input_features</code> - the number of <code>X</code> features coming into the model.</li> <li><code>output_features</code> - the ideal numbers of output features we'd like (this will be equivalent to <code>NUM_CLASSES</code> or the number of classes in your multi-class classification problem).</li> <li><code>hidden_units</code> - the number of hidden neurons we'd like each hidden layer to use.</li> </ul> <p>Since we're putting things together, let's setup some device agnostic code (we don't have to do this again in the same notebook, it's only a reminder).</p> <p>Then we'll create the model class using the hyperparameters above.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#83-creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model","title":"8.3 Creating a loss function and optimizer for a multi-class PyTorch model\u00b6","text":"<p>Since we're working on a multi-class classification problem, we'll use the <code>nn.CrossEntropyLoss()</code> method as our loss function.</p> <p>And we'll stick with using SGD with a learning rate of 0.1 for optimizing our <code>model_4</code> parameters.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#84-getting-prediction-probabilities-for-a-multi-class-pytorch-model","title":"8.4 Getting prediction probabilities for a multi-class PyTorch model\u00b6","text":"<p>Alright, we've got a loss function and optimizer ready, and we're ready to train our model but before we do let's do a single forward pass with our model to see if it works.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#85-creating-a-training-and-testing-loop-for-a-multi-class-pytorch-model","title":"8.5 Creating a training and testing loop for a multi-class PyTorch model\u00b6","text":"<p>Alright, now we've got all of the preparation steps out of the way, let's write a training and testing loop to improve and evaluation our model.</p> <p>We've done many of these steps before so much of this will be practice.</p> <p>The only difference is that we'll be adjusting the steps to turn the model outputs (logits) to prediction probabilities (using the softmax activation function) and then to prediction labels (by taking the argmax of the output of the softmax activation function).</p> <p>Let's train the model for <code>epochs=100</code> and evaluate it every 10 epochs.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#86-making-and-evaluating-predictions-with-a-pytorch-multi-class-model","title":"8.6 Making and evaluating predictions with a PyTorch multi-class model\u00b6","text":"<p>It looks like our trained model is performaning pretty well.</p> <p>But to make sure of this, let's make some predictions and visualize them.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#9-more-classification-evaluation-metrics","title":"9. More classification evaluation metrics\u00b6","text":"<p>So far we've only covered a couple of ways of evaluating a classification model (accuracy, loss and visualizing predictions).</p> <p>These are some of the most common methods you'll come across and are a good starting point.</p> <p>However, you may want to evaluate you classification model using more metrics such as the following:</p> Metric name/Evaluation method Defintion Code Accuracy Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct. <code>torchmetrics.Accuracy()</code> or <code>sklearn.metrics.accuracy_score()</code> Precision Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0). <code>torchmetrics.Precision()</code> or <code>sklearn.metrics.precision_score()</code> Recall Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives. <code>torchmetrics.Recall()</code> or <code>sklearn.metrics.recall_score()</code> F1-score Combines precision and recall into one metric. 1 is best, 0 is worst. <code>torchmetrics.F1Score()</code> or <code>sklearn.metrics.f1_score()</code> Confusion matrix Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line). <code>torchmetrics.ConfusionMatrix</code> or <code>sklearn.metrics.plot_confusion_matrix()</code> Classification report Collection of some of the main classification metrics such as precision, recall and f1-score. <code>sklearn.metrics.classification_report()</code> <p>Scikit-Learn (a popular and world-class machine learning library) has many implementations of the above metrics and you're looking for a PyTorch-like version, check out TorchMetrics, especially the TorchMetrics classification section.</p> <p>Let's try the <code>torchmetrics.Accuracy</code> metric out.</p>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code in the sections above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agonistic code.</p> <p>Resources: 2. Build a model by subclassing <code>nn.Module</code> that incorporates non-linear activation functions and is capable of fitting the data you created in 1.</p> <ul> <li>Exercise template notebook for 02</li> <li>Example solutions notebook for 02 (try the exercises before looking at this)</li> </ul> <ol> <li>Make a binary classification dataset with Scikit-Learn's <code>make_moons()</code> function.</li> </ol> <ul> <li>For consistency, the dataset should have 1000 samples and a <code>random_state=42</code>.</li> <li>Turn the data into PyTorch tensors. Split the data into training and test sets using <code>train_test_split</code> with 80% training and 20% testing.</li> </ul> <ul> <li>Feel free to use any combination of PyTorch layers (linear and non-linear) you want.</li> </ul> <ol> <li>Setup a binary classification compatible loss function and optimizer to use when training the model.</li> <li>Create a training and testing loop to fit the model you created in 2 to the data you created in 1.</li> </ol> <ul> <li>To measure model accuray, you can create your own accuracy function or use the accuracy function in TorchMetrics.</li> <li>Train the model for long enough for it to reach over 96% accuracy.</li> <li>The training loop should output progress every 10 epochs of the model's training and test set loss and accuracy.</li> </ul> <ol> <li>Make predictions with your trained model and plot them using the <code>plot_decision_boundary()</code> function created in this notebook.</li> <li>Replicate the Tanh (hyperbolic tangent) activation function in pure PyTorch.</li> </ol> <ul> <li>Feel free to reference the ML cheatsheet website for the formula.</li> </ul> <ol> <li>Create a multi-class dataset using the spirals data creation function from CS231n (see below for the code).</li> </ol> <ul> <li>Construct a model capable of fitting the data (you may need a combination of linear and non-linear layers).</li> <li>Build a loss function and optimizer capable of handling multi-class data (optional extension: use the Adam optimizer instead of SGD, you may have to experiment with different values of the learning rate to get it working).</li> <li>Make a training and testing loop for the multi-class data and train a model on it to reach over 95% testing accuracy (you can use any accuracy measuring function here that you like).</li> <li>Plot the decision boundaries on the spirals dataset from your model predictions, the <code>plot_decision_boundary()</code> function should work for this dataset too.</li> </ul> <pre># Code for creating a spiral dataset from CS231n\nimport numpy as np\nN = 100 # number of points per class\nD = 2 # dimensionality\nK = 3 # number of classes\nX = np.zeros((N*K,D)) # data matrix (each row = single example)\ny = np.zeros(N*K, dtype='uint8') # class labels\nfor j in range(K):\n  ix = range(N*j,N*(j+1))\n  r = np.linspace(0.0,1,N) # radius\n  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n  y[ix] = j\n# lets visualize the data\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.show()\n</pre>"},{"location":"Learn/z2m-pytorch/02_pytorch_classification/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Write down 3 problems where you think machine classification could be useful (these can be anything, get creative as you like, for example, classifying credit card transactions as fraud or not fraud based on the purchase amount and purchase location features).</li> <li>Research the concept of \"momentum\" in gradient-based optimizers (like SGD or Adam), what does it mean?</li> <li>Spend 10-minutes reading the Wikipedia page for different activation functions, how many of these can you line up with PyTorch's activation functions?</li> <li>Research when accuracy might be a poor metric to use (hint: read \"Beyond Accuracy\" by by Will Koehrsen for ideas).</li> <li>Watch: For an idea of what's happening within our neural networks and what they're doing to learn, watch MIT's Introduction to Deep Learning video.</li> </ul>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/","title":"03. PyTorch Computer Vision","text":"<p>Computer vision is the art of teaching a computer to see.</p> <p>\u8ba1\u7b97\u673a\u89c6\u89c9\u662f\u95e8\u827a\u672f\uff0c\u6839\u636e\u5b83\u6240\u5904\u7406\u7684\u5927\u591a\u6570\u662f\u5355\u5206\u7c7b\uff0c\u591a\u5206\u7c7b\uff0c\u7269\u4f53\u68c0\u6d4b\u548c\u56fe\u50cf\u5206\u5272\u7b49\u65b9\u9762\u7684\u5185\u5bb9\u3002</p> <p>For example, it could involve building a model to classify whether a photo is of a cat or a dog (binary classification).</p> <p>Or whether a photo is of a cat, dog or chicken (multi-class classification).</p> <p>Or identifying where a car appears in a video frame (object detection).</p> <p>Or figuring out where different objects in an image can be separated (panoptic segmentation).</p> <p> Example computer vision problems for binary classification, multiclass classification, object detection and segmentation.</p> In\u00a0[1]: Copied! <pre>import torch\nimport torchvision\nimport matplotlib.pyplot as plt\n\nprint(\"PyToch version:{}\\ntorchvision version:{}\".format(torch.__version__,torchvision.__version__))\n</pre> import torch import torchvision import matplotlib.pyplot as plt  print(\"PyToch version:{}\\ntorchvision version:{}\".format(torch.__version__,torchvision.__version__)) <pre>PyToch version:2.0.0\ntorchvision version:0.15.0\n</pre> In\u00a0[3]: Copied! <pre># Setup training data\ntrain_data = torchvision.datasets.FashionMNIST(\n    root=\"data/03\", # where to download data to?\n    train=True, # get training data\n    download=True, # download data if it doesn't exist on disk\n    transform=torchvision.transforms.ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n    target_transform=None # you can transform labels as well\n)\n\n# Setup testing data\ntest_data = torchvision.datasets.FashionMNIST(\n    root=\"data/03\",\n    train=False, # get test data\n    download=True,\n    transform=torchvision.transforms.ToTensor()\n)\n</pre> # Setup training data train_data = torchvision.datasets.FashionMNIST(     root=\"data/03\", # where to download data to?     train=True, # get training data     download=True, # download data if it doesn't exist on disk     transform=torchvision.transforms.ToTensor(), # images come as PIL format, we want to turn into Torch tensors     target_transform=None # you can transform labels as well )  # Setup testing data test_data = torchvision.datasets.FashionMNIST(     root=\"data/03\",     train=False, # get test data     download=True,     transform=torchvision.transforms.ToTensor() ) <pre>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/03/FashionMNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26421880/26421880 [00:19&lt;00:00, 1323620.61it/s]\n</pre> <pre>Extracting data/03/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/03/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/03/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29515/29515 [00:00&lt;00:00, 123917.68it/s]\n</pre> <pre>Extracting data/03/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/03/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/03/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4422102/4422102 [00:09&lt;00:00, 481413.96it/s] \n</pre> <pre>Extracting data/03/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/03/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/03/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5148/5148 [00:00&lt;00:00, 5429287.65it/s]</pre> <pre>Extracting data/03/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/03/FashionMNIST/raw\n\n</pre> <pre>\n</pre> <p>Let's check out the first sample of the training data.</p> In\u00a0[4]: Copied! <pre># See first training sample\nimage, label = train_data[0]\nimage, label\n</pre> # See first training sample image, label = train_data[0] image, label Out[4]: <pre>(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0039, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n           0.0157, 0.0000, 0.0000, 0.0118],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0471, 0.0392, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n           0.3020, 0.5098, 0.2824, 0.0588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n           0.5529, 0.3451, 0.6745, 0.2588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n           0.4824, 0.7686, 0.8980, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n           0.8745, 0.9608, 0.6784, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n           0.8627, 0.9529, 0.7922, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n           0.8863, 0.7725, 0.8196, 0.2039],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n           0.9608, 0.4667, 0.6549, 0.2196],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n           0.8510, 0.8196, 0.3608, 0.0000],\n          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n           0.8549, 1.0000, 0.3020, 0.0000],\n          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n           0.8784, 0.9569, 0.6235, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n           0.9137, 0.9333, 0.8431, 0.0000],\n          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n           0.8627, 0.9098, 0.9647, 0.0000],\n          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n           0.8706, 0.8941, 0.8824, 0.0000],\n          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n           0.8745, 0.8784, 0.8980, 0.1137],\n          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n           0.8627, 0.8667, 0.9020, 0.2627],\n          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n           0.7098, 0.8039, 0.8078, 0.4510],\n          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n           0.6549, 0.6941, 0.8235, 0.3608],\n          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n           0.7529, 0.8471, 0.6667, 0.0000],\n          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n           0.3882, 0.2275, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000]]]),\n 9)</pre> In\u00a0[5]: Copied! <pre># What's the shape of the image?\nimage.shape\n</pre> # What's the shape of the image? image.shape Out[5]: <pre>torch.Size([1, 28, 28])</pre> <p>The shape of the image tensor is <code>[1, 28, 28]</code> or more specifically:</p> <pre><code>[color_channels=1, height=28, width=28]\n</code></pre> <p>Having <code>color_channels=1</code> means the image is grayscale.</p> <p> Various problems will have various input and output shapes. But the premise reamins: encode data into numbers, build a model to find patterns in those numbers, convert those patterns into something meaningful.</p> <p>If <code>color_channels=3</code>, the image comes in pixel values for red, green and blue (this is also known a the RGB color model).</p> <p>The order of our current tensor is often referred to as <code>CHW</code> (Color Channels, Height, Width).</p> <p>There's debate on whether images should be represented as <code>CHW</code> (color channels first) or <code>HWC</code> (color channels last). \u8fd9\u4e2a\u4e89\u8bae\u6211\u4e4b\u524d\u662f\u672a\u4e86\u89e3\u8fc7\u7684\uff0c\u4e00\u822c\u90fd\u662fCHW\u3002</p> <p>Note: You'll also see <code>NCHW</code> and <code>NHWC</code> formats where <code>N</code> stands for number of images. For example if you have a <code>batch_size=32</code>, your tensor shape may be <code>[32, 1, 28, 28]</code>. We'll cover batch sizes later.</p> <p>PyTorch generally accepts <code>NCHW</code> (channels first) as the default for many operators.</p> <p>However, PyTorch also explains that <code>NHWC</code> (channels last) performs better and is considered best practice. \uff08\u5728\u4e00\u4e9b\u5927\u578b\u7684\u9879\u76ee\u4e2d\uff0cchannel\u4f4d\u4e8e\u4f55\u5904\u53ef\u80fd\u4e5f\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8d85\u53c2\u6570\uff09</p> <p>For now, since our dataset and models are relatively small, this won't make too much of a difference.</p> <p>But keep it in mind for when you're working on larger image datasets and using convolutional neural networks (we'll see these later).</p> <p>Let's check out more shapes of our data.</p> In\u00a0[6]: Copied! <pre># How many samples are there? \nlen(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)\n</pre> # How many samples are there?  len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets) Out[6]: <pre>(60000, 60000, 10000, 10000)</pre> <p>So we've got 60,000 training samples and 10,000 testing samples.</p> <p>What classes are there?</p> <p>We can find these via the <code>.classes</code> attribute. \u8fd9\u4e2a.classes\u53c2\u6570\u662ftorhcvison\u6570\u636e\u96c6\u4e2d\u7279\u6709\u7684\u3002</p> In\u00a0[20]: Copied! <pre># See classes\nclass_names = train_data.classes\nclass_names\n</pre> # See classes class_names = train_data.classes class_names Out[20]: <pre>['T-shirt/top',\n 'Trouser',\n 'Pullover',\n 'Dress',\n 'Coat',\n 'Sandal',\n 'Shirt',\n 'Sneaker',\n 'Bag',\n 'Ankle boot']</pre> <p>Sweet! It looks like we're dealing with 10 different kinds of clothes.</p> <p>Because we're working with 10 different classes, it means our problem is multi-class classification.</p> <p>Let's get visual.</p> In\u00a0[15]: Copied! <pre>image.squeeze().shape #\u901a\u8fc7squeeze\u6765\u8fdb\u884c\u964d\u4f4e\u7eac\u5ea6\uff0csqueeze\uff1a\u538b\u7f29\n</pre> image.squeeze().shape #\u901a\u8fc7squeeze\u6765\u8fdb\u884c\u964d\u4f4e\u7eac\u5ea6\uff0csqueeze\uff1a\u538b\u7f29 Out[15]: <pre>torch.Size([28, 28])</pre> In\u00a0[13]: Copied! <pre>plt.imshow(image.squeeze())\n#imshow\u662f\u7528\u6765\u663e\u793a\u56fe\u7247\u7684\uff0c\u5373\u8f93\u5165\u7684\u77e9\u9635\uff0cshow\u662f\u5c55\u793a\u6240\u6709\u56fe\u50cf\n</pre> plt.imshow(image.squeeze()) #imshow\u662f\u7528\u6765\u663e\u793a\u56fe\u7247\u7684\uff0c\u5373\u8f93\u5165\u7684\u77e9\u9635\uff0cshow\u662f\u5c55\u793a\u6240\u6709\u56fe\u50cf Out[13]: <pre>&lt;matplotlib.image.AxesImage at 0x7f1951662400&gt;</pre> <p>We can turn the image into grayscale using the <code>cmap</code> parameter of <code>plt.imshow()</code>.</p> In\u00a0[16]: Copied! <pre>plt.imshow(image.squeeze(), cmap=\"gray\") #cmap:\u8c03\u8272\u7248\uff0c\u6709\u5f88\u591a\u5df2\u7ecf\u5b9a\u4e49\u597d\u7684\u989c\u8272\u677f\u3002\u600e\u4e48\u81ea\u5b9a\u4e49\u81ea\u5df1\u7684\u8c03\u8272\u677f\u5462\uff1f\nplt.title(class_names[label]);\n</pre> plt.imshow(image.squeeze(), cmap=\"gray\") #cmap:\u8c03\u8272\u7248\uff0c\u6709\u5f88\u591a\u5df2\u7ecf\u5b9a\u4e49\u597d\u7684\u989c\u8272\u677f\u3002\u600e\u4e48\u81ea\u5b9a\u4e49\u81ea\u5df1\u7684\u8c03\u8272\u677f\u5462\uff1f plt.title(class_names[label]); <p>Beautiful, well as beautiful as a pixelated grayscale ankle boot can get.</p> <p>Let's view a few more.</p> In\u00a0[21]: Copied! <pre>torch.manual_seed(3407)\nfig,axs = plt.subplots(4,4,figsize=(9,9))\naxs = axs.flatten() #\u62c9\u5e73\u5b50\u56fe\nfor i in axs:\n    random_idx = torch.randint(0,len(train_data),size=[1]).item()\n    img,label = train_data[random_idx]\n    i.imshow(img.squeeze(),cmap=\"gray\")\n    i.set_title(class_names[label])\n    i.axis(False)\n</pre> torch.manual_seed(3407) fig,axs = plt.subplots(4,4,figsize=(9,9)) axs = axs.flatten() #\u62c9\u5e73\u5b50\u56fe for i in axs:     random_idx = torch.randint(0,len(train_data),size=[1]).item()     img,label = train_data[random_idx]     i.imshow(img.squeeze(),cmap=\"gray\")     i.set_title(class_names[label])     i.axis(False) <p>Hmmm, this dataset doesn't look too aesthetic.</p> <p>But the principles we're going to learn on how to build a model for it will be similar across a wide range of computer vision problems.</p> <p>In essence, taking pixel values and building a model to find patterns in them to use on future pixel values.</p> <p>Plus, even for this small dataset (yes, even 60,000 images in deep learning is considered quite small), could you write a program to classify each one of them?</p> <p>You probably could.</p> <p>But I think coding a model in PyTorch would be faster.</p> <p>Question: Do you think the above data can be model with only straight (linear) lines? Or do you think you'd also need non-straight (non-linear) lines?</p> <p>Answer: \u7ebf\u6027\u6a21\u578b\u80af\u5b9a\u662f\u53ef\u4ee5\u53bb\u505a\u7684\uff0c\u4e0d\u8fc7\u76f8\u5bf9\u7684\u6548\u679c\u4e5f\u4f1a\u5f88\u5dee\uff0c\u8fd8\u662f\u9700\u8981\u975e\u7ebf\u6027\u624d\u80fd\u5b8c\u6210\u3002</p> In\u00a0[25]: Copied! <pre>#\u8fd9\u91cc\u5176\u5b9e\u4e0d\u662f\u4e00\u4e2a\u6807\u51c6\u7684\u6d41\u7a0b\uff0cDataLoader\u662f\u5efa\u7acb\u5728Dataset\u7684\u57fa\u7840\u4e0a\u7684\uff0c\u4ecetorchvision\u4e2d\u83b7\u5f97\u7684\u6570\u636e\u96c6\u4e3aDataset\u7c7b\uff0c\u5982\u679c\u662f\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u8bdd\uff0c\u9700\u8981\u9996\u5148\u6784\u5efaDataset\u7c7b\nBATCH_SIZE = 32 #hyperparameter\u5168\u4e3a\u5927\u5199\u6bd4\u8f83\u597d\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=BATCH_SIZE,\n                                               shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_data,\n                                              batch_size = BATCH_SIZE,\n                                              shuffle = True)\nprint(f\"Dataloaders: {train_dataloader, test_dataloader}\") \nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n</pre> #\u8fd9\u91cc\u5176\u5b9e\u4e0d\u662f\u4e00\u4e2a\u6807\u51c6\u7684\u6d41\u7a0b\uff0cDataLoader\u662f\u5efa\u7acb\u5728Dataset\u7684\u57fa\u7840\u4e0a\u7684\uff0c\u4ecetorchvision\u4e2d\u83b7\u5f97\u7684\u6570\u636e\u96c6\u4e3aDataset\u7c7b\uff0c\u5982\u679c\u662f\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u8bdd\uff0c\u9700\u8981\u9996\u5148\u6784\u5efaDataset\u7c7b BATCH_SIZE = 32 #hyperparameter\u5168\u4e3a\u5927\u5199\u6bd4\u8f83\u597d  train_dataloader = torch.utils.data.DataLoader(train_data,                                                batch_size=BATCH_SIZE,                                                shuffle=True) test_dataloader = torch.utils.data.DataLoader(test_data,                                               batch_size = BATCH_SIZE,                                               shuffle = True) print(f\"Dataloaders: {train_dataloader, test_dataloader}\")  print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\") print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\") <pre>Dataloaders: (&lt;torch.utils.data.dataloader.DataLoader object at 0x7f1931741970&gt;, &lt;torch.utils.data.dataloader.DataLoader object at 0x7f1931741df0&gt;)\nLength of train dataloader: 1875 batches of 32\nLength of test dataloader: 313 batches of 32\n</pre> <p>\u7531\u6b64\u6211\u4eec\u770b\u5230\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u5bf9\u8c61\u3002</p> In\u00a0[26]: Copied! <pre># Check out what's inside the training dataloader\ntrain_features_batch, train_labels_batch = next(iter(train_dataloader))\ntrain_features_batch.shape, train_labels_batch.shape\n#next\uff1a\u83b7\u5f97\u8fed\u4ee3\u5bf9\u8c61\u7684\u4e0b\u4e00\u4e2a\u51fd\u6570\n#iter\u7528\u4e8e\u53ef\u8fed\u4ee3\u5bf9\u8c61\n</pre> # Check out what's inside the training dataloader train_features_batch, train_labels_batch = next(iter(train_dataloader)) train_features_batch.shape, train_labels_batch.shape #next\uff1a\u83b7\u5f97\u8fed\u4ee3\u5bf9\u8c61\u7684\u4e0b\u4e00\u4e2a\u51fd\u6570 #iter\u7528\u4e8e\u53ef\u8fed\u4ee3\u5bf9\u8c61 Out[26]: <pre>(torch.Size([32, 1, 28, 28]), torch.Size([32]))</pre> <p>And we can see that the data remains unchanged by checking a single sample.</p> In\u00a0[35]: Copied! <pre># Show a sample\ntorch.manual_seed(42)\nrandom_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\nimg, label = train_features_batch[random_idx], train_labels_batch[random_idx]\nplt.imshow(img.squeeze(), cmap=\"gray\")\nplt.title(class_names[label])\nplt.axis(\"Off\");\nprint(f\"Image size: {img.shape}\")\nprint(f\"Label: {label}, label size: {label.shape}\")\n</pre> # Show a sample torch.manual_seed(42) random_idx = torch.randint(0, len(train_features_batch), size=[1]).item() img, label = train_features_batch[random_idx], train_labels_batch[random_idx] plt.imshow(img.squeeze(), cmap=\"gray\") plt.title(class_names[label]) plt.axis(\"Off\"); print(f\"Image size: {img.shape}\") print(f\"Label: {label}, label size: {label.shape}\") <pre>Image size: torch.Size([1, 28, 28])\nLabel: 2, label size: torch.Size([])\n</pre> In\u00a0[37]: Copied! <pre># Create a flatten layer\nflatten_model = torch.nn.Flatten() # all nn modules function as a model (can do a forward pass)\n\n# Get a single sample\nx = train_features_batch[0]\n\n# Flatten the sample\noutput = flatten_model(x) # perform forward pass\n\n# Print out what happened\nprint(f\"Shape before flattening: {x.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Shape after flattening: {output.shape} -&gt; [color_channels, height*width]\")\n\n# Try uncommenting below and see what happens\n#print(x)\n#print(output)\n</pre> # Create a flatten layer flatten_model = torch.nn.Flatten() # all nn modules function as a model (can do a forward pass)  # Get a single sample x = train_features_batch[0]  # Flatten the sample output = flatten_model(x) # perform forward pass  # Print out what happened print(f\"Shape before flattening: {x.shape} -&gt; [color_channels, height, width]\") print(f\"Shape after flattening: {output.shape} -&gt; [color_channels, height*width]\")  # Try uncommenting below and see what happens #print(x) #print(output) <pre>Shape before flattening: torch.Size([1, 28, 28]) -&gt; [color_channels, height, width]\nShape after flattening: torch.Size([1, 784]) -&gt; [color_channels, height*width]\n</pre> <p>The <code>nn.Flatten()</code> layer took our shape from <code>[color_channels, height, width]</code> to <code>[color_channels, height*width]</code>.</p> <p>Why do this?</p> <p>Because we've now turned our pixel data from height and width dimensions into one long feature vector.</p> <p>And <code>nn.Linear()</code> layers like their inputs to be in the form of feature vectors.  \u7ebf\u6027\u5c42\u8f93\u5165\u7684\u662f\u7279\u5f81\u5411\u91cf</p> <p>Let's create our first model using <code>nn.Flatten()</code> as the first layer.</p> In\u00a0[14]: Copied! <pre>from torch import nn\nclass FashionMNISTModelV0(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # neural networks like their inputs in vector form\n            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n            nn.Linear(in_features=hidden_units, out_features=output_shape)\n        )\n    \n    def forward(self, x):\n        return self.layer_stack(x)\n</pre> from torch import nn class FashionMNISTModelV0(nn.Module):     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.layer_stack = nn.Sequential(             nn.Flatten(), # neural networks like their inputs in vector form             nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)             nn.Linear(in_features=hidden_units, out_features=output_shape)         )          def forward(self, x):         return self.layer_stack(x) <p>Wonderful!</p> <p>We've got a baseline model class we can use, now let's instantiate a model.</p> <p>We'll need to set the following parameters:</p> <ul> <li><code>input_shape=784</code> - this is how many features you've got going in the model, in our case, it's one for every pixel in the target image (28 pixels high by 28 pixels wide = 784 features).</li> <li><code>hidden_units=10</code> - number of units/neurons in the hidden layer(s), this number could be whatever you want but to keep the model small we'll start with <code>10</code>.</li> <li><code>output_shape=len(class_names)</code> - since we're working with a multi-class classification problem, we need an output neuron per class in our dataset.</li> </ul> <p>Let's create an instance of our model and send to the CPU for now (we'll run a small test for running <code>model_0</code> on CPU vs. a similar model on GPU soon).</p> In\u00a0[15]: Copied! <pre>torch.manual_seed(42)\n\n# Need to setup model with input parameters\nmodel_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)\n    hidden_units=10, # how many units in the hiden layer\n    output_shape=len(class_names) # one for every class\n)\nmodel_0.to(\"cpu\") # keep model on CPU to begin with\n</pre> torch.manual_seed(42)  # Need to setup model with input parameters model_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)     hidden_units=10, # how many units in the hiden layer     output_shape=len(class_names) # one for every class ) model_0.to(\"cpu\") # keep model on CPU to begin with  Out[15]: <pre>FashionMNISTModelV0(\n  (layer_stack): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=784, out_features=10, bias=True)\n    (2): Linear(in_features=10, out_features=10, bias=True)\n  )\n)</pre> In\u00a0[16]: Copied! <pre>import requests\nfrom pathlib import Path \n\n# Download helper functions from Learn PyTorch repo (if not already downloaded)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  # Note: you need the \"raw\" GitHub URL for this to work\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n</pre> import requests from pathlib import Path   # Download helper functions from Learn PyTorch repo (if not already downloaded) if Path(\"helper_functions.py\").is_file():   print(\"helper_functions.py already exists, skipping download\") else:   print(\"Downloading helper_functions.py\")   # Note: you need the \"raw\" GitHub URL for this to work   request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")   with open(\"helper_functions.py\", \"wb\") as f:     f.write(request.content) <pre>helper_functions.py already exists, skipping download\n</pre> In\u00a0[17]: Copied! <pre># Import accuracy metric\nfrom helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy()\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\noptimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n</pre> # Import accuracy metric from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy()  # Setup loss function and optimizer loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1) In\u00a0[18]: Copied! <pre>from timeit import default_timer as timer \ndef print_train_time(start: float, end: float, device: torch.device = None):\n\"\"\"Prints difference between start and end time.\n\n    Args:\n        start (float): Start time of computation (preferred in timeit format). \n        end (float): End time of computation.\n        device ([type], optional): Device that compute is running on. Defaults to None.\n\n    Returns:\n        float: time between start and end in seconds (higher is longer).\n    \"\"\"\n    total_time = end - start\n    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n    return total_time\n</pre> from timeit import default_timer as timer  def print_train_time(start: float, end: float, device: torch.device = None):     \"\"\"Prints difference between start and end time.      Args:         start (float): Start time of computation (preferred in timeit format).          end (float): End time of computation.         device ([type], optional): Device that compute is running on. Defaults to None.      Returns:         float: time between start and end in seconds (higher is longer).     \"\"\"     total_time = end - start     print(f\"Train time on {device}: {total_time:.3f} seconds\")     return total_time In\u00a0[19]: Copied! <pre># Import tqdm for progress bar\nfrom tqdm.auto import tqdm\n\n# Set the seed and start the timer\ntorch.manual_seed(42)\ntrain_time_start_on_cpu = timer()\n\n# Set the number of epochs (we'll keep this small for faster training times)\nepochs = 3\n\n# Create training and testing loop\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n-------\")\n    ### Training\n    train_loss = 0\n    # Add a loop to loop through training batches\n    for batch, (X, y) in enumerate(train_dataloader):\n        model_0.train() \n        # 1. Forward pass\n        y_pred = model_0(X)\n\n        # 2. Calculate loss (per batch)\n        loss = loss_fn(y_pred, y)\n        train_loss += loss # accumulatively add up the loss per epoch \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Print out how many samples have been seen\n        if batch % 400 == 0:\n            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n\n    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n    train_loss /= len(train_dataloader)\n    \n    ### Testing\n    # Setup variables for accumulatively adding up loss and accuracy \n    test_loss, test_acc = 0, 0 \n    model_0.eval()\n    with torch.inference_mode():\n        for X, y in test_dataloader:\n            # 1. Forward pass\n            test_pred = model_0(X)\n           \n            # 2. Calculate loss (accumatively)\n            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n\n            # 3. Calculate accuracy (preds need to be same as y_true)\n            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n        \n        # Calculations on test metrics need to happen inside torch.inference_mode()\n        # Divide total test loss by length of test dataloader (per batch)\n        test_loss /= len(test_dataloader)\n\n        # Divide total accuracy by length of test dataloader (per batch)\n        test_acc /= len(test_dataloader)\n\n    ## Print out what's happening\n    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n\n# Calculate training time      \ntrain_time_end_on_cpu = timer()\ntotal_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n                                           end=train_time_end_on_cpu,\n                                           device=str(next(model_0.parameters()).device))\n</pre> # Import tqdm for progress bar from tqdm.auto import tqdm  # Set the seed and start the timer torch.manual_seed(42) train_time_start_on_cpu = timer()  # Set the number of epochs (we'll keep this small for faster training times) epochs = 3  # Create training and testing loop for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n-------\")     ### Training     train_loss = 0     # Add a loop to loop through training batches     for batch, (X, y) in enumerate(train_dataloader):         model_0.train()          # 1. Forward pass         y_pred = model_0(X)          # 2. Calculate loss (per batch)         loss = loss_fn(y_pred, y)         train_loss += loss # accumulatively add up the loss per epoch           # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()          # Print out how many samples have been seen         if batch % 400 == 0:             print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")      # Divide total train loss by length of train dataloader (average loss per batch per epoch)     train_loss /= len(train_dataloader)          ### Testing     # Setup variables for accumulatively adding up loss and accuracy      test_loss, test_acc = 0, 0      model_0.eval()     with torch.inference_mode():         for X, y in test_dataloader:             # 1. Forward pass             test_pred = model_0(X)                         # 2. Calculate loss (accumatively)             test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch              # 3. Calculate accuracy (preds need to be same as y_true)             test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))                  # Calculations on test metrics need to happen inside torch.inference_mode()         # Divide total test loss by length of test dataloader (per batch)         test_loss /= len(test_dataloader)          # Divide total accuracy by length of test dataloader (per batch)         test_acc /= len(test_dataloader)      ## Print out what's happening     print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")  # Calculate training time       train_time_end_on_cpu = timer() total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,                                             end=train_time_end_on_cpu,                                            device=str(next(model_0.parameters()).device)) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%\n\nEpoch: 1\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%\n\nEpoch: 2\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%\n\nTrain time on cpu: 14.975 seconds\n</pre> <p>Nice! Looks like our baseline model did fairly well.</p> <p>It didn't take too long to train either, even just on the CPU, I wonder if it'll speed up on the GPU?</p> <p>Let's write some code to evaluate our model.</p> In\u00a0[20]: Copied! <pre>torch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn):\n\"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Make predictions with the model\n            y_pred = model(X)\n            \n            # Accumulate the loss and accuracy values per batch\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, \n                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)\n        \n        # Scale loss and acc to find the average loss/acc per batch\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n        \n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calculate model 0 results on test dataset\nmodel_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n)\nmodel_0_results\n</pre> torch.manual_seed(42) def eval_model(model: torch.nn.Module,                 data_loader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 accuracy_fn):     \"\"\"Returns a dictionary containing the results of model predicting on data_loader.      Args:         model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.         data_loader (torch.utils.data.DataLoader): The target dataset to predict on.         loss_fn (torch.nn.Module): The loss function of model.         accuracy_fn: An accuracy function to compare the models predictions to the truth labels.      Returns:         (dict): Results of model making predictions on data_loader.     \"\"\"     loss, acc = 0, 0     model.eval()     with torch.inference_mode():         for X, y in data_loader:             # Make predictions with the model             y_pred = model(X)                          # Accumulate the loss and accuracy values per batch             loss += loss_fn(y_pred, y)             acc += accuracy_fn(y_true=y,                                  y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)                  # Scale loss and acc to find the average loss/acc per batch         loss /= len(data_loader)         acc /= len(data_loader)              return {\"model_name\": model.__class__.__name__, # only works when model was created with a class             \"model_loss\": loss.item(),             \"model_acc\": acc}  # Calculate model 0 results on test dataset model_0_results = eval_model(model=model_0, data_loader=test_dataloader,     loss_fn=loss_fn, accuracy_fn=accuracy_fn ) model_0_results Out[20]: <pre>{'model_name': 'FashionMNISTModelV0',\n 'model_loss': 0.47663894295692444,\n 'model_acc': 83.42651757188499}</pre> <p>Looking good!</p> <p>We can use this dictionary to compare the baseline model results to other models later on.</p> In\u00a0[21]: Copied! <pre># Setup device agnostic code\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Setup device agnostic code import torch device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[21]: <pre>'cuda'</pre> <p>Beautiful!</p> <p>Let's build another model.</p> In\u00a0[22]: Copied! <pre># Create a model with non-linear and linear layers\nclass FashionMNISTModelV1(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # flatten inputs into single vector\n            nn.Linear(in_features=input_shape, out_features=hidden_units),\n            nn.ReLU(),\n            nn.Linear(in_features=hidden_units, out_features=output_shape),\n            nn.ReLU()\n        )\n    \n    def forward(self, x: torch.Tensor):\n        return self.layer_stack(x)\n</pre> # Create a model with non-linear and linear layers class FashionMNISTModelV1(nn.Module):     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.layer_stack = nn.Sequential(             nn.Flatten(), # flatten inputs into single vector             nn.Linear(in_features=input_shape, out_features=hidden_units),             nn.ReLU(),             nn.Linear(in_features=hidden_units, out_features=output_shape),             nn.ReLU()         )          def forward(self, x: torch.Tensor):         return self.layer_stack(x) <p>That looks good.</p> <p>Now let's instantiate it with the same settings we used before.</p> <p>We'll need <code>input_shape=784</code> (equal to the number of features of our image data), <code>hidden_units=10</code> (starting small and the same as our baseline model) and <code>output_shape=len(class_names)</code> (one output unit per class).</p> <p>Note: Notice how we kept most of the settings of our model the same except for one change: adding non-linear layers. This is a standard practice for running a series of machine learning experiments, change one thing and see what happens, then do it again, again, again.</p> In\u00a0[23]: Copied! <pre>torch.manual_seed(42)\nmodel_1 = FashionMNISTModelV1(input_shape=784, # number of input features\n    hidden_units=10,\n    output_shape=len(class_names) # number of output classes desired\n).to(device) # send model to GPU if it's available\nnext(model_1.parameters()).device # check model device\n</pre> torch.manual_seed(42) model_1 = FashionMNISTModelV1(input_shape=784, # number of input features     hidden_units=10,     output_shape=len(class_names) # number of output classes desired ).to(device) # send model to GPU if it's available next(model_1.parameters()).device # check model device Out[23]: <pre>device(type='cuda', index=0)</pre> In\u00a0[24]: Copied! <pre>from helper_functions import accuracy_fn\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_1.parameters(), \n                            lr=0.1)\n</pre> from helper_functions import accuracy_fn loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model_1.parameters(),                              lr=0.1) In\u00a0[25]: Copied! <pre>def train_step(model: torch.nn.Module,\n               data_loader: torch.utils.data.DataLoader,\n               loss_fn: torch.nn.Module,\n               optimizer: torch.optim.Optimizer,\n               accuracy_fn,\n               device: torch.device = device):\n    train_loss, train_acc = 0, 0\n    for batch, (X, y) in enumerate(data_loader):\n        # Send data to GPU\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss\n        train_acc += accuracy_fn(y_true=y,\n                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -&gt; pred labels\n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n    # Calculate loss and accuracy per epoch and print out what's happening\n    train_loss /= len(data_loader)\n    train_acc /= len(data_loader)\n    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n\ndef test_step(data_loader: torch.utils.data.DataLoader,\n              model: torch.nn.Module,\n              loss_fn: torch.nn.Module,\n              accuracy_fn,\n              device: torch.device = device):\n    test_loss, test_acc = 0, 0\n    model.eval() # put model in eval mode\n    # Turn on inference context manager\n    with torch.inference_mode(): \n        for X, y in data_loader:\n            # Send data to GPU\n            X, y = X.to(device), y.to(device)\n            \n            # 1. Forward pass\n            test_pred = model(X)\n            \n            # 2. Calculate loss and accuracy\n            test_loss += loss_fn(test_pred, y)\n            test_acc += accuracy_fn(y_true=y,\n                y_pred=test_pred.argmax(dim=1) # Go from logits -&gt; pred labels\n            )\n        \n        # Adjust metrics and print out\n        test_loss /= len(data_loader)\n        test_acc /= len(data_loader)\n        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n</pre> def train_step(model: torch.nn.Module,                data_loader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module,                optimizer: torch.optim.Optimizer,                accuracy_fn,                device: torch.device = device):     train_loss, train_acc = 0, 0     for batch, (X, y) in enumerate(data_loader):         # Send data to GPU         X, y = X.to(device), y.to(device)          # 1. Forward pass         y_pred = model(X)          # 2. Calculate loss         loss = loss_fn(y_pred, y)         train_loss += loss         train_acc += accuracy_fn(y_true=y,                                  y_pred=y_pred.argmax(dim=1)) # Go from logits -&gt; pred labels          # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()      # Calculate loss and accuracy per epoch and print out what's happening     train_loss /= len(data_loader)     train_acc /= len(data_loader)     print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")  def test_step(data_loader: torch.utils.data.DataLoader,               model: torch.nn.Module,               loss_fn: torch.nn.Module,               accuracy_fn,               device: torch.device = device):     test_loss, test_acc = 0, 0     model.eval() # put model in eval mode     # Turn on inference context manager     with torch.inference_mode():          for X, y in data_loader:             # Send data to GPU             X, y = X.to(device), y.to(device)                          # 1. Forward pass             test_pred = model(X)                          # 2. Calculate loss and accuracy             test_loss += loss_fn(test_pred, y)             test_acc += accuracy_fn(y_true=y,                 y_pred=test_pred.argmax(dim=1) # Go from logits -&gt; pred labels             )                  # Adjust metrics and print out         test_loss /= len(data_loader)         test_acc /= len(data_loader)         print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\") <p>Woohoo!</p> <p>Now we've got some functions for training and testing our model, let's run them.</p> <p>We'll do so inside another loop for each epoch.</p> <p>That way for each epoch we're going a training and a testing step.</p> <p>Note: You can customize how often you do a testing step. Sometimes people do them every five epochs or 10 epochs or in our case, every epoch.</p> <p>Let's also time things to see how long our code takes to run on the GPU.</p> In\u00a0[26]: Copied! <pre>torch.manual_seed(42)\n\n# Measure time\nfrom timeit import default_timer as timer\ntrain_time_start_on_gpu = timer()\n\nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_1, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_1,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn\n    )\n\ntrain_time_end_on_gpu = timer()\ntotal_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n                                            end=train_time_end_on_gpu,\n                                            device=device)\n</pre> torch.manual_seed(42)  # Measure time from timeit import default_timer as timer train_time_start_on_gpu = timer()  epochs = 3 for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n---------\")     train_step(data_loader=train_dataloader,          model=model_1,          loss_fn=loss_fn,         optimizer=optimizer,         accuracy_fn=accuracy_fn     )     test_step(data_loader=test_dataloader,         model=model_1,         loss_fn=loss_fn,         accuracy_fn=accuracy_fn     )  train_time_end_on_gpu = timer() total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,                                             end=train_time_end_on_gpu,                                             device=device) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0\n---------\nTrain loss: 1.09199 | Train accuracy: 61.34%\nTest loss: 0.95636 | Test accuracy: 65.00%\n\nEpoch: 1\n---------\nTrain loss: 0.78101 | Train accuracy: 71.93%\nTest loss: 0.72227 | Test accuracy: 73.91%\n\nEpoch: 2\n---------\nTrain loss: 0.67027 | Train accuracy: 75.94%\nTest loss: 0.68500 | Test accuracy: 75.02%\n\nTrain time on cuda: 16.943 seconds\n</pre> <p>Excellent!</p> <p>Our model trained but the training time took longer?</p> <p>Note: The training time on CUDA vs CPU will depend largely on the quality of the CPU/GPU you're using. Read on for a more explained answer.</p> <p>Question: \"I used a a GPU but my model didn't train faster, why might that be?\"</p> <p>Answer: Well, one reason could be because your dataset and model are both so small (like the dataset and model we're working with) the benefits of using a GPU are outweighed by the time it actually takes to transfer the data there.</p> <p>There's a small bottleneck between copying data from the CPU memory (default) to the GPU memory.</p> <p>So for smaller models and datasets, the CPU might actually be the optimal place to compute on.</p> <p>But for larger datasets and models, the speed of computing the GPU can offer usually far outweighs the cost of getting the data there.</p> <p>However, this is largely dependant on the hardware you're using. With practice, you will get used to where the best place to train your models is.</p> <p>Let's evaluate our trained <code>model_1</code> using our <code>eval_model()</code> function and see how it went.</p> In\u00a0[27]: Copied! <pre>torch.manual_seed(42)\n\n# Note: This will error due to `eval_model()` not using device agnostic code \nmodel_1_results = eval_model(model=model_1, \n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn) \nmodel_1_results\n</pre> torch.manual_seed(42)  # Note: This will error due to `eval_model()` not using device agnostic code  model_1_results = eval_model(model=model_1,      data_loader=test_dataloader,     loss_fn=loss_fn,      accuracy_fn=accuracy_fn)  model_1_results  <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/tmp/ipykernel_1084458/2906876561.py in &lt;module&gt;\n      2 \n      3 # Note: This will error due to `eval_model()` not using device agnostic code\n----&gt; 4 model_1_results = eval_model(model=model_1, \n      5     data_loader=test_dataloader,\n      6     loss_fn=loss_fn,\n\n/tmp/ipykernel_1084458/2300884397.py in eval_model(model, data_loader, loss_fn, accuracy_fn)\n     20         for X, y in data_loader:\n     21             # Make predictions with the model\n---&gt; 22             y_pred = model(X)\n     23 \n     24             # Accumulate the loss and accuracy values per batch\n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n/tmp/ipykernel_1084458/3744982926.py in forward(self, x)\n     12 \n     13     def forward(self, x: torch.Tensor):\n---&gt; 14         return self.layer_stack(x)\n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/container.py in forward(self, input)\n    139     def forward(self, input):\n    140         for module in self:\n--&gt; 141             input = module(input)\n    142         return input\n    143 \n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/linear.py in forward(self, input)\n    101 \n    102     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 103         return F.linear(input, self.weight, self.bias)\n    104 \n    105     def extra_repr(self) -&gt; str:\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</pre> <p>Oh no!</p> <p>It looks like our <code>eval_model()</code> function errors out with:</p> <p><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</code></p> <p>It's because we've setup our data and model to use device-agnostic code but not our evaluation function.</p> <p>How about we fix that by passing a target <code>device</code> parameter to our <code>eval_model()</code> function?</p> <p>Then we'll try calculating the results again.</p> In\u00a0[28]: Copied! <pre># Move values to device\ntorch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn, \n               device: torch.device = device):\n\"\"\"Evaluates a given model on a given dataset.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n        device (str, optional): Target device to compute on. Defaults to device.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Send data to the target device\n            X, y = X.to(device), y.to(device)\n            y_pred = model(X)\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n        \n        # Scale loss and acc\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calculate model 1 results with device-agnostic code \nmodel_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n    device=device\n)\nmodel_1_results\n</pre> # Move values to device torch.manual_seed(42) def eval_model(model: torch.nn.Module,                 data_loader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 accuracy_fn,                 device: torch.device = device):     \"\"\"Evaluates a given model on a given dataset.      Args:         model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.         data_loader (torch.utils.data.DataLoader): The target dataset to predict on.         loss_fn (torch.nn.Module): The loss function of model.         accuracy_fn: An accuracy function to compare the models predictions to the truth labels.         device (str, optional): Target device to compute on. Defaults to device.      Returns:         (dict): Results of model making predictions on data_loader.     \"\"\"     loss, acc = 0, 0     model.eval()     with torch.inference_mode():         for X, y in data_loader:             # Send data to the target device             X, y = X.to(device), y.to(device)             y_pred = model(X)             loss += loss_fn(y_pred, y)             acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))                  # Scale loss and acc         loss /= len(data_loader)         acc /= len(data_loader)     return {\"model_name\": model.__class__.__name__, # only works when model was created with a class             \"model_loss\": loss.item(),             \"model_acc\": acc}  # Calculate model 1 results with device-agnostic code  model_1_results = eval_model(model=model_1, data_loader=test_dataloader,     loss_fn=loss_fn, accuracy_fn=accuracy_fn,     device=device ) model_1_results Out[28]: <pre>{'model_name': 'FashionMNISTModelV1',\n 'model_loss': 0.6850008964538574,\n 'model_acc': 75.01996805111821}</pre> In\u00a0[29]: Copied! <pre># Check baseline results\nmodel_0_results\n</pre> # Check baseline results model_0_results Out[29]: <pre>{'model_name': 'FashionMNISTModelV0',\n 'model_loss': 0.47663894295692444,\n 'model_acc': 83.42651757188499}</pre> <p>Woah, in this case, it looks like adding non-linearities to our model made it perform worse than the baseline.</p> <p>That's a thing to note in machine learning, sometimes the thing you thought should work doesn't.</p> <p>And then the thing you thought might not work does.</p> <p>It's part science, part art.</p> <p>From the looks of things, it seems like our model is overfitting on the training data.</p> <p>Overfitting means our model is learning the training data well but those patterns aren't generalizing to the testing data.</p> <p>Two of the main to fix overfitting include:</p> <ol> <li>Using a smaller or different model (some models fit certain kinds of data better than others).</li> <li>Using a larger dataset (the more data, the more chance a model has to learn generalizable patterns).</li> </ol> <p>There are more, but I'm going to leave that as a challenge for you to explore.</p> <p>Try searching online, \"ways to prevent overfitting in machine learning\" and see what comes up.</p> <p>In the meantime, let's take a look at number 1: using a different model.</p> In\u00a0[34]: Copied! <pre># Create a convolutional neural network \nclass FashionMNISTModelV2(nn.Module):\n\"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*7*7, \n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.block_1(x)\n        # print(x.shape)\n        x = self.block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n\ntorch.manual_seed(42)\nmodel_2 = FashionMNISTModelV2(input_shape=1, \n    hidden_units=10, \n    output_shape=len(class_names)).to(device)\nmodel_2\n</pre> # Create a convolutional neural network  class FashionMNISTModelV2(nn.Module):     \"\"\"     Model architecture copying TinyVGG from:      https://poloclub.github.io/cnn-explainer/     \"\"\"     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.block_1 = nn.Sequential(             nn.Conv2d(in_channels=input_shape,                        out_channels=hidden_units,                        kernel_size=3, # how big is the square that's going over the image?                       stride=1, # default                       padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number              nn.ReLU(),             nn.Conv2d(in_channels=hidden_units,                        out_channels=hidden_units,                       kernel_size=3,                       stride=1,                       padding=1),             nn.ReLU(),             nn.MaxPool2d(kernel_size=2,                          stride=2) # default stride value is same as kernel_size         )         self.block_2 = nn.Sequential(             nn.Conv2d(hidden_units, hidden_units, 3, padding=1),             nn.ReLU(),             nn.Conv2d(hidden_units, hidden_units, 3, padding=1),             nn.ReLU(),             nn.MaxPool2d(2)         )         self.classifier = nn.Sequential(             nn.Flatten(),             # Where did this in_features shape come from?              # It's because each layer of our network compresses and changes the shape of our inputs data.             nn.Linear(in_features=hidden_units*7*7,                        out_features=output_shape)         )          def forward(self, x: torch.Tensor):         x = self.block_1(x)         # print(x.shape)         x = self.block_2(x)         # print(x.shape)         x = self.classifier(x)         # print(x.shape)         return x  torch.manual_seed(42) model_2 = FashionMNISTModelV2(input_shape=1,      hidden_units=10,      output_shape=len(class_names)).to(device) model_2 Out[34]: <pre>FashionMNISTModelV2(\n  (block_1): Sequential(\n    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=490, out_features=10, bias=True)\n  )\n)</pre> <p>Nice!</p> <p>Our biggest model yet!</p> <p>What we've done is a common practice in machine learning.</p> <p>Find a model architecture somewhere and replicate it with code.</p> In\u00a0[35]: Copied! <pre>torch.manual_seed(42)\n\n# Create sample batch of random numbers with same size as image batch\nimages = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\ntest_image = images[0] # get a single image for testing\nprint(f\"Image batch shape: {images.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Single image shape: {test_image.shape} -&gt; [color_channels, height, width]\") \nprint(f\"Single image pixel values:\\n{test_image}\")\n</pre> torch.manual_seed(42)  # Create sample batch of random numbers with same size as image batch images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width] test_image = images[0] # get a single image for testing print(f\"Image batch shape: {images.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Single image shape: {test_image.shape} -&gt; [color_channels, height, width]\")  print(f\"Single image pixel values:\\n{test_image}\") <pre>Image batch shape: torch.Size([32, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nSingle image shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]\nSingle image pixel values:\ntensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n         ...,\n         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n\n        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n         ...,\n         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n\n        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n         ...,\n         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n</pre> <p>Let's create an example <code>nn.Conv2d()</code> with various parameters:</p> <ul> <li><code>in_channels</code> (int) - Number of channels in the input image.</li> <li><code>out_channels</code> (int) - Number of channels produced by the convolution.</li> <li><code>kernel_size</code> (int or tuple) - Size of the convolving kernel/filter.</li> <li><code>stride</code> (int or tuple, optional) - How big of a step the convolving kernel takes at a time. Default: 1.</li> <li><code>padding</code> (int, tuple, str) - Padding added to all four sides of input. Default: 0.</li> </ul> <p></p> <p>Example of what happens when you change the hyperparameters of a <code>nn.Conv2d()</code> layer.</p> In\u00a0[36]: Copied! <pre>torch.manual_seed(42)\n\n# Create a convolutional layer with same dimensions as TinyVGG \n# (try changing any of the parameters and see what happens)\nconv_layer = nn.Conv2d(in_channels=3,\n                       out_channels=10,\n                       kernel_size=3,\n                       stride=1,\n                       padding=0) # also try using \"valid\" or \"same\" here \n\n# Pass the data through the convolutional layer\nconv_layer(test_image) # Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)\n</pre> torch.manual_seed(42)  # Create a convolutional layer with same dimensions as TinyVGG  # (try changing any of the parameters and see what happens) conv_layer = nn.Conv2d(in_channels=3,                        out_channels=10,                        kernel_size=3,                        stride=1,                        padding=0) # also try using \"valid\" or \"same\" here   # Pass the data through the convolutional layer conv_layer(test_image) # Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)  Out[36]: <pre>tensor([[[ 1.5396,  0.0516,  0.6454,  ..., -0.3673,  0.8711,  0.4256],\n         [ 0.3662,  1.0114, -0.5997,  ...,  0.8983,  0.2809, -0.2741],\n         [ 1.2664, -1.4054,  0.3727,  ..., -0.3409,  1.2191, -0.0463],\n         ...,\n         [-0.1541,  0.5132, -0.3624,  ..., -0.2360, -0.4609, -0.0035],\n         [ 0.2981, -0.2432,  1.5012,  ..., -0.6289, -0.7283, -0.5767],\n         [-0.0386, -0.0781, -0.0388,  ...,  0.2842,  0.4228, -0.1802]],\n\n        [[-0.2840, -0.0319, -0.4455,  ..., -0.7956,  1.5599, -1.2449],\n         [ 0.2753, -0.1262, -0.6541,  ..., -0.2211,  0.1999, -0.8856],\n         [-0.5404, -1.5489,  0.0249,  ..., -0.5932, -1.0913, -0.3849],\n         ...,\n         [ 0.3870, -0.4064, -0.8236,  ...,  0.1734, -0.4330, -0.4951],\n         [-0.1984, -0.6386,  1.0263,  ..., -0.9401, -0.0585, -0.7833],\n         [-0.6306, -0.2052, -0.3694,  ..., -1.3248,  0.2456, -0.7134]],\n\n        [[ 0.4414,  0.5100,  0.4846,  ..., -0.8484,  0.2638,  1.1258],\n         [ 0.8117,  0.3191, -0.0157,  ...,  1.2686,  0.2319,  0.5003],\n         [ 0.3212,  0.0485, -0.2581,  ...,  0.2258,  0.2587, -0.8804],\n         ...,\n         [-0.1144, -0.1869,  0.0160,  ..., -0.8346,  0.0974,  0.8421],\n         [ 0.2941,  0.4417,  0.5866,  ..., -0.1224,  0.4814, -0.4799],\n         [ 0.6059, -0.0415, -0.2028,  ...,  0.1170,  0.2521, -0.4372]],\n\n        ...,\n\n        [[-0.2560, -0.0477,  0.6380,  ...,  0.6436,  0.7553, -0.7055],\n         [ 1.5595, -0.2209, -0.9486,  ..., -0.4876,  0.7754,  0.0750],\n         [-0.0797,  0.2471,  1.1300,  ...,  0.1505,  0.2354,  0.9576],\n         ...,\n         [ 1.1065,  0.6839,  1.2183,  ...,  0.3015, -0.1910, -0.1902],\n         [-0.3486, -0.7173, -0.3582,  ...,  0.4917,  0.7219,  0.1513],\n         [ 0.0119,  0.1017,  0.7839,  ..., -0.3752, -0.8127, -0.1257]],\n\n        [[ 0.3841,  1.1322,  0.1620,  ...,  0.7010,  0.0109,  0.6058],\n         [ 0.1664,  0.1873,  1.5924,  ...,  0.3733,  0.9096, -0.5399],\n         [ 0.4094, -0.0861, -0.7935,  ..., -0.1285, -0.9932, -0.3013],\n         ...,\n         [ 0.2688, -0.5630, -1.1902,  ...,  0.4493,  0.5404, -0.0103],\n         [ 0.0535,  0.4411,  0.5313,  ...,  0.0148, -1.0056,  0.3759],\n         [ 0.3031, -0.1590, -0.1316,  ..., -0.5384, -0.4271, -0.4876]],\n\n        [[-1.1865, -0.7280, -1.2331,  ..., -0.9013, -0.0542, -1.5949],\n         [-0.6345, -0.5920,  0.5326,  ..., -1.0395, -0.7963, -0.0647],\n         [-0.1132,  0.5166,  0.2569,  ...,  0.5595, -1.6881,  0.9485],\n         ...,\n         [-0.0254, -0.2669,  0.1927,  ..., -0.2917,  0.1088, -0.4807],\n         [-0.2609, -0.2328,  0.1404,  ..., -0.1325, -0.8436, -0.7524],\n         [-1.1399, -0.1751, -0.8705,  ...,  0.1589,  0.3377,  0.3493]]],\n       grad_fn=&lt;SqueezeBackward1&gt;)</pre> <p>If we try to pass a single image in, we get a shape mismatch error:</p> <p><code>RuntimeError: Expected 4-dimensional input for 4-dimensional weight [10, 3, 3, 3], but got 3-dimensional input of size [3, 64, 64] instead</code></p> <p>Note: If you're running PyTorch 1.11.0+, this error won't occur.</p> <p>This is because our <code>nn.Conv2d()</code> layer expects a 4-dimensional tensor as input with size <code>(N, C, H, W)</code> or <code>[batch_size, color_channels, height, width]</code>.</p> <p>Right now our single image <code>test_image</code> only has a shape of <code>[color_channels, height, width]</code> or <code>[3, 64, 64]</code>.</p> <p>We can fix this for a single image using <code>test_image.unsqueeze(dim=0)</code> to add an extra dimension for <code>N</code>.</p> In\u00a0[37]: Copied! <pre># Add extra dimension to test image\ntest_image.unsqueeze(dim=0).shape\n</pre> # Add extra dimension to test image test_image.unsqueeze(dim=0).shape Out[37]: <pre>torch.Size([1, 3, 64, 64])</pre> In\u00a0[38]: Copied! <pre># Pass test image with extra dimension through conv_layer\nconv_layer(test_image.unsqueeze(dim=0)).shape\n</pre> # Pass test image with extra dimension through conv_layer conv_layer(test_image.unsqueeze(dim=0)).shape Out[38]: <pre>torch.Size([1, 10, 62, 62])</pre> <p>Hmm, notice what happens to our shape (the same shape as the first layer of TinyVGG on CNN Explainer), we get different channel sizes as well as different pixel sizes.</p> <p>What if we changed the values of <code>conv_layer</code>?</p> In\u00a0[39]: Copied! <pre>torch.manual_seed(42)\n# Create a new conv_layer with different values (try setting these to whatever you like)\nconv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n                         out_channels=10,\n                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n                         stride=2,\n                         padding=0)\n\n# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\nconv_layer_2(test_image.unsqueeze(dim=0)).shape\n</pre> torch.manual_seed(42) # Create a new conv_layer with different values (try setting these to whatever you like) conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image                          out_channels=10,                          kernel_size=(5, 5), # kernel is usually a square so a tuple also works                          stride=2,                          padding=0)  # Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input) conv_layer_2(test_image.unsqueeze(dim=0)).shape Out[39]: <pre>torch.Size([1, 10, 30, 30])</pre> <p>Woah, we get another shape change.</p> <p>Now our image is of shape <code>[1, 10, 30, 30]</code> (it will be different if you use different values) or <code>[batch_size=1, color_channels=10, height=30, width=30]</code>.</p> <p>What's going on here?</p> <p>Behind the scenes, our <code>nn.Conv2d()</code> is compressing the information stored in the image.</p> <p>It does this by performing operations on the input (our test image) against its internal parameters.</p> <p>The goal of this is similar to all of the other neural networks we've been building.</p> <p>Data goes in and the layers try to update their internal parameters (patterns) to lower the loss function thanks to some help of the optimizer.</p> <p>The only difference is how the different layers calculate their parameter updates or in PyTorch terms, the operation present in the layer <code>forward()</code> method.</p> <p>If we check out our <code>conv_layer_2.state_dict()</code> we'll find a similar weight and bias setup as we've seen before.</p> In\u00a0[40]: Copied! <pre># Check out the conv_layer_2 internal parameters\nprint(conv_layer_2.state_dict())\n</pre> # Check out the conv_layer_2 internal parameters print(conv_layer_2.state_dict()) <pre>OrderedDict([('weight', tensor([[[[ 0.0883,  0.0958, -0.0271,  0.1061, -0.0253],\n          [ 0.0233, -0.0562,  0.0678,  0.1018, -0.0847],\n          [ 0.1004,  0.0216,  0.0853,  0.0156,  0.0557],\n          [-0.0163,  0.0890,  0.0171, -0.0539,  0.0294],\n          [-0.0532, -0.0135, -0.0469,  0.0766, -0.0911]],\n\n         [[-0.0532, -0.0326, -0.0694,  0.0109, -0.1140],\n          [ 0.1043, -0.0981,  0.0891,  0.0192, -0.0375],\n          [ 0.0714,  0.0180,  0.0933,  0.0126, -0.0364],\n          [ 0.0310, -0.0313,  0.0486,  0.1031,  0.0667],\n          [-0.0505,  0.0667,  0.0207,  0.0586, -0.0704]],\n\n         [[-0.1143, -0.0446, -0.0886,  0.0947,  0.0333],\n          [ 0.0478,  0.0365, -0.0020,  0.0904, -0.0820],\n          [ 0.0073, -0.0788,  0.0356, -0.0398,  0.0354],\n          [-0.0241,  0.0958, -0.0684, -0.0689, -0.0689],\n          [ 0.1039,  0.0385,  0.1111, -0.0953, -0.1145]]],\n\n\n        [[[-0.0903, -0.0777,  0.0468,  0.0413,  0.0959],\n          [-0.0596, -0.0787,  0.0613, -0.0467,  0.0701],\n          [-0.0274,  0.0661, -0.0897, -0.0583,  0.0352],\n          [ 0.0244, -0.0294,  0.0688,  0.0785, -0.0837],\n          [-0.0616,  0.1057, -0.0390, -0.0409, -0.1117]],\n\n         [[-0.0661,  0.0288, -0.0152, -0.0838,  0.0027],\n          [-0.0789, -0.0980, -0.0636, -0.1011, -0.0735],\n          [ 0.1154,  0.0218,  0.0356, -0.1077, -0.0758],\n          [-0.0384,  0.0181, -0.1016, -0.0498, -0.0691],\n          [ 0.0003, -0.0430, -0.0080, -0.0782, -0.0793]],\n\n         [[-0.0674, -0.0395, -0.0911,  0.0968, -0.0229],\n          [ 0.0994,  0.0360, -0.0978,  0.0799, -0.0318],\n          [-0.0443, -0.0958, -0.1148,  0.0330, -0.0252],\n          [ 0.0450, -0.0948,  0.0857, -0.0848, -0.0199],\n          [ 0.0241,  0.0596,  0.0932,  0.1052, -0.0916]]],\n\n\n        [[[ 0.0291, -0.0497, -0.0127, -0.0864,  0.1052],\n          [-0.0847,  0.0617,  0.0406,  0.0375, -0.0624],\n          [ 0.1050,  0.0254,  0.0149, -0.1018,  0.0485],\n          [-0.0173, -0.0529,  0.0992,  0.0257, -0.0639],\n          [-0.0584, -0.0055,  0.0645, -0.0295, -0.0659]],\n\n         [[-0.0395, -0.0863,  0.0412,  0.0894, -0.1087],\n          [ 0.0268,  0.0597,  0.0209, -0.0411,  0.0603],\n          [ 0.0607,  0.0432, -0.0203, -0.0306,  0.0124],\n          [-0.0204, -0.0344,  0.0738,  0.0992, -0.0114],\n          [-0.0259,  0.0017, -0.0069,  0.0278,  0.0324]],\n\n         [[-0.1049, -0.0426,  0.0972,  0.0450, -0.0057],\n          [-0.0696, -0.0706, -0.1034, -0.0376,  0.0390],\n          [ 0.0736,  0.0533, -0.1021, -0.0694, -0.0182],\n          [ 0.1117,  0.0167, -0.0299,  0.0478, -0.0440],\n          [-0.0747,  0.0843, -0.0525, -0.0231, -0.1149]]],\n\n\n        [[[ 0.0773,  0.0875,  0.0421, -0.0805, -0.1140],\n          [-0.0938,  0.0861,  0.0554,  0.0972,  0.0605],\n          [ 0.0292, -0.0011, -0.0878, -0.0989, -0.1080],\n          [ 0.0473, -0.0567, -0.0232, -0.0665, -0.0210],\n          [-0.0813, -0.0754,  0.0383, -0.0343,  0.0713]],\n\n         [[-0.0370, -0.0847, -0.0204, -0.0560, -0.0353],\n          [-0.1099,  0.0646, -0.0804,  0.0580,  0.0524],\n          [ 0.0825, -0.0886,  0.0830, -0.0546,  0.0428],\n          [ 0.1084, -0.0163, -0.0009, -0.0266, -0.0964],\n          [ 0.0554, -0.1146,  0.0717,  0.0864,  0.1092]],\n\n         [[-0.0272, -0.0949,  0.0260,  0.0638, -0.1149],\n          [-0.0262, -0.0692, -0.0101, -0.0568, -0.0472],\n          [-0.0367, -0.1097,  0.0947,  0.0968, -0.0181],\n          [-0.0131, -0.0471, -0.1043, -0.1124,  0.0429],\n          [-0.0634, -0.0742, -0.0090, -0.0385, -0.0374]]],\n\n\n        [[[ 0.0037, -0.0245, -0.0398, -0.0553, -0.0940],\n          [ 0.0968, -0.0462,  0.0306, -0.0401,  0.0094],\n          [ 0.1077,  0.0532, -0.1001,  0.0458,  0.1096],\n          [ 0.0304,  0.0774,  0.1138, -0.0177,  0.0240],\n          [-0.0803, -0.0238,  0.0855,  0.0592, -0.0731]],\n\n         [[-0.0926, -0.0789, -0.1140, -0.0891, -0.0286],\n          [ 0.0779,  0.0193, -0.0878, -0.0926,  0.0574],\n          [-0.0859, -0.0142,  0.0554, -0.0534, -0.0126],\n          [-0.0101, -0.0273, -0.0585, -0.1029, -0.0933],\n          [-0.0618,  0.1115, -0.0558, -0.0775,  0.0280]],\n\n         [[ 0.0318,  0.0633,  0.0878,  0.0643, -0.1145],\n          [ 0.0102,  0.0699, -0.0107, -0.0680,  0.1101],\n          [-0.0432, -0.0657, -0.1041,  0.0052,  0.0512],\n          [ 0.0256,  0.0228, -0.0876, -0.1078,  0.0020],\n          [ 0.1053,  0.0666, -0.0672, -0.0150, -0.0851]]],\n\n\n        [[[-0.0557,  0.0209,  0.0629,  0.0957, -0.1060],\n          [ 0.0772, -0.0814,  0.0432,  0.0977,  0.0016],\n          [ 0.1051, -0.0984, -0.0441,  0.0673, -0.0252],\n          [-0.0236, -0.0481,  0.0796,  0.0566,  0.0370],\n          [-0.0649, -0.0937,  0.0125,  0.0342, -0.0533]],\n\n         [[-0.0323,  0.0780,  0.0092,  0.0052, -0.0284],\n          [-0.1046, -0.1086, -0.0552, -0.0587,  0.0360],\n          [-0.0336, -0.0452,  0.1101,  0.0402,  0.0823],\n          [-0.0559, -0.0472,  0.0424, -0.0769, -0.0755],\n          [-0.0056, -0.0422, -0.0866,  0.0685,  0.0929]],\n\n         [[ 0.0187, -0.0201, -0.1070, -0.0421,  0.0294],\n          [ 0.0544, -0.0146, -0.0457,  0.0643, -0.0920],\n          [ 0.0730, -0.0448,  0.0018, -0.0228,  0.0140],\n          [-0.0349,  0.0840, -0.0030,  0.0901,  0.1110],\n          [-0.0563, -0.0842,  0.0926,  0.0905, -0.0882]]],\n\n\n        [[[-0.0089, -0.1139, -0.0945,  0.0223,  0.0307],\n          [ 0.0245, -0.0314,  0.1065,  0.0165, -0.0681],\n          [-0.0065,  0.0277,  0.0404, -0.0816,  0.0433],\n          [-0.0590, -0.0959, -0.0631,  0.1114,  0.0987],\n          [ 0.1034,  0.0678,  0.0872, -0.0155, -0.0635]],\n\n         [[ 0.0577, -0.0598, -0.0779, -0.0369,  0.0242],\n          [ 0.0594, -0.0448, -0.0680,  0.0156, -0.0681],\n          [-0.0752,  0.0602, -0.0194,  0.1055,  0.1123],\n          [ 0.0345,  0.0397,  0.0266,  0.0018, -0.0084],\n          [ 0.0016,  0.0431,  0.1074, -0.0299, -0.0488]],\n\n         [[-0.0280, -0.0558,  0.0196,  0.0862,  0.0903],\n          [ 0.0530, -0.0850, -0.0620, -0.0254, -0.0213],\n          [ 0.0095, -0.1060,  0.0359, -0.0881, -0.0731],\n          [-0.0960,  0.1006, -0.1093,  0.0871, -0.0039],\n          [-0.0134,  0.0722, -0.0107,  0.0724,  0.0835]]],\n\n\n        [[[-0.1003,  0.0444,  0.0218,  0.0248,  0.0169],\n          [ 0.0316, -0.0555, -0.0148,  0.1097,  0.0776],\n          [-0.0043, -0.1086,  0.0051, -0.0786,  0.0939],\n          [-0.0701, -0.0083, -0.0256,  0.0205,  0.1087],\n          [ 0.0110,  0.0669,  0.0896,  0.0932, -0.0399]],\n\n         [[-0.0258,  0.0556, -0.0315,  0.0541, -0.0252],\n          [-0.0783,  0.0470,  0.0177,  0.0515,  0.1147],\n          [ 0.0788,  0.1095,  0.0062, -0.0993, -0.0810],\n          [-0.0717, -0.1018, -0.0579, -0.1063, -0.1065],\n          [-0.0690, -0.1138, -0.0709,  0.0440,  0.0963]],\n\n         [[-0.0343, -0.0336,  0.0617, -0.0570, -0.0546],\n          [ 0.0711, -0.1006,  0.0141,  0.1020,  0.0198],\n          [ 0.0314, -0.0672, -0.0016,  0.0063,  0.0283],\n          [ 0.0449,  0.1003, -0.0881,  0.0035, -0.0577],\n          [-0.0913, -0.0092, -0.1016,  0.0806,  0.0134]]],\n\n\n        [[[-0.0622,  0.0603, -0.1093, -0.0447, -0.0225],\n          [-0.0981, -0.0734, -0.0188,  0.0876,  0.1115],\n          [ 0.0735, -0.0689, -0.0755,  0.1008,  0.0408],\n          [ 0.0031,  0.0156, -0.0928, -0.0386,  0.1112],\n          [-0.0285, -0.0058, -0.0959, -0.0646, -0.0024]],\n\n         [[-0.0717, -0.0143,  0.0470, -0.1130,  0.0343],\n          [-0.0763, -0.0564,  0.0443,  0.0918, -0.0316],\n          [-0.0474, -0.1044, -0.0595, -0.1011, -0.0264],\n          [ 0.0236, -0.1082,  0.1008,  0.0724, -0.1130],\n          [-0.0552,  0.0377, -0.0237, -0.0126, -0.0521]],\n\n         [[ 0.0927, -0.0645,  0.0958,  0.0075,  0.0232],\n          [ 0.0901, -0.0190, -0.0657, -0.0187,  0.0937],\n          [-0.0857,  0.0262, -0.1135,  0.0605,  0.0427],\n          [ 0.0049,  0.0496,  0.0001,  0.0639, -0.0914],\n          [-0.0170,  0.0512,  0.1150,  0.0588, -0.0840]]],\n\n\n        [[[ 0.0888, -0.0257, -0.0247, -0.1050, -0.0182],\n          [ 0.0817,  0.0161, -0.0673,  0.0355, -0.0370],\n          [ 0.1054, -0.1002, -0.0365, -0.1115, -0.0455],\n          [ 0.0364,  0.1112,  0.0194,  0.1132,  0.0226],\n          [ 0.0667,  0.0926,  0.0965, -0.0646,  0.1062]],\n\n         [[ 0.0699, -0.0540, -0.0551, -0.0969,  0.0290],\n          [-0.0936,  0.0488,  0.0365, -0.1003,  0.0315],\n          [-0.0094,  0.0527,  0.0663, -0.1148,  0.1059],\n          [ 0.0968,  0.0459, -0.1055, -0.0412, -0.0335],\n          [-0.0297,  0.0651,  0.0420,  0.0915, -0.0432]],\n\n         [[ 0.0389,  0.0411, -0.0961, -0.1120, -0.0599],\n          [ 0.0790, -0.1087, -0.1005,  0.0647,  0.0623],\n          [ 0.0950, -0.0872, -0.0845,  0.0592,  0.1004],\n          [ 0.0691,  0.0181,  0.0381,  0.1096, -0.0745],\n          [-0.0524,  0.0808, -0.0790, -0.0637,  0.0843]]]])), ('bias', tensor([ 0.0364,  0.0373, -0.0489, -0.0016,  0.1057, -0.0693,  0.0009,  0.0549,\n        -0.0797,  0.1121]))])\n</pre> <p>Look at that! A bunch of random numbers for a weight and bias tensor.</p> <p>The shapes of these are manipulated by the inputs we passed to <code>nn.Conv2d()</code> when we set it up.</p> <p>Let's check them out.</p> In\u00a0[41]: Copied! <pre># Get shapes of weight and bias tensors within conv_layer_2\nprint(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\nprint(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -&gt; [out_channels=10]\")\n</pre> # Get shapes of weight and bias tensors within conv_layer_2 print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\") print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -&gt; [out_channels=10]\") <pre>conv_layer_2 weight shape: \ntorch.Size([10, 3, 5, 5]) -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\n\nconv_layer_2 bias shape: \ntorch.Size([10]) -&gt; [out_channels=10]\n</pre> <p>Question: What should we set the parameters of our <code>nn.Conv2d()</code> layers?</p> <p>That's a good one. But similar to many other things in machine learning, the values of these aren't set in stone (and recall, because these values are ones we can set ourselves, they're referred to as \"hyperparameters\").</p> <p>The best way to find out is to try out different values and see how they effect your model's performance.</p> <p>Or even better, find a working example on a problem similar to yours (like we've done with TinyVGG) and copy it.</p> <p>We're working with a different of layer here to what we've seen before.</p> <p>But the premise remains the same: start with random numbers and update them to better represent the data.</p> In\u00a0[42]: Copied! <pre># Print out original image shape without and with unsqueezed dimension\nprint(f\"Test image original shape: {test_image.shape}\")\nprint(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n\n# Create a sample nn.MaxPoo2d() layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2)\n\n# Pass data through just the conv_layer\ntest_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\nprint(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n\n# Pass data through the max pool layer\ntest_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\nprint(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")\n</pre> # Print out original image shape without and with unsqueezed dimension print(f\"Test image original shape: {test_image.shape}\") print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")  # Create a sample nn.MaxPoo2d() layer max_pool_layer = nn.MaxPool2d(kernel_size=2)  # Pass data through just the conv_layer test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0)) print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")  # Pass data through the max pool layer test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv) print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\") <pre>Test image original shape: torch.Size([3, 64, 64])\nTest image with unsqueezed dimension: torch.Size([1, 3, 64, 64])\nShape after going through conv_layer(): torch.Size([1, 10, 62, 62])\nShape after going through conv_layer() and max_pool_layer(): torch.Size([1, 10, 31, 31])\n</pre> <p>Notice the change in the shapes of what's happening in and out of a <code>nn.MaxPool2d()</code> layer.</p> <p>The <code>kernel_size</code> of the <code>nn.MaxPool2d()</code> layer will effects the size of the output shape.</p> <p>In our case, the shape halves from a <code>62x62</code> image to <code>31x31</code> image.</p> <p>Let's see this work with a smaller tensor.</p> In\u00a0[43]: Copied! <pre>torch.manual_seed(42)\n# Create a random tensor with a similiar number of dimensions to our images\nrandom_tensor = torch.randn(size=(1, 1, 2, 2))\nprint(f\"Random tensor:\\n{random_tensor}\")\nprint(f\"Random tensor shape: {random_tensor.shape}\")\n\n# Create a max pool layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value \n\n# Pass the random tensor through the max pool layer\nmax_pool_tensor = max_pool_layer(random_tensor)\nprint(f\"\\nMax pool tensor:\\n{max_pool_tensor} &lt;- this is the maximum value from random_tensor\")\nprint(f\"Max pool tensor shape: {max_pool_tensor.shape}\")\n</pre> torch.manual_seed(42) # Create a random tensor with a similiar number of dimensions to our images random_tensor = torch.randn(size=(1, 1, 2, 2)) print(f\"Random tensor:\\n{random_tensor}\") print(f\"Random tensor shape: {random_tensor.shape}\")  # Create a max pool layer max_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value   # Pass the random tensor through the max pool layer max_pool_tensor = max_pool_layer(random_tensor) print(f\"\\nMax pool tensor:\\n{max_pool_tensor} &lt;- this is the maximum value from random_tensor\") print(f\"Max pool tensor shape: {max_pool_tensor.shape}\") <pre>Random tensor:\ntensor([[[[0.3367, 0.1288],\n          [0.2345, 0.2303]]]])\nRandom tensor shape: torch.Size([1, 1, 2, 2])\n\nMax pool tensor:\ntensor([[[[0.3367]]]]) &lt;- this is the maximum value from random_tensor\nMax pool tensor shape: torch.Size([1, 1, 1, 1])\n</pre> <p>Notice the final two dimensions between <code>random_tensor</code> and <code>max_pool_tensor</code>, they go from <code>[2, 2]</code> to <code>[1, 1]</code>.</p> <p>In essence, they get halved.</p> <p>And the change would be different for different values of <code>kernel_size</code> for <code>nn.MaxPool2d()</code>.</p> <p>Also notice the value leftover in <code>max_pool_tensor</code> is the maximum value from <code>random_tensor</code>.</p> <p>What's happening here?</p> <p>This is another important piece of the puzzle of neural networks.</p> <p>Essentially, every layer in a neural network is trying to compress data from higher dimensional space to lower dimensional space.</p> <p>In other words, take a lot of numbers (raw data) and learn patterns in those numbers, patterns that are predictive whilst also being smaller in size than the original values.</p> <p>From an artificial intelligence perspective, you could consider the whole goal of a neural network to compress information.</p> <p></p> <p>This means, that from the point of view of a neural network, intelligence is compression.</p> <p>This is the idea of the use of a <code>nn.MaxPool2d()</code> layer: take the maximum value from a portion of a tensor and disregard the rest.</p> <p>In essence, lowering the dimensionality of a tensor whilst still retaining a (hopefully) significant portion of the information.</p> <p>It is the same story for a <code>nn.Conv2d()</code> layer.</p> <p>Except instead of just taking the maximum, the <code>nn.Conv2d()</code> performs a conovlutional operation on the data (see this in action on the CNN Explainer webpage).</p> <p>Exercise: What do you think the <code>nn.AvgPool2d()</code> layer does? Try making a random tensor like we did above and passing it through. Check the input and output shapes as well as the input and output values.</p> <p>Extra-curriculum: Lookup \"most common convolutional neural networks\", what architectures do you find? Are any of them contained within the <code>torchvision.models</code> library? What do you think you could do with these?</p> In\u00a0[44]: Copied! <pre># Setup loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_2.parameters(), \n                             lr=0.1)\n</pre> # Setup loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model_2.parameters(),                               lr=0.1) In\u00a0[45]: Copied! <pre>torch.manual_seed(42)\n\n# Measure time\nfrom timeit import default_timer as timer\ntrain_time_start_model_2 = timer()\n\n# Train and test model \nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_2, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_2,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n\ntrain_time_end_model_2 = timer()\ntotal_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n                                           end=train_time_end_model_2,\n                                           device=device)\n</pre> torch.manual_seed(42)  # Measure time from timeit import default_timer as timer train_time_start_model_2 = timer()  # Train and test model  epochs = 3 for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n---------\")     train_step(data_loader=train_dataloader,          model=model_2,          loss_fn=loss_fn,         optimizer=optimizer,         accuracy_fn=accuracy_fn,         device=device     )     test_step(data_loader=test_dataloader,         model=model_2,         loss_fn=loss_fn,         accuracy_fn=accuracy_fn,         device=device     )  train_time_end_model_2 = timer() total_train_time_model_2 = print_train_time(start=train_time_start_model_2,                                            end=train_time_end_model_2,                                            device=device) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0\n---------\nTrain loss: 0.59411 | Train accuracy: 78.41%\nTest loss: 0.39967 | Test accuracy: 85.70%\n\nEpoch: 1\n---------\nTrain loss: 0.36450 | Train accuracy: 86.81%\nTest loss: 0.34607 | Test accuracy: 87.48%\n\nEpoch: 2\n---------\nTrain loss: 0.32553 | Train accuracy: 88.33%\nTest loss: 0.32664 | Test accuracy: 88.23%\n\nTrain time on cuda: 21.099 seconds\n</pre> <p>Woah! Looks like the convolutional and max pooling layers helped improve performance a little.</p> <p>Let's evaluate <code>model_2</code>'s results with our <code>eval_model()</code> function.</p> In\u00a0[46]: Copied! <pre># Get model_2 results \nmodel_2_results = eval_model(\n    model=model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn,\n    accuracy_fn=accuracy_fn\n)\nmodel_2_results\n</pre> # Get model_2 results  model_2_results = eval_model(     model=model_2,     data_loader=test_dataloader,     loss_fn=loss_fn,     accuracy_fn=accuracy_fn ) model_2_results Out[46]: <pre>{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.32664385437965393,\n 'model_acc': 88.22883386581469}</pre> In\u00a0[47]: Copied! <pre>import pandas as pd\ncompare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\ncompare_results\n</pre> import pandas as pd compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results]) compare_results Out[47]: model_name model_loss model_acc 0 FashionMNISTModelV0 0.476639 83.426518 1 FashionMNISTModelV1 0.685001 75.019968 2 FashionMNISTModelV2 0.326644 88.228834 <p>Nice!</p> <p>We can add the training time values too.</p> In\u00a0[48]: Copied! <pre># Add training times to results comparison\ncompare_results[\"training_time\"] = [total_train_time_model_0,\n                                    total_train_time_model_1,\n                                    total_train_time_model_2]\ncompare_results\n</pre> # Add training times to results comparison compare_results[\"training_time\"] = [total_train_time_model_0,                                     total_train_time_model_1,                                     total_train_time_model_2] compare_results Out[48]: model_name model_loss model_acc training_time 0 FashionMNISTModelV0 0.476639 83.426518 14.974887 1 FashionMNISTModelV1 0.685001 75.019968 16.942553 2 FashionMNISTModelV2 0.326644 88.228834 21.098929 <p>It looks like our CNN (<code>FashionMNISTModelV2</code>) model performed the best (lowest loss, highest accuracy) but had the longest training time.</p> <p>And our baseline model (<code>FashionMNISTModelV0</code>) performed better than <code>model_1</code> (<code>FashionMNISTModelV1</code>) but took longer to train (this is likely because we used a CPU to train <code>model_0</code> but a GPU to train <code>model_1</code>).</p> <p>The tradeoffs here are known as the performance-speed tradeoff.</p> <p>Generally, you get better performance out of a larger, more complex model (like we did with <code>model_2</code>).</p> <p>However, this performance increase often comes at a sacrifice of training speed and inference speed.</p> <p>Note: The training times you get will be very dependant on the hardware you use.</p> <p>Generally, the more CPU cores you have, the faster your models will train on CPU. And similar for GPUs.</p> <p>Newer hardware (in terms of age) will also often train models faster due to incorporating technology advances.</p> <p>How about we get visual?</p> In\u00a0[49]: Copied! <pre># Visualize our model results\ncompare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\nplt.xlabel(\"accuracy (%)\")\nplt.ylabel(\"model\");\n</pre> # Visualize our model results compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\") plt.xlabel(\"accuracy (%)\") plt.ylabel(\"model\"); In\u00a0[50]: Copied! <pre>def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n    pred_probs = []\n    model.eval()\n    with torch.inference_mode():\n        for sample in data:\n            # Prepare sample\n            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n\n            # Forward pass (model outputs raw logit)\n            pred_logit = model(sample)\n\n            # Get prediction probability (logit -&gt; prediction probability)\n            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n\n            # Get pred_prob off GPU for further calculations\n            pred_probs.append(pred_prob.cpu())\n            \n    # Stack the pred_probs to turn list into a tensor\n    return torch.stack(pred_probs)\n</pre> def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):     pred_probs = []     model.eval()     with torch.inference_mode():         for sample in data:             # Prepare sample             sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device              # Forward pass (model outputs raw logit)             pred_logit = model(sample)              # Get prediction probability (logit -&gt; prediction probability)             pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)              # Get pred_prob off GPU for further calculations             pred_probs.append(pred_prob.cpu())                  # Stack the pred_probs to turn list into a tensor     return torch.stack(pred_probs) In\u00a0[51]: Copied! <pre>import random\nrandom.seed(42)\ntest_samples = []\ntest_labels = []\nfor sample, label in random.sample(list(test_data), k=9):\n    test_samples.append(sample)\n    test_labels.append(label)\n\n# View the first test sample shape and label\nprint(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")\n</pre> import random random.seed(42) test_samples = [] test_labels = [] for sample, label in random.sample(list(test_data), k=9):     test_samples.append(sample)     test_labels.append(label)  # View the first test sample shape and label print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\") <pre>Test sample image shape: torch.Size([1, 28, 28])\nTest sample label: 5 (Sandal)\n</pre> <p>And now we can use our <code>make_predictions()</code> function to predict on <code>test_samples</code>.</p> In\u00a0[52]: Copied! <pre># Make predictions on test samples with model 2\npred_probs= make_predictions(model=model_2, \n                             data=test_samples)\n\n# View first two prediction probabilities list\npred_probs[:2]\n</pre> # Make predictions on test samples with model 2 pred_probs= make_predictions(model=model_2,                               data=test_samples)  # View first two prediction probabilities list pred_probs[:2] Out[52]: <pre>tensor([[2.3550e-07, 1.7185e-08, 4.6618e-07, 6.1371e-08, 5.1185e-08, 9.9957e-01,\n         3.7702e-07, 1.5924e-05, 3.7681e-05, 3.7831e-04],\n        [7.3275e-02, 6.7410e-01, 3.7231e-03, 8.8129e-02, 1.0114e-01, 6.9186e-05,\n         5.8674e-02, 4.2595e-04, 3.8635e-04, 7.1354e-05]])</pre> <p>Excellent!</p> <p>And now we can go from prediction probabilities to prediction labels by taking the <code>torch.argmax()</code> of the output of the <code>torch.softmax()</code> activation function.</p> In\u00a0[53]: Copied! <pre># Turn the prediction probabilities into prediction labels by taking the argmax()\npred_classes = pred_probs.argmax(dim=1)\npred_classes\n</pre> # Turn the prediction probabilities into prediction labels by taking the argmax() pred_classes = pred_probs.argmax(dim=1) pred_classes Out[53]: <pre>tensor([5, 1, 7, 4, 3, 0, 4, 7, 1])</pre> In\u00a0[54]: Copied! <pre># Are our predictions in the same form as our test labels? \ntest_labels, pred_classes\n</pre> # Are our predictions in the same form as our test labels?  test_labels, pred_classes Out[54]: <pre>([5, 1, 7, 4, 3, 0, 4, 7, 1], tensor([5, 1, 7, 4, 3, 0, 4, 7, 1]))</pre> <p>Now our predicted classes are in the same format as our test labels, we can compare.</p> <p>Since we're dealing with image data, let's stay true to the data explorer's motto.</p> <p>\"Visualize, visualize, visualize!\"</p> In\u00a0[55]: Copied! <pre># Plot predictions\nplt.figure(figsize=(9, 9))\nnrows = 3\nncols = 3\nfor i, sample in enumerate(test_samples):\n  # Create a subplot\n  plt.subplot(nrows, ncols, i+1)\n\n  # Plot the target image\n  plt.imshow(sample.squeeze(), cmap=\"gray\")\n\n  # Find the prediction label (in text form, e.g. \"Sandal\")\n  pred_label = class_names[pred_classes[i]]\n\n  # Get the truth label (in text form, e.g. \"T-shirt\")\n  truth_label = class_names[test_labels[i]] \n\n  # Create the title text of the plot\n  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n  \n  # Check for equality and change title colour accordingly\n  if pred_label == truth_label:\n      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n  else:\n      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n  plt.axis(False);\n</pre> # Plot predictions plt.figure(figsize=(9, 9)) nrows = 3 ncols = 3 for i, sample in enumerate(test_samples):   # Create a subplot   plt.subplot(nrows, ncols, i+1)    # Plot the target image   plt.imshow(sample.squeeze(), cmap=\"gray\")    # Find the prediction label (in text form, e.g. \"Sandal\")   pred_label = class_names[pred_classes[i]]    # Get the truth label (in text form, e.g. \"T-shirt\")   truth_label = class_names[test_labels[i]]     # Create the title text of the plot   title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"      # Check for equality and change title colour accordingly   if pred_label == truth_label:       plt.title(title_text, fontsize=10, c=\"g\") # green text if correct   else:       plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong   plt.axis(False); <p>Well, well, well, doesn't that look good!</p> <p>Not bad for a couple dozen lines of PyTorch code!</p> In\u00a0[56]: Copied! <pre># Import tqdm for progress bar\nfrom tqdm.auto import tqdm\n\n# 1. Make predictions with trained model\ny_preds = []\nmodel_2.eval()\nwith torch.inference_mode():\n  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n    # Send data and targets to target device\n    X, y = X.to(device), y.to(device)\n    # Do the forward pass\n    y_logit = model_2(X)\n    # Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels\n    y_pred = torch.softmax(y_logit.squeeze(), dim=0).argmax(dim=1)\n    # Put predictions on CPU for evaluation\n    y_preds.append(y_pred.cpu())\n# Concatenate list of predictions into a tensor\ny_pred_tensor = torch.cat(y_preds)\n</pre> # Import tqdm for progress bar from tqdm.auto import tqdm  # 1. Make predictions with trained model y_preds = [] model_2.eval() with torch.inference_mode():   for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):     # Send data and targets to target device     X, y = X.to(device), y.to(device)     # Do the forward pass     y_logit = model_2(X)     # Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels     y_pred = torch.softmax(y_logit.squeeze(), dim=0).argmax(dim=1)     # Put predictions on CPU for evaluation     y_preds.append(y_pred.cpu()) # Concatenate list of predictions into a tensor y_pred_tensor = torch.cat(y_preds) <pre>Making predictions:   0%|          | 0/313 [00:00&lt;?, ?it/s]</pre> <p>Wonderful!</p> <p>Now we've got predictions, let's go through steps 2 &amp; 3: 2. Make a confusion matrix using <code>torchmetrics.ConfusionMatrix</code>. 3. Plot the confusion matrix using <code>mlxtend.plotting.plot_confusion_matrix()</code>.</p> <p>First we'll need to make sure we've got <code>torchmetrics</code> and <code>mlxtend</code> installed (these two libraries will help us make and visual a confusion matrix).</p> <p>Note: If you're using Google Colab, the default version of <code>mlxtend</code> installed is 0.14.0 (as of March 2022), however, for the parameters of the <code>plot_confusion_matrix()</code> function we'd like use, we need 0.19.0 or higher.</p> In\u00a0[57]: Copied! <pre># See if torchmetrics exists, if not, install it\ntry:\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n    assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend verison should be 0.19.0 or higher\"\nexcept:\n    !pip install -q torchmetrics -U mlxtend # &lt;- Note: If you're using Google Colab, this may require restarting the runtime\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n</pre> # See if torchmetrics exists, if not, install it try:     import torchmetrics, mlxtend     print(f\"mlxtend version: {mlxtend.__version__}\")     assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend verison should be 0.19.0 or higher\" except:     !pip install -q torchmetrics -U mlxtend # &lt;- Note: If you're using Google Colab, this may require restarting the runtime     import torchmetrics, mlxtend     print(f\"mlxtend version: {mlxtend.__version__}\") <pre>mlxtend version: 0.19.0\n</pre> <p>To plot the confusion matrix, we need to make sure we've got and <code>mlxtend</code> version of 0.19.0 or higher.</p> In\u00a0[58]: Copied! <pre># Import mlxtend upgraded version\nimport mlxtend \nprint(mlxtend.__version__)\nassert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # should be version 0.19.0 or higher\n</pre> # Import mlxtend upgraded version import mlxtend  print(mlxtend.__version__) assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # should be version 0.19.0 or higher <pre>0.19.0\n</pre> <p><code>torchmetrics</code> and <code>mlxtend</code> installed, let's make a confusion matrix!</p> <p>First we'll create a <code>torchmetrics.ConfusionMatrix</code> instance telling it how many classes we're dealing with by setting <code>num_classes=len(class_names)</code>.</p> <p>Then we'll create a confusion matrix (in tensor format) by passing our instance our model's predictions (<code>preds=y_pred_tensor</code>) and targets (<code>target=test_data.targets</code>).</p> <p>Finally we can plot our confision matrix using the <code>plot_confusion_matrix()</code> function from <code>mlxtend.plotting</code>.</p> In\u00a0[59]: Copied! <pre>from torchmetrics import ConfusionMatrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\n# 2. Setup confusion matrix instance and compare predictions to targets\nconfmat = ConfusionMatrix(num_classes=len(class_names))\nconfmat_tensor = confmat(preds=y_pred_tensor,\n                         target=test_data.targets)\n\n# 3. Plot the confusion matrix\nfig, ax = plot_confusion_matrix(\n    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy \n    class_names=class_names, # turn the row and column labels into class names\n    figsize=(10, 7)\n);\n</pre> from torchmetrics import ConfusionMatrix from mlxtend.plotting import plot_confusion_matrix  # 2. Setup confusion matrix instance and compare predictions to targets confmat = ConfusionMatrix(num_classes=len(class_names)) confmat_tensor = confmat(preds=y_pred_tensor,                          target=test_data.targets)  # 3. Plot the confusion matrix fig, ax = plot_confusion_matrix(     conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy      class_names=class_names, # turn the row and column labels into class names     figsize=(10, 7) ); <p>Woah! Doesn't that look good?</p> <p>We can see our model does fairly well since most of the dark squares are down the diagonal from top left to bottom right (and ideal model will have only values in these squares and 0 everywhere else).</p> <p>The model gets most \"confused\" on classes that are similar, for example predicting \"Pullover\" for images that are actually labelled \"Shirt\".</p> <p>And the same for predicting \"Shirt\" for classes that are actually labelled \"T-shirt/top\".</p> <p>This kind of information is often more helpful that a single accuracy metric because it tells use where a model is getting things wrong.</p> <p>It also hints at why the model may be getting certain things wrong.</p> <p>It's understandable the model sometimes predicts \"Shirt\" for images labelled \"T-shirt/top\".</p> <p>We can use this kind of information to further inspect our models and data to see how it could be improved.</p> <p>Exercise: Use the trained <code>model_2</code> to make predictions on the test FashionMNIST dataset. Then plot some predictions where the model was wrong alongside what the label of the image should've been. After visualing these predictions do you think it's more of a modelling error or a data error? As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?</p> In\u00a0[60]: Copied! <pre>from pathlib import Path\n\n# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, # create parent directories if needed\n                 exist_ok=True # if models directory already exists, don't error\n)\n\n# Create model save path\nMODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# Save the model state dict\nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters\n           f=MODEL_SAVE_PATH)\n</pre> from pathlib import Path  # Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, # create parent directories if needed                  exist_ok=True # if models directory already exists, don't error )  # Create model save path MODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # Save the model state dict print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters            f=MODEL_SAVE_PATH) <pre>Saving model to: models/03_pytorch_computer_vision_model_2.pth\n</pre> <p>Now we've got a saved model <code>state_dict()</code> we can load it back in using a combination of <code>load_state_dict()</code> and <code>torch.load()</code>.</p> <p>Since we're using <code>load_state_dict()</code>, we'll need to create a new instance of <code>FashionMNISTModelV2()</code> with the same input parameters as our saved model <code>state_dict()</code>.</p> In\u00a0[61]: Copied! <pre># Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict())\n# Note: loading model will error if the shapes here aren't the same as the saved version\nloaded_model_2 = FashionMNISTModelV2(input_shape=1, \n                                    hidden_units=10, # try changing this to 128 and seeing what happens \n                                    output_shape=10) \n\n# Load in the saved state_dict()\nloaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n\n# Send model to GPU\nloaded_model_2 = loaded_model_2.to(device)\n</pre> # Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict()) # Note: loading model will error if the shapes here aren't the same as the saved version loaded_model_2 = FashionMNISTModelV2(input_shape=1,                                      hidden_units=10, # try changing this to 128 and seeing what happens                                      output_shape=10)   # Load in the saved state_dict() loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))  # Send model to GPU loaded_model_2 = loaded_model_2.to(device) <p>And now we've got a loaded model we can evaluate it with <code>eval_model()</code> to make sure its parameters work similarly to <code>model_2</code> prior to saving.</p> In\u00a0[62]: Copied! <pre># Evaluate loaded model\ntorch.manual_seed(42)\n\nloaded_model_2_results = eval_model(\n    model=loaded_model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn\n)\n\nloaded_model_2_results\n</pre> # Evaluate loaded model torch.manual_seed(42)  loaded_model_2_results = eval_model(     model=loaded_model_2,     data_loader=test_dataloader,     loss_fn=loss_fn,      accuracy_fn=accuracy_fn )  loaded_model_2_results Out[62]: <pre>{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.32664385437965393,\n 'model_acc': 88.22883386581469}</pre> <p>Do these results look the same as <code>model_2_results</code>?</p> In\u00a0[63]: Copied! <pre>model_2_results\n</pre> model_2_results Out[63]: <pre>{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.32664385437965393,\n 'model_acc': 88.22883386581469}</pre> <p>We can find out if two tensors are close to each other using <code>torch.isclose()</code> and passing in a tolerance level of closeness via the parameters <code>atol</code> (absolute tolerance) and <code>rtol</code> (relative tolerance).</p> <p>If our model's results are close, the output of <code>torch.isclose()</code> should be true.</p> In\u00a0[64]: Copied! <pre># Check to see if results are close to each other (if they are very far away, there may be an error)\ntorch.isclose(torch.tensor(model_2_results[\"model_loss\"]), \n              torch.tensor(loaded_model_2_results[\"model_loss\"]),\n              atol=1e-08, # absolute tolerance\n              rtol=0.0001) # relative tolerance\n</pre> # Check to see if results are close to each other (if they are very far away, there may be an error) torch.isclose(torch.tensor(model_2_results[\"model_loss\"]),                torch.tensor(loaded_model_2_results[\"model_loss\"]),               atol=1e-08, # absolute tolerance               rtol=0.0001) # relative tolerance Out[64]: <pre>tensor(True)</pre>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#03-pytorch-computer-vision","title":"03. PyTorch Computer Vision\u00b6","text":"<p>\u4f30\u8ba1\u5728\u63a5\u4e0b\u6765\u7684\u4e00\u5e74\u5185\uff0c\u5e94\u8be5\u4e0d\u4f1a\u53bb\u6d89\u53ca\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u76f8\u5173\u5185\u5bb9\uff0c\u4f46\u662f\u4e4b\u540e\u5e76\u4e0d\u80fd\u660e\u786e\uff0c\u5f71\u50cf\u5b66\u662f\u975e\u5e38\u5173\u952e\u7684\uff0c\u533b\u7597\u5f71\u50cf\u65b9\u9762\u7684\u5c31\u4e1a\u4e5f\u975e\u5e38\u597d\u3002\u4f46\u662f\u56fe\u50cf\u8bc6\u522b\u57fa\u4e8e\u7684CNN\u7f51\u7edc\u662f\u975e\u5e38\u6709\u7528\u7684\uff0c\u4e5f\u53ef\u4ee5\u9002\u7528\u5230\u5176\u4ed6\u7684\u95ee\u9898\u5f53\u4e2d\u3002</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#where-does-computer-vision-get-used","title":"Where does computer vision get used?\u00b6","text":"<p>If you use a smartphone, you've already used computer vision.</p> <p>Camera and photo apps use computer vision to enhance and sort images.</p> <p>Modern\uff08\u73b0\u4ee3\uff09 cars use computer vision to avoid other cars and stay within lane lines.</p> <p>Manufacturers\uff08\u5382\u5bb6\uff09 use computer vision to identify defects in various products.</p> <p>Security cameras use computer vision to detect potential intruders\uff08\u5165\u4fb5\u8005\uff09.</p> <p>In essence, anything that can described in a visual sense can be a potential computer vision problem.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to apply the PyTorch Workflow we've been learning in the past couple of sections to computer vision.</p> <p></p> <p>Specifically, we're going to cover:</p> Topic Contents 0. Computer vision libraries in PyTorch PyTorch has a bunch of built-in helpful computer vision libraries, let's check them out. 1. Load data To practice computer vision, we'll start with some images of different pieces of clothing from FashionMNIST. 2. Prepare data We've got some images, let's load them in with a PyTorch <code>DataLoader</code> so we can use them with our training loop. 3. Model 0: Building a baseline model Here we'll create a multi-class classification model to learn patterns in the data, we'll also choose a loss function, optimizer and build a training loop. 4. Making predictions and evaluting model 0 Let's make some predictions with our baseline model and evaluate them. 5. Setup device agnostic code for future models It's best practice to write device-agnostic code, so let's set it up. 6. Model 1: Adding non-linearity Experimenting is a large part of machine learning, let's try and improve upon our baseline model by adding non-linear layers. 7. Model 2: Convolutional Neural Network (CNN) Time to get computer vision specific and introduce the powerful convolutional neural network architecture. 8. Comparing our models We've built three different models, let's compare them. 9. Evaluating our best model Let's make some predictons on random images and evaluate our best model. 10. Making a confusion matrix A confusion matrix is a great way to evaluate a classification model, let's see how we can make one. 11. Saving and loading the best performing model Since we might want to use our model for later, let's save it and make sure it loads back in correctly."},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#where-can-can-you-get-help","title":"Where can can you get help?\u00b6","text":"<p>All of the materials for this course live on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page there too.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#0-computer-vision-libraries-in-pytorch","title":"0. Computer vision libraries in PyTorch\u00b6","text":"<p>Before we get started writing code, let's talk about some PyTorch computer vision libraries you should be aware of.</p> PyTorch module What does it do? <code>torchvision</code> Contains datasets, model architectures and image transformations often used for computer vision problems. <code>torchvision.datasets</code> Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains a series of base classes for making custom datasets. <code>torchvision.models</code> This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems. <code>torchvision.transforms</code> Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here. <code>torch.utils.data.Dataset</code> Base dataset class for PyTorch. <code>torch.utils.data.DataLoader</code> Creates a Python iteralbe over a dataset (created with <code>torch.utils.data.Dataset</code>). <p>Note: The <code>torch.utils.data.Dataset</code> and <code>torch.utils.data.DataLoader</code> classes aren't only for computer vision in PyTorch, they are capable of dealing with many different types of data.</p> <p><code>torch.utils.Dataset</code>\u548c<code>torch.utils.DataLoader</code>\u662f\u5728\u4efb\u4f55\u95ee\u9898\u4e2d\u90fd\u53ef\u4ee5\u7528\u5230\u7684\u51fd\u6570\u3002\u8fd9\u79cd\u5c06\u6570\u636e\u96c6\u5c01\u88c5\u597d\u7684\u65b9\u5f0f\u66f4\u65b9\u4fbf\u3002</p> <p>Now we've covered some of the most important PyTorch computer vision libraries, let's import the relevant dependencies.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#1-getting-a-dataset","title":"1. Getting a dataset\u00b6","text":"<p>To begin working on a computer vision problem, let's get a computer vision dataset.</p> <p>We're going to start with FashionMNIST.\uff08\u7ecf\u5178\u6570\u636e\u96c6\uff09</p> <p>MNIST stands for Modified National Institute of Standards and Technology.</p> <p>The original MNIST dataset contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services.\uff08\u539f\u59cb\u6570\u636e\u96c6\u662f0-9\u7684\u6570\u5b57\uff09</p> <p>FashionMNIST, made by Zalando Research, is a similar setup\uff08\u8bbe\u7f6e\uff09.</p> <p>Except it contains grayscale images of 10 different kinds of clothing.\uff0810\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u7070\u5ea6\u56fe\u56fe\u50cf\uff09</p> <p> <code>torchvision.datasets</code> contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), it's a multi-class classification problem.</p> <p>Later, we'll be building a computer vision neural network to identify the different styles of clothing in these images.</p> <p>PyTorch has a bunch of common computer vision datasets stored in <code>torchvision.datasets</code>.</p> <p>Including FashionMNIST in <code>torchvision.datasets.FashionMNIST()</code>.</p> <p>To download it, we provide the following parameters:</p> <ul> <li><code>root: str</code> - which folder do you want to download the data to? \u4e0b\u8f7d\u5730\u5740</li> <li><code>train: Bool</code> - do you want the training or test split? \u8bad\u7ec3\u96c6\u8fd8\u662f\u6d4b\u8bd5\u96c6</li> <li><code>download: Bool</code> - should the data be downloaded? \u6570\u636e\u662f\u5426\u5e94\u8be5\u88ab\u4e0b\u8f7d</li> <li><code>transform: torchvision.transforms</code> - what transformations would you like to do on the data? \u5982\u4f55\u5bf9\u6570\u636e\u8f6c\u6362</li> <li><code>target_transform</code> - you can transform the targets (labels) if you like too. \u5982\u4f55\u5bf9label\u8f6c\u6362</li> </ul> <p>Many other datasets in <code>torchvision</code> have these parameter options.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#11-input-and-output-shapes-of-a-computer-vision-model","title":"1.1 Input and output shapes of a computer vision model\u00b6","text":"<p>We've got a big tensor of values (the image) leading to a single value for the target (the label).</p> <p>Let's see the image shape.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#12-visualizing-our-data","title":"1.2 Visualizing our data\u00b6","text":""},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#2-prepare-dataloader","title":"2. Prepare DataLoader\u00b6","text":"<p>Now we've got a dataset ready to go.</p> <p>The next step is to prepare it with a <code>torch.utils.data.DataLoader</code> or <code>DataLoader</code> for short.</p> <p>The <code>DataLoader</code> does what you think it might do.</p> <p>It helps load data into a model.</p> <p>For training and for inference.</p> <p>It turns a large <code>Dataset</code> into a Python iterable of smaller chunks. DataLoader\u6240\u505a\u7684\u4e8b\u60c5\u5c31\u662f\u5c06\u6570\u636e\u5206\u6210\u5c0f\u6279\u6b21\u7684\u8f93\u5165\u5230\u6a21\u578b\u4e2d\uff0c\u90a3\u4e48\u4e3a\u4ec0\u4e48\u9700\u8981batch\u5462\uff1f</p> <p>These smaller chunks are called batches or mini-batches and can be set by the <code>batch_size</code> parameter.</p> <p>Why do this? \u4e0b\u9762\u56de\u7b54\u4e86\u95ee\u9898</p> <p>Because it's more computationally efficient.</p> <p>In an ideal world you could do the forward pass and backward pass across all of your data at once.</p> <p>But once you start using really large datasets, unless you've got infinite computing power, it's easier to break them up into batches.</p> <p>It also gives your model more opportunities to improve.</p> <p>With mini-batches (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).\u56e0\u4e3a\u6bcf\u6b21\u4e0d\u540c\u7684batch\u90fd\u4f1a\u66f4\u65b0\u53c2\u6570\uff0c\u56e0\u6b64mini-batches\u6709\u66f4\u591a\u7684\u53c2\u6570\u66f4\u65b0\u673a\u4f1a\uff0c\u53ef\u80fd\u66f4\u6709\u5229\u4e8e\u6a21\u578b\u8bad\u7ec3\u3002</p> <p>What's a good batch size?</p> <p>32 is a good place to start for a fair amount of problems. \u63a8\u8350\u4ee532\u4e3abatch_size\uff0c\u4f46\u5b83\u4e5f\u662f\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u8d85\u53c2\u6570\u3002</p> <p>But since this is a value you can set (a hyperparameter) you can try all different kinds of values, though generally powers of 2 are used most often (e.g. 32, 64, 128, 256, 512).</p> <p> Batching FashionMNIST with a batch size of 32 and shuffle turned on. A similar batching process will occur for other datasets but will differ depending on the batch size.</p> <p>Let's create <code>DataLoader</code>'s for our training and test sets. DataLoaer\u53ef\u4ee5\u5b8c\u6210\u7684\u4e8b\u60c5\u5f88\u591a\uff0c\u4e0d\u8fc7\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u52a0\u8f7d\u548c\u5e76\u884c\u5904\u7406\u8fd9\u4e24\u9879\u3002</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#3-model-0-build-a-baseline-model","title":"3. Model 0: Build a baseline model\u00b6","text":"<p>Data loaded and prepared!</p> <p>Time to build a baseline model by subclassing <code>nn.Module</code>.</p> <p>A baseline model is one of the simplest models you can imagine.</p> <p>You use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.</p> <p>Our baseline will consist of two <code>nn.Linear()</code> layers.</p> <p>We've done this in a previous section but there's going to one slight difference.</p> <p>Because we're working with image data, we're going to use a different layer to start things off.</p> <p>And that's the <code>nn.Flatten()</code> layer. \u8fd9\u4e2a\u662f\u62c9\u5e73\u64cd\u4f5c\uff0c\u5c06\u5176\u8f6c\u6362\u62101\u7ef4\u7684\u5411\u91cf\u3002</p> <p><code>nn.Flatten()</code> compresses the dimensions of a tensor into a single vector.</p> <p>This is easier to understand when you see it.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#31-setup-loss-optimizer-and-evaluation-metrics","title":"3.1 Setup loss, optimizer and evaluation metrics\u00b6","text":"<p>Since we're working on a classification problem, let's bring in our <code>helper_functions.py</code> script and subsequently the <code>accuracy_fn()</code> we defined in notebook 02.</p> <p>Note: Rather than importing and using our own accuracy function or evaluation metric(s), you could import various evaluation metrics from the TorchMetrics package.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#32-creating-a-function-to-time-our-experiments","title":"3.2 Creating a function to time our experiments\u00b6","text":"<p>Loss function and optimizer ready!</p> <p>It's time to start training a model.</p> <p>But how about we do a little experiment while we train.</p> <p>I mean, let's make a timing function to measure the time it takes our model to train on CPU versus using a GPU.</p> <p>We'll train this model on the CPU but the next one on the GPU and see what happens.</p> <p>Our timing function will import the <code>timeit.default_timer()</code> function from the Python <code>timeit</code> module.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#33-creating-a-training-loop-and-training-a-model-on-batches-of-data","title":"3.3 Creating a training loop and training a model on batches of data\u00b6","text":"<p>Beautiful!</p> <p>Looks like we've got all of the pieces of the puzzle ready to go, a timer, a loss function, an optimizer, a model and most importantly, some data.</p> <p>Let's now create a training loop and a testing loop to train and evaluate our model.</p> <p>We'll be using the same steps as the previous notebook(s), though since our data is now in batch form, we'll add another loop to loop through our data batches.</p> <p>Our data batches are contained within our <code>DataLoader</code>s, <code>train_dataloader</code> and <code>test_dataloader</code> for the training and test data splits respectively.</p> <p>A batch is <code>BATCH_SIZE</code> samples of <code>X</code> (features) and <code>y</code> (labels), since we're using <code>BATCH_SIZE=32</code>, our batches have 32 samples of images and targets.</p> <p>And since we're computing on batches of data, our loss and evaluation metrics will be calculated per batch rather than across the whole dataset.</p> <p>This means we'll have to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader.</p> <p>Let's step through it:</p> <ol> <li>Loop through epochs.</li> <li>Loop through training batches, perform training steps, calculate the train loss per batch.</li> <li>Loop through testing batches, perform testing steps, calculate the test loss per batch.</li> <li>Print out what's happening.</li> <li>Time it all (for fun).</li> </ol> <p>A fair few steps but...</p> <p>...if in doubt, code it out.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#4-make-predictions-and-get-model-0-results","title":"4. Make predictions and get Model 0 results\u00b6","text":"<p>Since we're going to be building a few models, it's a good idea to write some code to evaluate them all in similar ways.</p> <p>Namely, let's create a function that takes in a trained model, a <code>DataLoader</code>, a loss function and an accuracy function.</p> <p>The function will use the model to make predictions on the data in the <code>DataLoader</code> and then we can evaluate those predictions using the loss function and accuracy function.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#5-setup-device-agnostic-code-for-using-a-gpu-if-there-is-one","title":"5. Setup device agnostic-code (for using a GPU if there is one)\u00b6","text":"<p>We've seen how long it takes to train ma PyTorch model on 60,000 samples on CPU.</p> <p>Note: Model training time is dependent on hardware used. Generally, more processors means faster training and smaller models on smaller datasets will often train faster than large models and large datasets.</p> <p>Now let's setup some device-agnostic code for our models and data to run on GPU if it's available.</p> <p>If you're running this notebook on Google Colab, and you don't a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>. If you do this, your runtime will likely reset and you'll have to run all of the cells above by going <code>Runtime -&gt; Run before</code>.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#6-model-1-building-a-better-model-with-non-linearity","title":"6. Model 1: Building a better model with non-linearity\u00b6","text":"<p>We learned about the power of non-linearity in notebook 02.</p> <p>Seeing the data we've been working with, do you think it needs non-linear functions?</p> <p>And remember, linear means straight and non-linear means non-straight.</p> <p>Let's find out.</p> <p>We'll do so by recreating a similar model to before, except this time we'll put non-linear functions (<code>nn.ReLU()</code>) in between each linear layer.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#61-setup-loss-optimizer-and-evaluation-metrics","title":"6.1 Setup loss, optimizer and evaluation metrics\u00b6","text":"<p>As usual, we'll setup a loss function, an optimizer and an evaluation metric (we could do multiple evaluation metrics but we'll stick with accuracy for now).</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#62-functionizing-training-and-test-loops","title":"6.2 Functionizing training and test loops\u00b6","text":"<p>So far we've been writing train and test loops over and over.</p> <p>Let's write them again but this time we'll put them in functions so they can be called again and again.</p> <p>And because we're using device-agnostic code now, we'll be sure to call <code>.to(device)</code> on our feature (<code>X</code>) and target (<code>y</code>) tensors.</p> <p>For the training loop we'll create a function called <code>train_step()</code> which takes in a model, a <code>DataLoader</code> a loss function and an optimizer.</p> <p>The testing loop will be similar but it'll be called <code>test_step()</code> and it'll take in a model, a <code>DataLoader</code>, a loss function and an evaluation function.</p> <p>Note: Since these are functions, you can customize them in any way you like. What we're making here can be considered barebones training and testing functions for our specific classification use case.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#7-model-2-building-a-convolutional-neural-network-cnn","title":"7. Model 2: Building a Convolutional Neural Network (CNN)\u00b6","text":"<p>Alright, time to step things up a notch.</p> <p>It's time to create a Convolutional Neural Network (CNN or ConvNet).</p> <p>CNN's are known for their capabilities to find patterns in visual data.</p> <p>And since we're dealing with visual data, let's see if using a CNN model can improve upon our baseline.</p> <p>The CNN model we're going to be using is known as TinyVGG from the CNN Explainer website.</p> <p>It follows the typical structure of a convolutional neural network:</p> <p><code>Input layer -&gt; [Convolutional layer -&gt; activation layer -&gt; pooling layer] -&gt; Output layer</code></p> <p>Where the contents of <code>[Convolutional layer -&gt; activation layer -&gt; pooling layer]</code> can be upscaled and repeated multiple times, depending on requirements.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#what-model-should-i-use","title":"What model should I use?\u00b6","text":"<p>Question: Wait, you say CNN's are good for images, are there any other model types I should be aware of?</p> <p>Good question.</p> <p>This table is a good general guide for which model to use (though there are exceptions).</p> Problem type Model to use (generally) Code example Structured data (Excel spreadsheets, row and column data) Gradient boosted models, Random Forests, XGBoost <code>sklearn.ensemble</code>, XGBoost library Unstructured data (images, audio, language) Convolutional Neural Networks, Transformers <code>torchvision.models</code>, HuggingFace Transformers <p>Note: The table above is only for reference, the model you end up using will be highly dependant on the problem you're working on and the constraints you have (amount of data, latency requirements).</p> <p>Enough talking about models, let's now build a CNN that replicates the model on the CNN Explainer website.</p> <p></p> <p>To do so, we'll leverage the <code>nn.Conv2d()</code> and <code>nn.MaxPool2d()</code> layers from <code>torch.nn</code>.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#71-stepping-through-nnconv2d","title":"7.1 Stepping through <code>nn.Conv2d()</code>\u00b6","text":"<p>We could start using our model above and see what happens but let's first step through the two new layers we've added:</p> <ul> <li><code>nn.Conv2d()</code>, also known as a convolutional layer.</li> <li><code>nn.MaxPool2d()</code>, also known as a max pooling layer.</li> </ul> <p>Question: What does the \"2d\" in <code>nn.Conv2d()</code> stand for?</p> <p>The 2d is for 2-dimensional data. As in, our images have two dimensions: height and width. Yes, there's color channel dimension but each of the color channel dimensions have two dimensions too: height and width.</p> <p>For other dimensional data (such as 1D for text or 3D for 3D objects) there's also <code>nn.Conv1d()</code> and <code>nn.Conv3d()</code>.</p> <p>To test the layers out, let's create some toy data just like the data used on CNN Explainer.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#72-stepping-through-nnmaxpool2d","title":"7.2 Stepping through <code>nn.MaxPool2d()</code>\u00b6","text":"<p>Now let's check out what happens when we move data through <code>nn.MaxPool2d()</code>.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#73-setup-a-loss-function-and-optimizer-for-model_2","title":"7.3 Setup a loss function and optimizer for <code>model_2</code>\u00b6","text":"<p>We've stepped through the layers in our first CNN enough.</p> <p>But remember, if something still isn't clear, try starting small.</p> <p>Pick a single layer of a model, pass some data through it and see what happens.</p> <p>Now it's time to move forward and get to training!</p> <p>Let's setup a loss function and an optimizer.</p> <p>We'll use the functions as before, <code>nn.CrossEntropyLoss()</code> as the loss function (since we're working with multi-class classification data).</p> <p>And <code>torch.optim.SGD()</code> as the optimizer to optimize <code>model_2.parameters()</code> with a learning rate of <code>0.1</code>.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#74-training-and-testing-model_2-using-our-training-and-test-functions","title":"7.4 Training and testing <code>model_2</code> using our training and test functions\u00b6","text":"<p>Loss and optimizer ready!</p> <p>Time to train and test.</p> <p>We'll use our <code>train_step()</code> and <code>test_step()</code> functions we created before.</p> <p>We'll also measure the time to compare it to our other models.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#8-compare-model-results-and-training-time","title":"8. Compare model results and training time\u00b6","text":"<p>We've trained three different models.</p> <ol> <li><code>model_0</code> - our baseline model with two <code>nn.Linear()</code> layers.</li> <li><code>model_1</code> - the same setup as our baseline model except with <code>nn.ReLU()</code> layers in between the <code>nn.Linear()</code> layers.</li> <li><code>model_2</code> - our first CNN model that mimics the TinyVGG architecture on the CNN Explainer website.</li> </ol> <p>This is a regular practice in machine learning.</p> <p>Building multiple models and performing multiple training experiments to see which performs best.</p> <p>Let's combine our model results dictionaries into a DataFrame and find out.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#9-make-and-evaluate-random-predictions-with-best-model","title":"9. Make and evaluate random predictions with best model\u00b6","text":"<p>Alright, we've compared our models to each other, let's further evaluate our best performing model, <code>model_2</code>.</p> <p>To do so, let's create a function <code>make_predictions()</code> where we can pass the model and some data for it to predict on.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#10-making-a-confusion-matrix-for-further-prediction-evaluation","title":"10. Making a confusion matrix for further prediction evaluation\u00b6","text":"<p>There are many different evaluation metrics we can use for classification problems.</p> <p>One of the most visual is a confusion matrix.</p> <p>A confusion matrix shows you where your classification model got confused between predicitons and true labels.</p> <p>To make a confusion matrix, we'll go through three steps:</p> <ol> <li>Make predictions with our trained model, <code>model_2</code> (a confusion matrix compares predictions to true labels).</li> <li>Make a confusion matrix using <code>torch.ConfusionMatrix</code>.</li> <li>Plot the confusion matrix using <code>mlxtend.plotting.plot_confusion_matrix()</code>.</li> </ol> <p>Let's start by making predictions with our trained model.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#11-save-and-load-best-performing-model","title":"11. Save and load best performing model\u00b6","text":"<p>Let's finish this section off by saving and loading in our best performing model.</p> <p>Recall from notebook 01 we can save and load a PyTorch model using a combination of:</p> <ul> <li><code>torch.save</code> - a function to save a whole PyTorch model or a model's <code>state_dict()</code>.</li> <li><code>torch.load</code> - a function to load in a saved PyTorch object.</li> <li><code>torch.nn.Module.load_state_dict()</code> - a function to load a saved <code>state_dict()</code> into an existing model instance.</li> </ul> <p>You can see more of these three in the PyTorch saving and loading models documentation.</p> <p>For now, let's save our <code>model_2</code>'s <code>state_dict()</code> then load it back in and evaluate it to make sure the save and load went correctly.</p>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code in the sections above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 03</li> <li>Example solutions notebook for 03 (try the exercises before looking at this)</li> </ul> <ol> <li>What are 3 areas in industry where computer vision is currently being used?</li> <li>Search \"what is overfitting in machine learning\" and write down a sentence about what you find.</li> <li>Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each. Note: there are lots of these, so don't worry too much about all of them, just pick 3 and start with those.</li> <li>Spend 20-minutes reading and clicking through the CNN Explainer website.<ul> <li>Upload your own example image using the \"upload\" button and see what happens in each layer of a CNN as your image passes through it.</li> </ul> </li> <li>Load the <code>torchvision.datasets.MNIST()</code> train and test datasets.</li> <li>Visualize at least 5 different samples of the MNIST training dataset.</li> <li>Turn the MNIST train and test datasets into dataloaders using <code>torch.utils.data.DataLoader</code>, set the <code>batch_size=32</code>.</li> <li>Recreate <code>model_2</code> used in this notebook (the same model from the CNN Explainer website, also known as TinyVGG) capable of fitting on the MNIST dataset.</li> <li>Train the model you built in exercise 8. on CPU and GPU and see how long it takes on each.</li> <li>Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label.</li> <li>Plot a confusion matrix comparing your model's predictions to the truth labels.</li> <li>Create a random tensor of shape <code>[1, 3, 64, 64]</code> and pass it through a <code>nn.Conv2d()</code> layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the <code>kernel_size</code> parameter goes up and down?</li> <li>Use a model similar to the trained <code>model_2</code> from this notebook to make predictions on the test <code>torchvision.datasets.FashionMNIST</code> dataset.<ul> <li>Then plot some predictions where the model was wrong alongside what the label of the image should've been.</li> <li>After visualing these predictions do you think it's more of a modelling error or a data error?</li> <li>As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?</li> </ul> </li> </ol>"},{"location":"Learn/z2m-pytorch/03_pytorch_computer_vision/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Watch: MIT's Introduction to Deep Computer Vision lecture. This will give you a great intuition behind convolutional neural networks.</li> <li>Spend 10-minutes clicking thorugh the different options of the PyTorch vision library, what different modules are available?</li> <li>Lookup \"most common convolutional neural networks\", what architectures do you find? Are any of them contained within the <code>torchvision.models</code> library? What do you think you could do with these?</li> <li>For a large number of pretrained PyTorch computer vision models as well as many different extensions to PyTorch's computer vision functionalities check out the PyTorch Image Models library <code>timm</code> (Torch Image Models) by Ross Wightman.</li> </ul>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/","title":"04. PyTorch Custom Datasets","text":"In\u00a0[2]: Copied! <pre>import torch\nfrom torch import nn\n\n# Note: this notebook requires torch &gt;= 1.10.0\ntorch.__version__\n</pre> import torch from torch import nn  # Note: this notebook requires torch &gt;= 1.10.0 torch.__version__ Out[2]: <pre>'1.11.0'</pre> <p>And now let's follow best practice and setup device-agnostic code.</p> <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>. If you do this, your runtime will likely reset and you'll have to run all of the cells above by going <code>Runtime -&gt; Run before</code>.</p> In\u00a0[3]: Copied! <pre># Setup device-agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Setup device-agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[2]: Copied! <pre>import requests\nimport zipfile\nfrom pathlib import Path\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n</pre> import requests import zipfile from pathlib import Path  # Setup path to data folder data_path = Path(\"data/\") image_path = data_path / \"pizza_steak_sushi\"  # If the image folder doesn't exist, download it and prepare it...  if image_path.is_dir():     print(f\"{image_path} directory exists.\") else:     print(f\"Did not find {image_path} directory, creating one...\")     image_path.mkdir(parents=True, exist_ok=True)          # Download pizza, steak, sushi data     with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:         request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")         print(\"Downloading pizza, steak, sushi data...\")         f.write(request.content)      # Unzip pizza, steak, sushi data     with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:         print(\"Unzipping pizza, steak, sushi data...\")          zip_ref.extractall(image_path) <pre>data/pizza_steak_sushi directory exists.\n</pre> In\u00a0[5]: Copied! <pre>import os\ndef walk_through_dir(dir_path):\n\"\"\"\n  Walks through dir_path returning its contents.\n  Args:\n    dir_path (str or pathlib.Path): target directory\n  Returns:\n    A print out of:\n      number of subdiretories in dir_path\n      number of images (files) in each subdirectory\n      name of each subdirectory\n  \"\"\"\n  for dirpath, dirnames, filenames in os.walk(dir_path):\n    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n</pre> import os def walk_through_dir(dir_path):   \"\"\"   Walks through dir_path returning its contents.   Args:     dir_path (str or pathlib.Path): target directory      Returns:     A print out of:       number of subdiretories in dir_path       number of images (files) in each subdirectory       name of each subdirectory   \"\"\"   for dirpath, dirnames, filenames in os.walk(dir_path):     print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\") In\u00a0[6]: Copied! <pre>walk_through_dir(image_path)\n</pre> walk_through_dir(image_path) <pre>There are 2 directories and 0 images in 'data\\pizza_steak_sushi'.\nThere are 3 directories and 0 images in 'data\\pizza_steak_sushi\\test'.\nThere are 0 directories and 25 images in 'data\\pizza_steak_sushi\\test\\pizza'.\nThere are 0 directories and 19 images in 'data\\pizza_steak_sushi\\test\\steak'.\nThere are 0 directories and 31 images in 'data\\pizza_steak_sushi\\test\\sushi'.\nThere are 3 directories and 0 images in 'data\\pizza_steak_sushi\\train'.\nThere are 0 directories and 78 images in 'data\\pizza_steak_sushi\\train\\pizza'.\nThere are 0 directories and 75 images in 'data\\pizza_steak_sushi\\train\\steak'.\nThere are 0 directories and 72 images in 'data\\pizza_steak_sushi\\train\\sushi'.\n</pre> <p>Excellent!</p> <p>It looks like we've got about 75 images per training class and 25 images per testing class.</p> <p>That should be enough to get started.</p> <p>Remember, these images are subsets of the original Food101 dataset.</p> <p>You can see how they were created in the data creation notebook.</p> <p>While we're at it, let's setup our training and testing paths.</p> In\u00a0[7]: Copied! <pre># Setup train and testing paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\ntrain_dir, test_dir\n</pre> # Setup train and testing paths train_dir = image_path / \"train\" test_dir = image_path / \"test\"  train_dir, test_dir Out[7]: <pre>(WindowsPath('data/pizza_steak_sushi/train'),\n WindowsPath('data/pizza_steak_sushi/test'))</pre> In\u00a0[8]: Copied! <pre>import random\nfrom PIL import Image\n\n# Set seed\nrandom.seed(42) # &lt;- try changing this and see what happens\n\n# 1. Get all image paths (* means \"any combination\")\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# 2. Get random image path\nrandom_image_path = random.choice(image_path_list)\n\n# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\nimage_class = random_image_path.parent.stem\n\n# 4. Open image\nimg = Image.open(random_image_path)\n\n# 5. Print metadata\nprint(f\"Random image path: {random_image_path}\")\nprint(f\"Image class: {image_class}\")\nprint(f\"Image height: {img.height}\") \nprint(f\"Image width: {img.width}\")\nimg\n</pre> import random from PIL import Image  # Set seed random.seed(42) # &lt;- try changing this and see what happens  # 1. Get all image paths (* means \"any combination\") image_path_list = list(image_path.glob(\"*/*/*.jpg\"))  # 2. Get random image path random_image_path = random.choice(image_path_list)  # 3. Get image class from path name (the image class is the name of the directory where the image is stored) image_class = random_image_path.parent.stem  # 4. Open image img = Image.open(random_image_path)  # 5. Print metadata print(f\"Random image path: {random_image_path}\") print(f\"Image class: {image_class}\") print(f\"Image height: {img.height}\")  print(f\"Image width: {img.width}\") img <pre>Random image path: data\\pizza_steak_sushi\\test\\sushi\\2394442.jpg\nImage class: sushi\nImage height: 408\nImage width: 512\n</pre> Out[8]: <p>We can do the same with <code>matplotlib.pyplot.imshow()</code>, except we have to convert the image to a NumPy array first.</p> In\u00a0[9]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Turn the image into an array\nimg_as_array = np.asarray(img)\n\n# Plot the image with matplotlib\nplt.figure(figsize=(10, 7))\nplt.imshow(img_as_array)\nplt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -&gt; [height, width, color_channels]\")\nplt.axis(False);\n</pre> import numpy as np import matplotlib.pyplot as plt  # Turn the image into an array img_as_array = np.asarray(img)  # Plot the image with matplotlib plt.figure(figsize=(10, 7)) plt.imshow(img_as_array) plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -&gt; [height, width, color_channels]\") plt.axis(False); In\u00a0[10]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n</pre> import torch from torch.utils.data import DataLoader from torchvision import datasets, transforms In\u00a0[11]: Copied! <pre># Write transform for image\ndata_transform = transforms.Compose([\n    # Resize the images to 64x64\n    transforms.Resize(size=(64, 64)),\n    # Flip the images randomly on the horizontal\n    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n    # Turn the image into a torch.Tensor\n    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n])\n</pre> # Write transform for image data_transform = transforms.Compose([     # Resize the images to 64x64     transforms.Resize(size=(64, 64)),     # Flip the images randomly on the horizontal     transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance     # Turn the image into a torch.Tensor     transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0  ]) <p>Now we've got a composition of transforms, let's write a function to try them out on various images.</p> In\u00a0[12]: Copied! <pre>def plot_transformed_images(image_paths, transform, n=3, seed=42):\n\"\"\"Plots a series of random images from image_paths.\n\n    Will open n image paths from image_paths, transform them\n    with transform and plot them side by side.\n\n    Args:\n        image_paths (list): List of target image paths. \n        transform (PyTorch Transforms): Transforms to apply to images.\n        n (int, optional): Number of images to plot. Defaults to 3.\n        seed (int, optional): Random seed for the random generator. Defaults to 42.\n    \"\"\"\n    random.seed(seed)\n    random_image_paths = random.sample(image_paths, k=n)\n    for image_path in random_image_paths:\n        with Image.open(image_path) as f:\n            fig, ax = plt.subplots(1, 2)\n            ax[0].imshow(f) \n            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n            ax[0].axis(\"off\")\n\n            # Transform and plot image\n            # Note: permute() will change shape of image to suit matplotlib \n            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n            transformed_image = transform(f).permute(1, 2, 0) \n            ax[1].imshow(transformed_image) \n            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n            ax[1].axis(\"off\")\n\n            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n\nplot_transformed_images(image_path_list, \n                        transform=data_transform, \n                        n=3)\n</pre> def plot_transformed_images(image_paths, transform, n=3, seed=42):     \"\"\"Plots a series of random images from image_paths.      Will open n image paths from image_paths, transform them     with transform and plot them side by side.      Args:         image_paths (list): List of target image paths.          transform (PyTorch Transforms): Transforms to apply to images.         n (int, optional): Number of images to plot. Defaults to 3.         seed (int, optional): Random seed for the random generator. Defaults to 42.     \"\"\"     random.seed(seed)     random_image_paths = random.sample(image_paths, k=n)     for image_path in random_image_paths:         with Image.open(image_path) as f:             fig, ax = plt.subplots(1, 2)             ax[0].imshow(f)              ax[0].set_title(f\"Original \\nSize: {f.size}\")             ax[0].axis(\"off\")              # Transform and plot image             # Note: permute() will change shape of image to suit matplotlib              # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])             transformed_image = transform(f).permute(1, 2, 0)              ax[1].imshow(transformed_image)              ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")             ax[1].axis(\"off\")              fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)  plot_transformed_images(image_path_list,                          transform=data_transform,                          n=3) <p>Nice!</p> <p>We've now got a way to convert our images to tensors using <code>torchvision.transforms</code>.</p> <p>We also manipulate their size and orientation if needed (some models prefer images of different sizes and shapes).</p> <p>Generally, the larger the shape of the image, the more information a model can recover.</p> <p>For example, an image of size <code>[256, 256, 3]</code> will have 16x more pixels than an image of size <code>[64, 64, 3]</code> (<code>(256*256*3)/(64*64*3)=16</code>).</p> <p>However, the tradeoff is that more pixels requires more computations.</p> <p>Exercise: Try commenting out one of the transforms in <code>data_transform</code> and running the plotting function <code>plot_transformed_images()</code> again, what happens?</p> In\u00a0[13]: Copied! <pre># Use ImageFolder to create dataset(s)\nfrom torchvision import datasets\ntrain_data = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\ntest_data = datasets.ImageFolder(root=test_dir, \n                                 transform=data_transform)\n\nprint(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n</pre> # Use ImageFolder to create dataset(s) from torchvision import datasets train_data = datasets.ImageFolder(root=train_dir, # target folder of images                                   transform=data_transform, # transforms to perform on data (images)                                   target_transform=None) # transforms to perform on labels (if necessary)  test_data = datasets.ImageFolder(root=test_dir,                                   transform=data_transform)  print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\") <pre>Train data:\nDataset ImageFolder\n    Number of datapoints: 225\n    Root location: data\\pizza_steak_sushi\\train\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n           )\nTest data:\nDataset ImageFolder\n    Number of datapoints: 75\n    Root location: data\\pizza_steak_sushi\\test\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n           )\n</pre> <p>Beautiful!</p> <p>It looks like PyTorch has registered our <code>Dataset</code>'s.</p> <p>Let's inspect them by checking out the <code>classes</code> and <code>class_to_idx</code> attributes as well as the lengths of our training and test sets.</p> In\u00a0[14]: Copied! <pre># Get class names as a list\nclass_names = train_data.classes\nclass_names\n</pre> # Get class names as a list class_names = train_data.classes class_names Out[14]: <pre>['pizza', 'steak', 'sushi']</pre> In\u00a0[15]: Copied! <pre># Can also get class names as a dict\nclass_dict = train_data.class_to_idx\nclass_dict\n</pre> # Can also get class names as a dict class_dict = train_data.class_to_idx class_dict Out[15]: <pre>{'pizza': 0, 'steak': 1, 'sushi': 2}</pre> In\u00a0[16]: Copied! <pre># Check the lengths\nlen(train_data), len(test_data)\n</pre> # Check the lengths len(train_data), len(test_data) Out[16]: <pre>(225, 75)</pre> <p>Nice! Looks like we'll be able to use these to reference for later.</p> <p>How about our images and labels?</p> <p>How do they look?</p> <p>We can index on our <code>train_data</code> and <code>test_data</code> <code>Dataset</code>'s to find samples and their target labels.</p> In\u00a0[17]: Copied! <pre>img, label = train_data[0][0], train_data[0][1]\nprint(f\"Image tensor:\\n{img}\")\nprint(f\"Image shape: {img.shape}\")\nprint(f\"Image datatype: {img.dtype}\")\nprint(f\"Image label: {label}\")\nprint(f\"Label datatype: {type(label)}\")\n</pre> img, label = train_data[0][0], train_data[0][1] print(f\"Image tensor:\\n{img}\") print(f\"Image shape: {img.shape}\") print(f\"Image datatype: {img.dtype}\") print(f\"Image label: {label}\") print(f\"Label datatype: {type(label)}\") <pre>Image tensor:\ntensor([[[0.1137, 0.1020, 0.0980,  ..., 0.1255, 0.1216, 0.1176],\n         [0.1059, 0.0980, 0.0980,  ..., 0.1294, 0.1294, 0.1294],\n         [0.1020, 0.0980, 0.0941,  ..., 0.1333, 0.1333, 0.1333],\n         ...,\n         [0.1098, 0.1098, 0.1255,  ..., 0.1686, 0.1647, 0.1686],\n         [0.0863, 0.0941, 0.1098,  ..., 0.1686, 0.1647, 0.1686],\n         [0.0863, 0.0863, 0.0980,  ..., 0.1686, 0.1647, 0.1647]],\n\n        [[0.0745, 0.0706, 0.0745,  ..., 0.0588, 0.0588, 0.0588],\n         [0.0706, 0.0706, 0.0745,  ..., 0.0627, 0.0627, 0.0627],\n         [0.0706, 0.0745, 0.0745,  ..., 0.0706, 0.0706, 0.0706],\n         ...,\n         [0.1255, 0.1333, 0.1373,  ..., 0.2510, 0.2392, 0.2392],\n         [0.1098, 0.1176, 0.1255,  ..., 0.2510, 0.2392, 0.2314],\n         [0.1020, 0.1059, 0.1137,  ..., 0.2431, 0.2353, 0.2275]],\n\n        [[0.0941, 0.0902, 0.0902,  ..., 0.0196, 0.0196, 0.0196],\n         [0.0902, 0.0863, 0.0902,  ..., 0.0196, 0.0157, 0.0196],\n         [0.0902, 0.0902, 0.0902,  ..., 0.0157, 0.0157, 0.0196],\n         ...,\n         [0.1294, 0.1333, 0.1490,  ..., 0.1961, 0.1882, 0.1804],\n         [0.1098, 0.1137, 0.1255,  ..., 0.1922, 0.1843, 0.1804],\n         [0.1059, 0.1020, 0.1059,  ..., 0.1843, 0.1804, 0.1765]]])\nImage shape: torch.Size([3, 64, 64])\nImage datatype: torch.float32\nImage label: 0\nLabel datatype: &lt;class 'int'&gt;\n</pre> <p>Our images are now in the form of a tensor (with shape <code>[3, 64, 64]</code>) and the labels are in the form of an integer relating to a specific class (as referenced by the <code>class_to_idx</code> attribute).</p> <p>How about we plot a single image tensor using <code>matplotlib</code>?</p> <p>We'll first have to to permute (rearrange the order of its dimensions) so it's compatible.</p> <p>Right now our image dimensions are in the format <code>CHW</code> (color channels, height, width) but <code>matplotlib</code> prefers <code>HWC</code> (height, width, color channels).</p> In\u00a0[18]: Copied! <pre># Rearrange the order of dimensions\nimg_permute = img.permute(1, 2, 0)\n\n# Print out different shapes (before and after permute)\nprint(f\"Original shape: {img.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Image permute shape: {img_permute.shape} -&gt; [height, width, color_channels]\")\n\n# Plot the image\nplt.figure(figsize=(10, 7))\nplt.imshow(img.permute(1, 2, 0))\nplt.axis(\"off\")\nplt.title(class_names[label], fontsize=14);\n</pre> # Rearrange the order of dimensions img_permute = img.permute(1, 2, 0)  # Print out different shapes (before and after permute) print(f\"Original shape: {img.shape} -&gt; [color_channels, height, width]\") print(f\"Image permute shape: {img_permute.shape} -&gt; [height, width, color_channels]\")  # Plot the image plt.figure(figsize=(10, 7)) plt.imshow(img.permute(1, 2, 0)) plt.axis(\"off\") plt.title(class_names[label], fontsize=14); <pre>Original shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]\nImage permute shape: torch.Size([64, 64, 3]) -&gt; [height, width, color_channels]\n</pre> <p>Notice the image is now more pixelated (less quality).</p> <p>This is due to it being resized from <code>512x512</code> to <code>64x64</code> pixels.</p> <p>The intuition here is that if you think the image is harder to recognize what's going on, chances are a model will find it harder to understand too.</p> In\u00a0[19]: Copied! <pre># Turn train and test Datasets into DataLoaders\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(dataset=train_data, \n                              batch_size=1, # how many samples per batch?\n                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n                              shuffle=True) # shuffle the data?\n\ntest_dataloader = DataLoader(dataset=test_data, \n                             batch_size=1, \n                             num_workers=1, \n                             shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader, test_dataloader\n</pre> # Turn train and test Datasets into DataLoaders from torch.utils.data import DataLoader train_dataloader = DataLoader(dataset=train_data,                                batch_size=1, # how many samples per batch?                               num_workers=1, # how many subprocesses to use for data loading? (higher = more)                               shuffle=True) # shuffle the data?  test_dataloader = DataLoader(dataset=test_data,                               batch_size=1,                               num_workers=1,                               shuffle=False) # don't usually need to shuffle testing data  train_dataloader, test_dataloader Out[19]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cf94fa0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cf94dc0&gt;)</pre> <p>Wonderful!</p> <p>Now our data is iterable.</p> <p>Let's try it out and check the shapes.</p> In\u00a0[20]: Copied! <pre>img, label = next(iter(train_dataloader))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label.shape}\")\n</pre> img, label = next(iter(train_dataloader))  # Batch size will now be 1, try changing the batch_size parameter above and see what happens print(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Label shape: {label.shape}\") <pre>Image shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n</pre> <p>We could now use these <code>DataLoader</code>'s with a training and testing loop to train a model.</p> <p>But before we do, let's look at another option to load images (or almost any other kind of data).</p> In\u00a0[21]: Copied! <pre>import os\nimport pathlib\nimport torch\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom typing import Tuple, Dict, List\n</pre> import os import pathlib import torch  from PIL import Image from torch.utils.data import Dataset from torchvision import transforms from typing import Tuple, Dict, List <p>Remember how our instances of <code>torchvision.datasets.ImageFolder()</code> allowed us to use the <code>classes</code> and <code>class_to_idx</code> attributes?</p> In\u00a0[22]: Copied! <pre># Instance of torchvision.datasets.ImageFolder()\ntrain_data.classes, train_data.class_to_idx\n</pre> # Instance of torchvision.datasets.ImageFolder() train_data.classes, train_data.class_to_idx Out[22]: <pre>(['pizza', 'steak', 'sushi'], {'pizza': 0, 'steak': 1, 'sushi': 2})</pre> In\u00a0[23]: Copied! <pre># Setup path for target directory\ntarget_directory = train_dir\nprint(f\"Target directory: {target_directory}\")\n\n# Get the class names from the target directory\nclass_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))])\nprint(f\"Class names found: {class_names_found}\")\n</pre> # Setup path for target directory target_directory = train_dir print(f\"Target directory: {target_directory}\")  # Get the class names from the target directory class_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))]) print(f\"Class names found: {class_names_found}\") <pre>Target directory: data\\pizza_steak_sushi\\train\nClass names found: ['pizza', 'steak', 'sushi']\n</pre> <p>Excellent!</p> <p>How about we turn it into a full function?</p> In\u00a0[24]: Copied! <pre># Make function to find classes in target directory\ndef find_classes(directory: str) -&gt; Tuple[List[str], Dict[str, int]]:\n\"\"\"Finds the class folder names in a target directory.\n    Assumes target directory is in standard image classification format.\n\n    Args:\n        directory (str): target directory to load classnames from.\n\n    Returns:\n        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n    Example:\n        find_classes(\"food_images/train\")\n        &gt;&gt;&gt; ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n    \"\"\"\n    # 1. Get the class names by scanning the target directory\n    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n    \n    # 2. Raise an error if class names not found\n    if not classes:\n        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n        \n    # 3. Crearte a dictionary of index labels (computers prefer numerical rather than string labels)\n    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    return classes, class_to_idx\n</pre> # Make function to find classes in target directory def find_classes(directory: str) -&gt; Tuple[List[str], Dict[str, int]]:     \"\"\"Finds the class folder names in a target directory.          Assumes target directory is in standard image classification format.      Args:         directory (str): target directory to load classnames from.      Returns:         Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))          Example:         find_classes(\"food_images/train\")         &gt;&gt;&gt; ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})     \"\"\"     # 1. Get the class names by scanning the target directory     classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())          # 2. Raise an error if class names not found     if not classes:         raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")              # 3. Crearte a dictionary of index labels (computers prefer numerical rather than string labels)     class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}     return classes, class_to_idx <p>Looking good!</p> <p>Now let's test out our <code>find_classes()</code> function.</p> In\u00a0[25]: Copied! <pre>find_classes(train_dir)\n</pre> find_classes(train_dir) Out[25]: <pre>(['pizza', 'steak', 'sushi'], {'pizza': 0, 'steak': 1, 'sushi': 2})</pre> <p>Woohoo! Looking good!</p> In\u00a0[26]: Copied! <pre># Write a custom dataset class (inherits from torch.utils.data.Dataset)\nfrom torch.utils.data import Dataset\n\n# 1. Subclass torch.utils.data.Dataset\nclass ImageFolderCustom(Dataset):\n    \n    # 2. Initialize with a targ_dir and transform (optional) parameter\n    def __init__(self, targ_dir: str, transform=None) -&gt; None:\n        \n        # 3. Create class attributes\n        # Get all image paths\n        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n        # Setup transforms\n        self.transform = transform\n        # Create classes and class_to_idx attributes\n        self.classes, self.class_to_idx = find_classes(targ_dir)\n\n    # 4. Make function to load images\n    def load_image(self, index: int) -&gt; Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.paths[index]\n        return Image.open(image_path) \n    \n    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -&gt; int:\n        \"Returns the total number of samples.\"\n        return len(self.paths)\n    \n    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, int]:\n        \"Returns one sample of data, data and label (X, y).\"\n        img = self.load_image(index)\n        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n        class_idx = self.class_to_idx[class_name]\n\n        # Transform if necessary\n        if self.transform:\n            return self.transform(img), class_idx # return data, label (X, y)\n        else:\n            return img, class_idx # return data, label (X, y)\n</pre> # Write a custom dataset class (inherits from torch.utils.data.Dataset) from torch.utils.data import Dataset  # 1. Subclass torch.utils.data.Dataset class ImageFolderCustom(Dataset):          # 2. Initialize with a targ_dir and transform (optional) parameter     def __init__(self, targ_dir: str, transform=None) -&gt; None:                  # 3. Create class attributes         # Get all image paths         self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's         # Setup transforms         self.transform = transform         # Create classes and class_to_idx attributes         self.classes, self.class_to_idx = find_classes(targ_dir)      # 4. Make function to load images     def load_image(self, index: int) -&gt; Image.Image:         \"Opens an image via a path and returns it.\"         image_path = self.paths[index]         return Image.open(image_path)           # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)     def __len__(self) -&gt; int:         \"Returns the total number of samples.\"         return len(self.paths)          # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)     def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, int]:         \"Returns one sample of data, data and label (X, y).\"         img = self.load_image(index)         class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg         class_idx = self.class_to_idx[class_name]          # Transform if necessary         if self.transform:             return self.transform(img), class_idx # return data, label (X, y)         else:             return img, class_idx # return data, label (X, y) <p>Woah! A whole bunch of code to load in our images.</p> <p>This is one of the downsides of creating your own custom <code>Dataset</code>'s.</p> <p>However, now we've written it once, we could move it into a <code>.py</code> file such as <code>data_loader.py</code> along with some other helpful data functions and reuse it later on.</p> <p>Before we test out our new <code>ImageFolderCustom</code> class, let's create some transforms to prepare our images.</p> In\u00a0[27]: Copied! <pre># Augment train data\ntrain_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])\n\n# Don't augment test data, only reshape\ntest_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n</pre> # Augment train data train_transforms = transforms.Compose([     transforms.Resize((64, 64)),     transforms.RandomHorizontalFlip(p=0.5),     transforms.ToTensor() ])  # Don't augment test data, only reshape test_transforms = transforms.Compose([     transforms.Resize((64, 64)),     transforms.ToTensor() ]) <p>Now comes the moment of truth!</p> <p>Let's turn our training images (contained in <code>train_dir</code>) and our testing images (contained in <code>test_dir</code>) into <code>Dataset</code>'s using our own <code>ImageFolderCustom</code> class.</p> In\u00a0[28]: Copied! <pre>train_data_custom = ImageFolderCustom(targ_dir=train_dir, \n                                      transform=train_transforms)\ntest_data_custom = ImageFolderCustom(targ_dir=test_dir, \n                                     transform=test_transforms)\ntrain_data_custom, test_data_custom\n</pre> train_data_custom = ImageFolderCustom(targ_dir=train_dir,                                        transform=train_transforms) test_data_custom = ImageFolderCustom(targ_dir=test_dir,                                       transform=test_transforms) train_data_custom, test_data_custom Out[28]: <pre>(&lt;__main__.ImageFolderCustom at 0x1fd2cf886d0&gt;,\n &lt;__main__.ImageFolderCustom at 0x1fd2cf5a0d0&gt;)</pre> <p>Hmm... no errors, did it work?</p> <p>Let's try calling <code>len()</code> on our new <code>Dataset</code>'s and find the <code>classes</code> and <code>class_to_idx</code> attributes.</p> In\u00a0[29]: Copied! <pre>len(train_data_custom), len(test_data_custom)\n</pre> len(train_data_custom), len(test_data_custom) Out[29]: <pre>(225, 75)</pre> In\u00a0[30]: Copied! <pre>train_data_custom.classes\n</pre> train_data_custom.classes Out[30]: <pre>['pizza', 'steak', 'sushi']</pre> In\u00a0[31]: Copied! <pre>train_data_custom.class_to_idx\n</pre> train_data_custom.class_to_idx Out[31]: <pre>{'pizza': 0, 'steak': 1, 'sushi': 2}</pre> <p><code>len(test_data_custom) == len(test_data)</code> and <code>len(test_data_custom) == len(test_data)</code> Yes!!!</p> <p>It looks like it worked.</p> <p>We could check for equality with the <code>Dataset</code>'s made by the <code>torchvision.datasets.ImageFolder()</code> class too.</p> In\u00a0[32]: Copied! <pre># Check for equality amongst our custom Dataset and ImageFolder Dataset\nprint((len(train_data_custom) == len(train_data)) &amp; (len(test_data_custom) == len(test_data)))\nprint(train_data_custom.classes == train_data.classes)\nprint(train_data_custom.class_to_idx == train_data.class_to_idx)\n</pre> # Check for equality amongst our custom Dataset and ImageFolder Dataset print((len(train_data_custom) == len(train_data)) &amp; (len(test_data_custom) == len(test_data))) print(train_data_custom.classes == train_data.classes) print(train_data_custom.class_to_idx == train_data.class_to_idx) <pre>True\nTrue\nTrue\n</pre> <p>Ho ho!</p> <p>Look at us go!</p> <p>Three <code>True</code>'s!</p> <p>You can't get much better than that.</p> <p>How about we take it up a notch and plot some random images to test our <code>__getitem__</code> override?</p> In\u00a0[33]: Copied! <pre># 1. Take in a Dataset as well as a list of class names\ndef display_random_images(dataset: torch.utils.data.dataset.Dataset,\n                          classes: List[str] = None,\n                          n: int = 10,\n                          display_shape: bool = True,\n                          seed: int = None):\n    \n    # 2. Adjust display if n too high\n    if n &gt; 10:\n        n = 10\n        display_shape = False\n        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n    \n    # 3. Set random seed\n    if seed:\n        random.seed(seed)\n\n    # 4. Get random sample indexes\n    random_samples_idx = random.sample(range(len(dataset)), k=n)\n\n    # 5. Setup plot\n    plt.figure(figsize=(16, 8))\n\n    # 6. Loop through samples and display random samples \n    for i, targ_sample in enumerate(random_samples_idx):\n        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n\n        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]\n        targ_image_adjust = targ_image.permute(1, 2, 0)\n\n        # Plot adjusted samples\n        plt.subplot(1, n, i+1)\n        plt.imshow(targ_image_adjust)\n        plt.axis(\"off\")\n        if classes:\n            title = f\"class: {classes[targ_label]}\"\n            if display_shape:\n                title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n        plt.title(title)\n</pre> # 1. Take in a Dataset as well as a list of class names def display_random_images(dataset: torch.utils.data.dataset.Dataset,                           classes: List[str] = None,                           n: int = 10,                           display_shape: bool = True,                           seed: int = None):          # 2. Adjust display if n too high     if n &gt; 10:         n = 10         display_shape = False         print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")          # 3. Set random seed     if seed:         random.seed(seed)      # 4. Get random sample indexes     random_samples_idx = random.sample(range(len(dataset)), k=n)      # 5. Setup plot     plt.figure(figsize=(16, 8))      # 6. Loop through samples and display random samples      for i, targ_sample in enumerate(random_samples_idx):         targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]          # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]         targ_image_adjust = targ_image.permute(1, 2, 0)          # Plot adjusted samples         plt.subplot(1, n, i+1)         plt.imshow(targ_image_adjust)         plt.axis(\"off\")         if classes:             title = f\"class: {classes[targ_label]}\"             if display_shape:                 title = title + f\"\\nshape: {targ_image_adjust.shape}\"         plt.title(title) <p>What a good looking function!</p> <p>Let's test it out first with the <code>Dataset</code> we created with <code>torchvision.datasets.ImageFolder()</code>.</p> In\u00a0[34]: Copied! <pre># Display random images from ImageFolder created Dataset\ndisplay_random_images(train_data, \n                      n=5, \n                      classes=class_names,\n                      seed=None)\n</pre> # Display random images from ImageFolder created Dataset display_random_images(train_data,                        n=5,                        classes=class_names,                       seed=None) <p>And now with the <code>Dataset</code> we created with our own <code>ImageFolderCustom</code>.</p> In\u00a0[35]: Copied! <pre># Display random images from ImageFolderCustom Dataset\ndisplay_random_images(train_data_custom, \n                      n=12, \n                      classes=class_names,\n                      seed=None) # Try setting the seed for reproducible images\n</pre> # Display random images from ImageFolderCustom Dataset display_random_images(train_data_custom,                        n=12,                        classes=class_names,                       seed=None) # Try setting the seed for reproducible images <pre>For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\n</pre> <p>Nice!!!</p> <p>Looks like our <code>ImageFolderCustom</code> is working just as we'd like it to.</p> In\u00a0[39]: Copied! <pre># Turn train and test custom Dataset's into DataLoader's\nfrom torch.utils.data import DataLoader\ntrain_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset\n                                     batch_size=1, # how many samples per batch?\n                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n                                     shuffle=True) # shuffle the data?\n\ntest_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset\n                                    batch_size=1, \n                                    num_workers=0, \n                                    shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader_custom, test_dataloader_custom\n</pre> # Turn train and test custom Dataset's into DataLoader's from torch.utils.data import DataLoader train_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset                                      batch_size=1, # how many samples per batch?                                      num_workers=0, # how many subprocesses to use for data loading? (higher = more)                                      shuffle=True) # shuffle the data?  test_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset                                     batch_size=1,                                      num_workers=0,                                      shuffle=False) # don't usually need to shuffle testing data  train_dataloader_custom, test_dataloader_custom Out[39]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cfabf10&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cfabb80&gt;)</pre> <p>Do the shapes of the samples look the same?</p> In\u00a0[40]: Copied! <pre># Get image and label from custom DataLoader\nimg_custom, label_custom = next(iter(train_dataloader_custom))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img_custom.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label_custom.shape}\")\n</pre> # Get image and label from custom DataLoader img_custom, label_custom = next(iter(train_dataloader_custom))  # Batch size will now be 1, try changing the batch_size parameter above and see what happens print(f\"Image shape: {img_custom.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Label shape: {label_custom.shape}\") <pre>Image shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n</pre> <p>They sure do!</p> <p>Let's now take a lot at some other forms of data transforms.</p> In\u00a0[41]: Copied! <pre>from torchvision import transforms\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense \n    transforms.ToTensor() # use ToTensor() last to get everything between 0 &amp; 1\n])\n\n# Don't need to perform augmentation on the test data\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), \n    transforms.ToTensor()\n])\n</pre> from torchvision import transforms  train_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense      transforms.ToTensor() # use ToTensor() last to get everything between 0 &amp; 1 ])  # Don't need to perform augmentation on the test data test_transforms = transforms.Compose([     transforms.Resize((224, 224)),      transforms.ToTensor() ]) <p>Note: You usually don't perform data augmentation on the test set. The idea of data augmentation is to to artificially increase the diversity of the training set to better predict on the testing set.</p> <p>However, you do need to make sure your test set images are transformed to tensors. We size the test images to the same size as our training images too, however, inference can be done on different size images if necessary (though this may alter performance).</p> <p>Beautiful, now we've got a training transform (with data augmentation) and test transform (without data augmentation).</p> <p>Let's test our data augmentation out!</p> In\u00a0[42]: Copied! <pre># Get all image paths\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# Plot random images\nplot_transformed_images(\n    image_paths=image_path_list,\n    transform=train_transforms,\n    n=3,\n    seed=None\n)\n</pre> # Get all image paths image_path_list = list(image_path.glob(\"*/*/*.jpg\"))  # Plot random images plot_transformed_images(     image_paths=image_path_list,     transform=train_transforms,     n=3,     seed=None ) <p>Try running the cell above a few times and seeing how the original image changes as it goes through the transform.</p> In\u00a0[43]: Copied! <pre># Create simple transform\nsimple_transform = transforms.Compose([ \n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n])\n</pre> # Create simple transform simple_transform = transforms.Compose([      transforms.Resize((64, 64)),     transforms.ToTensor(), ]) <p>Excellent, now we've got a simple transform, let's:</p> <ol> <li>Load the data, turning each of our training and test folders first into a <code>Dataset</code> with <code>torchvision.datasets.ImageFolder()</code></li> <li>Then into a <code>DataLoader</code> using <code>torch.utils.data.DataLoader()</code>.<ul> <li>We'll set the <code>batch_size=32</code> and <code>num_workers</code> to as many CPUs on our machine (this will depend on what machine you're using).</li> </ul> </li> </ol> In\u00a0[44]: Copied! <pre># 1. Load and transform data\nfrom torchvision import datasets\ntrain_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)\ntest_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)\n\n# 2. Turn data into DataLoaders\nimport os\nfrom torch.utils.data import DataLoader\n\n# Setup batch size and number of workers \nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\nprint(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n\n# Create DataLoader's\ntrain_dataloader_simple = DataLoader(train_data_simple, \n                                     batch_size=BATCH_SIZE, \n                                     shuffle=True, \n                                     num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_simple, test_dataloader_simple\n</pre> # 1. Load and transform data from torchvision import datasets train_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform) test_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)  # 2. Turn data into DataLoaders import os from torch.utils.data import DataLoader  # Setup batch size and number of workers  BATCH_SIZE = 32 NUM_WORKERS = os.cpu_count() print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")  # Create DataLoader's train_dataloader_simple = DataLoader(train_data_simple,                                       batch_size=BATCH_SIZE,                                       shuffle=True,                                       num_workers=NUM_WORKERS)  test_dataloader_simple = DataLoader(test_data_simple,                                      batch_size=BATCH_SIZE,                                      shuffle=False,                                      num_workers=NUM_WORKERS)  train_dataloader_simple, test_dataloader_simple <pre>Creating DataLoader's with batch size 32 and 16 workers.\n</pre> Out[44]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x1fd2ce5c4f0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x1fd1d0e10d0&gt;)</pre> <p><code>DataLoader</code>'s created!</p> <p>Let's build a model.</p> In\u00a0[45]: Copied! <pre>class TinyVGG(nn.Module):\n\"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*16*16,\n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.conv_block_1(x)\n        # print(x.shape)\n        x = self.conv_block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion\n\ntorch.manual_seed(42)\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\nmodel_0\n</pre> class TinyVGG(nn.Module):     \"\"\"     Model architecture copying TinyVGG from:      https://poloclub.github.io/cnn-explainer/     \"\"\"     def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:         super().__init__()         self.conv_block_1 = nn.Sequential(             nn.Conv2d(in_channels=input_shape,                        out_channels=hidden_units,                        kernel_size=3, # how big is the square that's going over the image?                       stride=1, # default                       padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number              nn.ReLU(),             nn.Conv2d(in_channels=hidden_units,                        out_channels=hidden_units,                       kernel_size=3,                       stride=1,                       padding=1),             nn.ReLU(),             nn.MaxPool2d(kernel_size=2,                          stride=2) # default stride value is same as kernel_size         )         self.conv_block_2 = nn.Sequential(             nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),             nn.ReLU(),             nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),             nn.ReLU(),             nn.MaxPool2d(2)         )         self.classifier = nn.Sequential(             nn.Flatten(),             # Where did this in_features shape come from?              # It's because each layer of our network compresses and changes the shape of our inputs data.             nn.Linear(in_features=hidden_units*16*16,                       out_features=output_shape)         )          def forward(self, x: torch.Tensor):         x = self.conv_block_1(x)         # print(x.shape)         x = self.conv_block_2(x)         # print(x.shape)         x = self.classifier(x)         # print(x.shape)         return x         # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion  torch.manual_seed(42) model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)                    hidden_units=10,                    output_shape=len(train_data.classes)).to(device) model_0 Out[45]: <pre>TinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=3, bias=True)\n  )\n)</pre> <p>Note: One of the ways to speed up deep learning models computing on a GPU is to leverage operator fusion.</p> <p>This means in the <code>forward()</code> method in our model above, instead of calling a layer block and reassigning <code>x</code> every time, we call each block in succession (see the final line of the <code>forward()</code> method in the model above for an example).</p> <p>This saves the time spent reassigning <code>x</code> (memory heavy) and focuses on only computing on <code>x</code>.</p> <p>See Making Deep Learning Go Brrrr From First Principles by Horace He for more ways on how to speed up machine learning models.</p> <p>Now that's a nice looking model!</p> <p>How about we test it out with a forward pass on a single image?</p> In\u00a0[46]: Copied! <pre># 1. Get a batch of images and labels from the DataLoader\nimg_batch, label_batch = next(iter(train_dataloader_simple))\n\n# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\nimg_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\nprint(f\"Single image shape: {img_single.shape}\\n\")\n\n# 3. Perform a forward pass on a single image\nmodel_0.eval()\nwith torch.inference_mode():\n    pred = model_0(img_single.to(device))\n    \n# 4. Print out what's happening and convert model logits -&gt; pred probs -&gt; pred label\nprint(f\"Output logits:\\n{pred}\\n\")\nprint(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\nprint(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\nprint(f\"Actual label:\\n{label_single}\")\n</pre> # 1. Get a batch of images and labels from the DataLoader img_batch, label_batch = next(iter(train_dataloader_simple))  # 2. Get a single image from the batch and unsqueeze the image so its shape fits the model img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0] print(f\"Single image shape: {img_single.shape}\\n\")  # 3. Perform a forward pass on a single image model_0.eval() with torch.inference_mode():     pred = model_0(img_single.to(device))      # 4. Print out what's happening and convert model logits -&gt; pred probs -&gt; pred label print(f\"Output logits:\\n{pred}\\n\") print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\") print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\") print(f\"Actual label:\\n{label_single}\") <pre>Single image shape: torch.Size([1, 3, 64, 64])\n\nOutput logits:\ntensor([[0.0578, 0.0635, 0.0352]], device='cuda:0')\n\nOutput prediction probabilities:\ntensor([[0.3352, 0.3371, 0.3277]], device='cuda:0')\n\nOutput prediction label:\ntensor([1], device='cuda:0')\n\nActual label:\n2\n</pre> <p>Wonderful, it looks like our model is outputting what we'd expect it to output.</p> <p>You can run the cell above a few times and each time have a different image be predicted on.</p> <p>And you'll probably notice the predictions are often wrong.</p> <p>This is to be expected because the model hasn't been trained yet and it's essentially guessing using random weights.</p> In\u00a0[47]: Copied! <pre># Install torchinfo if it's not available, import it if it is\ntry: \n    import torchinfo\nexcept:\n    !pip install torchinfo\n    import torchinfo\n    \nfrom torchinfo import summary\nsummary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size\n</pre> # Install torchinfo if it's not available, import it if it is try:      import torchinfo except:     !pip install torchinfo     import torchinfo      from torchinfo import summary summary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size  <pre>Collecting torchinfo\n  Downloading torchinfo-1.6.5-py3-none-any.whl (21 kB)\nInstalling collected packages: torchinfo\nSuccessfully installed torchinfo-1.6.5\n</pre> Out[47]: <pre>==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nTinyVGG                                  --                        --\n\u251c\u2500Sequential: 1-1                        [1, 10, 32, 32]           --\n\u2502    \u2514\u2500Conv2d: 2-1                       [1, 10, 64, 64]           280\n\u2502    \u2514\u2500ReLU: 2-2                         [1, 10, 64, 64]           --\n\u2502    \u2514\u2500Conv2d: 2-3                       [1, 10, 64, 64]           910\n\u2502    \u2514\u2500ReLU: 2-4                         [1, 10, 64, 64]           --\n\u2502    \u2514\u2500MaxPool2d: 2-5                    [1, 10, 32, 32]           --\n\u251c\u2500Sequential: 1-2                        [1, 10, 16, 16]           --\n\u2502    \u2514\u2500Conv2d: 2-6                       [1, 10, 32, 32]           910\n\u2502    \u2514\u2500ReLU: 2-7                         [1, 10, 32, 32]           --\n\u2502    \u2514\u2500Conv2d: 2-8                       [1, 10, 32, 32]           910\n\u2502    \u2514\u2500ReLU: 2-9                         [1, 10, 32, 32]           --\n\u2502    \u2514\u2500MaxPool2d: 2-10                   [1, 10, 16, 16]           --\n\u251c\u2500Sequential: 1-3                        [1, 3]                    --\n\u2502    \u2514\u2500Flatten: 2-11                     [1, 2560]                 --\n\u2502    \u2514\u2500Linear: 2-12                      [1, 3]                    7,683\n==========================================================================================\nTotal params: 10,693\nTrainable params: 10,693\nNon-trainable params: 0\nTotal mult-adds (M): 6.75\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 0.82\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.91\n==========================================================================================</pre> <p>Nice!</p> <p>The output of <code>torchinfo.summary()</code> gives us a whole bunch of information about our model.</p> <p>Such as <code>Total params</code>, the total number of parameters in our model, the <code>Estimated Total Size (MB)</code> which is the size of our model.</p> <p>You can also see the change in input and output shapes as data of a certain <code>input_size</code> moves through our model.</p> <p>Right now, our parameter numbers and total model size is low.</p> <p>This because we're starting with a small model.</p> <p>And if we need to increase its size later, we can.</p> In\u00a0[48]: Copied! <pre>def train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer):\n    # Put model in train mode\n    model.train()\n    \n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n    \n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n</pre> def train_step(model: torch.nn.Module,                 dataloader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 optimizer: torch.optim.Optimizer):     # Put model in train mode     model.train()          # Setup train loss and train accuracy values     train_loss, train_acc = 0, 0          # Loop through data loader data batches     for batch, (X, y) in enumerate(dataloader):         # Send data to target device         X, y = X.to(device), y.to(device)          # 1. Forward pass         y_pred = model(X)          # 2. Calculate  and accumulate loss         loss = loss_fn(y_pred, y)         train_loss += loss.item()           # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()          # Calculate and accumulate accuracy metric across all batches         y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)         train_acc += (y_pred_class == y).sum().item()/len(y_pred)      # Adjust metrics to get average loss and accuracy per batch      train_loss = train_loss / len(dataloader)     train_acc = train_acc / len(dataloader)     return train_loss, train_acc <p>Woohoo! <code>train_step()</code> function done.</p> <p>Now let's do the same for the <code>test_step()</code> function.</p> <p>The main difference here will be the <code>test_step()</code> won't take in an optimizer and therefore won't perform gradient descent.</p> <p>But since we'll be doing inference, we'll make sure to turn on the <code>torch.inference_mode()</code> context manager for making predictions.</p> In\u00a0[49]: Copied! <pre>def test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module):\n    # Put model in eval mode\n    model.eval() \n    \n    # Setup test loss and test accuracy values\n    test_loss, test_acc = 0, 0\n    \n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n    \n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n            \n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n            \n    # Adjust metrics to get average loss and accuracy per batch \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc\n</pre> def test_step(model: torch.nn.Module,                dataloader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module):     # Put model in eval mode     model.eval()           # Setup test loss and test accuracy values     test_loss, test_acc = 0, 0          # Turn on inference context manager     with torch.inference_mode():         # Loop through DataLoader batches         for batch, (X, y) in enumerate(dataloader):             # Send data to target device             X, y = X.to(device), y.to(device)                  # 1. Forward pass             test_pred_logits = model(X)              # 2. Calculate and accumulate loss             loss = loss_fn(test_pred_logits, y)             test_loss += loss.item()                          # Calculate and accumulate accuracy             test_pred_labels = test_pred_logits.argmax(dim=1)             test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))                  # Adjust metrics to get average loss and accuracy per batch      test_loss = test_loss / len(dataloader)     test_acc = test_acc / len(dataloader)     return test_loss, test_acc <p>Excellent!</p> In\u00a0[50]: Copied! <pre>from tqdm.auto import tqdm\n\n# 1. Take in various parameters required for training and test steps\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n          epochs: int = 5):\n    \n    # 2. Create empty results dictionary\n    results = {\"train_loss\": [],\n        \"train_acc\": [],\n        \"test_loss\": [],\n        \"test_acc\": []\n    }\n    \n    # 3. Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer)\n        test_loss, test_acc = test_step(model=model,\n            dataloader=test_dataloader,\n            loss_fn=loss_fn)\n        \n        # 4. Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # 5. Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n    # 6. Return the filled results at the end of the epochs\n    return results\n</pre> from tqdm.auto import tqdm  # 1. Take in various parameters required for training and test steps def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),           epochs: int = 5):          # 2. Create empty results dictionary     results = {\"train_loss\": [],         \"train_acc\": [],         \"test_loss\": [],         \"test_acc\": []     }          # 3. Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                            dataloader=train_dataloader,                                            loss_fn=loss_fn,                                            optimizer=optimizer)         test_loss, test_acc = test_step(model=model,             dataloader=test_dataloader,             loss_fn=loss_fn)                  # 4. Print out what's happening         print(             f\"Epoch: {epoch+1} | \"             f\"train_loss: {train_loss:.4f} | \"             f\"train_acc: {train_acc:.4f} | \"             f\"test_loss: {test_loss:.4f} | \"             f\"test_acc: {test_acc:.4f}\"         )          # 5. Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)      # 6. Return the filled results at the end of the epochs     return results In\u00a0[51]: Copied! <pre># Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Recreate an instance of TinyVGG\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_0 \nmodel_0_results = train(model=model_0, \n                        train_dataloader=train_dataloader_simple,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Set random seeds torch.manual_seed(42)  torch.cuda.manual_seed(42)  # Set number of epochs NUM_EPOCHS = 5  # Recreate an instance of TinyVGG model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)                    hidden_units=10,                    output_shape=len(train_data.classes)).to(device)  # Setup loss function and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)  # Start the timer from timeit import default_timer as timer  start_time = timer()  # Train model_0  model_0_results = train(model=model_0,                          train_dataloader=train_dataloader_simple,                         test_dataloader=test_dataloader_simple,                         optimizer=optimizer,                         loss_fn=loss_fn,                          epochs=NUM_EPOCHS)  # End the timer and print out how long it took end_time = timer() print(f\"Total training time: {end_time-start_time:.3f} seconds\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.1078 | train_acc: 0.2578 | test_loss: 1.1360 | test_acc: 0.2604\nEpoch: 2 | train_loss: 1.0847 | train_acc: 0.4258 | test_loss: 1.1620 | test_acc: 0.1979\nEpoch: 3 | train_loss: 1.1157 | train_acc: 0.2930 | test_loss: 1.1695 | test_acc: 0.1979\nEpoch: 4 | train_loss: 1.0955 | train_acc: 0.4141 | test_loss: 1.1380 | test_acc: 0.1979\nEpoch: 5 | train_loss: 1.0985 | train_acc: 0.2930 | test_loss: 1.1420 | test_acc: 0.1979\nTotal training time: 65.122 seconds\n</pre> <p>Hmm...</p> <p>It looks like our model performed pretty poorly.</p> <p>But that's okay for now, we'll keep persevering.</p> <p>What are some ways you could potentially improve it?</p> <p>Note: Check out the Improving a model (from a model perspective) section in notebook 02 for ideas on improving our TinyVGG model.</p> In\u00a0[52]: Copied! <pre># Check the model_0_results keys\nmodel_0_results.keys()\n</pre> # Check the model_0_results keys model_0_results.keys() Out[52]: <pre>dict_keys(['train_loss', 'train_acc', 'test_loss', 'test_acc'])</pre> <p>We'll need to extract each of these keys and turn them into a plot.</p> In\u00a0[53]: Copied! <pre>def plot_loss_curves(results: Dict[str, List[float]]):\n\"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n    \n    # Get the loss values of the results dictionary (training and test)\n    loss = results['train_loss']\n    test_loss = results['test_loss']\n\n    # Get the accuracy values of the results dictionary (training and test)\n    accuracy = results['train_acc']\n    test_accuracy = results['test_acc']\n\n    # Figure out how many epochs there were\n    epochs = range(len(results['train_loss']))\n\n    # Setup a plot \n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label='train_loss')\n    plt.plot(epochs, test_loss, label='test_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label='train_accuracy')\n    plt.plot(epochs, test_accuracy, label='test_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();\n</pre> def plot_loss_curves(results: Dict[str, List[float]]):     \"\"\"Plots training curves of a results dictionary.      Args:         results (dict): dictionary containing list of values, e.g.             {\"train_loss\": [...],              \"train_acc\": [...],              \"test_loss\": [...],              \"test_acc\": [...]}     \"\"\"          # Get the loss values of the results dictionary (training and test)     loss = results['train_loss']     test_loss = results['test_loss']      # Get the accuracy values of the results dictionary (training and test)     accuracy = results['train_acc']     test_accuracy = results['test_acc']      # Figure out how many epochs there were     epochs = range(len(results['train_loss']))      # Setup a plot      plt.figure(figsize=(15, 7))      # Plot loss     plt.subplot(1, 2, 1)     plt.plot(epochs, loss, label='train_loss')     plt.plot(epochs, test_loss, label='test_loss')     plt.title('Loss')     plt.xlabel('Epochs')     plt.legend()      # Plot accuracy     plt.subplot(1, 2, 2)     plt.plot(epochs, accuracy, label='train_accuracy')     plt.plot(epochs, test_accuracy, label='test_accuracy')     plt.title('Accuracy')     plt.xlabel('Epochs')     plt.legend(); <p>Okay, let's test our <code>plot_loss_curves()</code> function out.</p> In\u00a0[54]: Copied! <pre>plot_loss_curves(model_0_results)\n</pre> plot_loss_curves(model_0_results) <p>Woah.</p> <p>Looks like things are all over the place...</p> <p>But we kind of knew that because our model's print out results during training didn't show much promise.</p> <p>You could try training the model for longer and see what happens when you plot a loss curve over a longer time horizon.</p> In\u00a0[55]: Copied! <pre># Create training transform with TrivialAugment\ntrain_transform_trivial_augment = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n    transforms.ToTensor() \n])\n\n# Create testing transform (no data augmentation)\ntest_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n</pre> # Create training transform with TrivialAugment train_transform_trivial_augment = transforms.Compose([     transforms.Resize((64, 64)),     transforms.TrivialAugmentWide(num_magnitude_bins=31),     transforms.ToTensor()  ])  # Create testing transform (no data augmentation) test_transform = transforms.Compose([     transforms.Resize((64, 64)),     transforms.ToTensor() ]) <p>Wonderful!</p> <p>Now let's turn our images into <code>Dataset</code>'s using <code>torchvision.datasets.ImageFolder()</code> and then into <code>DataLoader</code>'s with <code>torch.utils.data.DataLoader()</code>.</p> In\u00a0[56]: Copied! <pre># Turn image folders into Datasets\ntrain_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment)\ntest_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)\n\ntrain_data_augmented, test_data_simple\n</pre> # Turn image folders into Datasets train_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment) test_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)  train_data_augmented, test_data_simple Out[56]: <pre>(Dataset ImageFolder\n     Number of datapoints: 225\n     Root location: data\\pizza_steak_sushi\\train\n     StandardTransform\n Transform: Compose(\n                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n                TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n                ToTensor()\n            ),\n Dataset ImageFolder\n     Number of datapoints: 75\n     Root location: data\\pizza_steak_sushi\\test\n     StandardTransform\n Transform: Compose(\n                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n                ToTensor()\n            ))</pre> <p>And we'll make <code>DataLoader</code>'s with a <code>batch_size=32</code> and with <code>num_workers</code> set to the number of CPUs available on our machine (we can get this using Python's <code>os.cpu_count()</code>).</p> In\u00a0[57]: Copied! <pre># Turn Datasets into DataLoader's\nimport os\nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\n\ntorch.manual_seed(42)\ntrain_dataloader_augmented = DataLoader(train_data_augmented, \n                                        batch_size=BATCH_SIZE, \n                                        shuffle=True,\n                                        num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_augmented, test_dataloader\n</pre> # Turn Datasets into DataLoader's import os BATCH_SIZE = 32 NUM_WORKERS = os.cpu_count()  torch.manual_seed(42) train_dataloader_augmented = DataLoader(train_data_augmented,                                          batch_size=BATCH_SIZE,                                          shuffle=True,                                         num_workers=NUM_WORKERS)  test_dataloader_simple = DataLoader(test_data_simple,                                      batch_size=BATCH_SIZE,                                      shuffle=False,                                      num_workers=NUM_WORKERS)  train_dataloader_augmented, test_dataloader Out[57]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x1fd2d0531c0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cf94dc0&gt;)</pre> In\u00a0[58]: Copied! <pre># Create model_1 and send it to the target device\ntorch.manual_seed(42)\nmodel_1 = TinyVGG(\n    input_shape=3,\n    hidden_units=10,\n    output_shape=len(train_data_augmented.classes)).to(device)\nmodel_1\n</pre> # Create model_1 and send it to the target device torch.manual_seed(42) model_1 = TinyVGG(     input_shape=3,     hidden_units=10,     output_shape=len(train_data_augmented.classes)).to(device) model_1 Out[58]: <pre>TinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=3, bias=True)\n  )\n)</pre> <p>Model ready!</p> <p>Time to train!</p> <p>Since we've already got functions for the training loop (<code>train_step()</code>) and testing loop (<code>test_step()</code>) and a function to put them together in <code>train()</code>, let's reuse those.</p> <p>We'll use the same setup as <code>model_0</code> with only the <code>train_dataloader</code> parameter varying:</p> <ul> <li>Train for 5 epochs.</li> <li>Use <code>train_dataloader=train_dataloader_augmented</code> as the training data in <code>train()</code>.</li> <li>Use <code>torch.nn.CrossEntropyLoss()</code> as the loss function (since we're working with multi-class classification).</li> <li>Use <code>torch.optim.Adam()</code> with <code>lr=0.001</code> as the learning rate as the optimizer.</li> </ul> In\u00a0[59]: Copied! <pre># Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_1\nmodel_1_results = train(model=model_1, \n                        train_dataloader=train_dataloader_augmented,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Set random seeds torch.manual_seed(42)  torch.cuda.manual_seed(42)  # Set number of epochs NUM_EPOCHS = 5  # Setup loss function and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)  # Start the timer from timeit import default_timer as timer  start_time = timer()  # Train model_1 model_1_results = train(model=model_1,                          train_dataloader=train_dataloader_augmented,                         test_dataloader=test_dataloader_simple,                         optimizer=optimizer,                         loss_fn=loss_fn,                          epochs=NUM_EPOCHS)  # End the timer and print out how long it took end_time = timer() print(f\"Total training time: {end_time-start_time:.3f} seconds\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.1074 | train_acc: 0.2461 | test_loss: 1.1058 | test_acc: 0.2604\nEpoch: 2 | train_loss: 1.0791 | train_acc: 0.4258 | test_loss: 1.1382 | test_acc: 0.2604\nEpoch: 3 | train_loss: 1.0804 | train_acc: 0.4258 | test_loss: 1.1683 | test_acc: 0.2604\nEpoch: 4 | train_loss: 1.1287 | train_acc: 0.3047 | test_loss: 1.1626 | test_acc: 0.2604\nEpoch: 5 | train_loss: 1.0884 | train_acc: 0.4258 | test_loss: 1.1481 | test_acc: 0.2604\nTotal training time: 67.543 seconds\n</pre> <p>Hmm...</p> <p>It doesn't look like our model performed very well again.</p> <p>Let's check out its loss curves.</p> In\u00a0[60]: Copied! <pre>plot_loss_curves(model_1_results)\n</pre> plot_loss_curves(model_1_results) <p>Wow...</p> <p>These don't look very good either...</p> <p>Is our model underfitting or overfitting?</p> <p>Or both?</p> <p>Ideally we'd like it have higher accuracy and lower loss right?</p> <p>What are some methods you could try to use to achieve these?</p> In\u00a0[61]: Copied! <pre>import pandas as pd\nmodel_0_df = pd.DataFrame(model_0_results)\nmodel_1_df = pd.DataFrame(model_1_results)\nmodel_0_df\n</pre> import pandas as pd model_0_df = pd.DataFrame(model_0_results) model_1_df = pd.DataFrame(model_1_results) model_0_df Out[61]: train_loss train_acc test_loss test_acc 0 1.107832 0.257812 1.136025 0.260417 1 1.084726 0.425781 1.161953 0.197917 2 1.115656 0.292969 1.169479 0.197917 3 1.095543 0.414062 1.137993 0.197917 4 1.098464 0.292969 1.142002 0.197917 <p>And now we can write some plotting code using <code>matplotlib</code> to visualize the results of <code>model_0</code> and <code>model_1</code> together.</p> In\u00a0[62]: Copied! <pre># Setup a plot \nplt.figure(figsize=(15, 10))\n\n# Get number of epochs\nepochs = range(len(model_0_df))\n\n# Plot train loss\nplt.subplot(2, 2, 1)\nplt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\nplt.title(\"Train Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot test loss\nplt.subplot(2, 2, 2)\nplt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\nplt.title(\"Test Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot train accuracy\nplt.subplot(2, 2, 3)\nplt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\nplt.title(\"Train Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot test accuracy\nplt.subplot(2, 2, 4)\nplt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\nplt.title(\"Test Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n</pre> # Setup a plot  plt.figure(figsize=(15, 10))  # Get number of epochs epochs = range(len(model_0_df))  # Plot train loss plt.subplot(2, 2, 1) plt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\") plt.title(\"Train Loss\") plt.xlabel(\"Epochs\") plt.legend()  # Plot test loss plt.subplot(2, 2, 2) plt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\") plt.title(\"Test Loss\") plt.xlabel(\"Epochs\") plt.legend()  # Plot train accuracy plt.subplot(2, 2, 3) plt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\") plt.title(\"Train Accuracy\") plt.xlabel(\"Epochs\") plt.legend()  # Plot test accuracy plt.subplot(2, 2, 4) plt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\") plt.title(\"Test Accuracy\") plt.xlabel(\"Epochs\") plt.legend(); <p>It looks like our models both performed equally poorly and were kind of sporadic (the metrics go up and down sharply).</p> <p>If you built <code>model_2</code>, what would you do differently to try and improve performance?</p> In\u00a0[63]: Copied! <pre># Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n</pre> # Download custom image import requests  # Setup custom image path custom_image_path = data_path / \"04-pizza-dad.jpeg\"  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\") <pre>data\\04-pizza-dad.jpeg already exists, skipping download.\n</pre> In\u00a0[64]: Copied! <pre>import torchvision\n\n# Read in custom image\ncustom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n\n# Print out image data\nprint(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\nprint(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n</pre> import torchvision  # Read in custom image custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))  # Print out image data print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\") print(f\"Custom image shape: {custom_image_uint8.shape}\\n\") print(f\"Custom image dtype: {custom_image_uint8.dtype}\") <pre>Custom image tensor:\ntensor([[[154, 175, 181,  ...,  21,  18,  14],\n         [146, 167, 180,  ...,  21,  18,  15],\n         [124, 146, 171,  ...,  18,  17,  15],\n         ...,\n         [ 72,  59,  45,  ..., 152, 150, 148],\n         [ 64,  55,  41,  ..., 150, 147, 144],\n         [ 64,  60,  46,  ..., 149, 146, 143]],\n\n        [[171, 189, 193,  ...,  22,  19,  15],\n         [163, 181, 194,  ...,  22,  19,  16],\n         [141, 163, 185,  ...,  19,  18,  16],\n         ...,\n         [ 55,  42,  28,  ..., 106, 104, 102],\n         [ 47,  38,  24,  ..., 108, 105, 102],\n         [ 47,  43,  29,  ..., 107, 104, 101]],\n\n        [[117, 138, 145,  ...,  17,  14,  10],\n         [109, 130, 145,  ...,  17,  14,  11],\n         [ 87, 111, 136,  ...,  14,  13,  11],\n         ...,\n         [ 35,  22,   8,  ...,  54,  52,  50],\n         [ 27,  18,   4,  ...,  50,  47,  44],\n         [ 27,  23,   9,  ...,  49,  46,  43]]], dtype=torch.uint8)\n\nCustom image shape: torch.Size([3, 4032, 3024])\n\nCustom image dtype: torch.uint8\n</pre> <p>Nice! Looks like our image is in tensor format, however, is this image format compatible with our model?</p> <p>Our <code>custom_image</code> tensor is of datatype <code>torch.uint8</code> and its values are between <code>[0, 255]</code>.</p> <p>But our model takes image tensors of datatype <code>torch.float32</code> and with values between <code>[0, 1]</code>.</p> <p>So before we use our custom image with our model, we'll need to convert it to the same format as the data our model is trained on.</p> <p>If we don't do this, our model will error.</p> In\u00a0[65]: Copied! <pre># Try to make a prediction on image in uint8 format (this will error)\nmodel_1.eval()\nwith torch.inference_mode():\n    model_1(custom_image_uint8.to(device))\n</pre> # Try to make a prediction on image in uint8 format (this will error) model_1.eval() with torch.inference_mode():     model_1(custom_image_uint8.to(device)) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [65], in &lt;cell line: 3&gt;()\n      2 model_1.eval()\n      3 with torch.inference_mode():\n----&gt; 4     model_1(custom_image_uint8.to(device))\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [45], in TinyVGG.forward(self, x)\n     39 def forward(self, x: torch.Tensor):\n---&gt; 40     x = self.conv_block_1(x)\n     41     # print(x.shape)\n     42     x = self.conv_block_2(x)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\container.py:141, in Sequential.forward(self, input)\n    139 def forward(self, input):\n    140     for module in self:\n--&gt; 141         input = module(input)\n    142     return input\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447, in Conv2d.forward(self, input)\n    446 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 447     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443, in Conv2d._conv_forward(self, input, weight, bias)\n    439 if self.padding_mode != 'zeros':\n    440     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    441                     weight, bias, self.stride,\n    442                     _pair(0), self.dilation, self.groups)\n--&gt; 443 return F.conv2d(input, weight, bias, self.stride,\n    444 self.padding, self.dilation, self.groups)\n\nRuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same</pre> <p>If we try to make a prediction on an image in a different datatype to what our model was trained on, we get an error like the following:</p> <p><code>RuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same</code></p> <p>Let's fix this by converting our custom image to the same datatype as what our model was trained on (<code>torch.float32</code>).</p> In\u00a0[66]: Copied! <pre># Load in custom image and convert the tensor values to float32\ncustom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n\n# Divide the image pixel values by 255 to get them between [0, 1]\ncustom_image = custom_image / 255. \n\n# Print out image data\nprint(f\"Custom image tensor:\\n{custom_image}\\n\")\nprint(f\"Custom image shape: {custom_image.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image.dtype}\")\n</pre> # Load in custom image and convert the tensor values to float32 custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)  # Divide the image pixel values by 255 to get them between [0, 1] custom_image = custom_image / 255.   # Print out image data print(f\"Custom image tensor:\\n{custom_image}\\n\") print(f\"Custom image shape: {custom_image.shape}\\n\") print(f\"Custom image dtype: {custom_image.dtype}\") <pre>Custom image tensor:\ntensor([[[0.6039, 0.6863, 0.7098,  ..., 0.0824, 0.0706, 0.0549],\n         [0.5725, 0.6549, 0.7059,  ..., 0.0824, 0.0706, 0.0588],\n         [0.4863, 0.5725, 0.6706,  ..., 0.0706, 0.0667, 0.0588],\n         ...,\n         [0.2824, 0.2314, 0.1765,  ..., 0.5961, 0.5882, 0.5804],\n         [0.2510, 0.2157, 0.1608,  ..., 0.5882, 0.5765, 0.5647],\n         [0.2510, 0.2353, 0.1804,  ..., 0.5843, 0.5725, 0.5608]],\n\n        [[0.6706, 0.7412, 0.7569,  ..., 0.0863, 0.0745, 0.0588],\n         [0.6392, 0.7098, 0.7608,  ..., 0.0863, 0.0745, 0.0627],\n         [0.5529, 0.6392, 0.7255,  ..., 0.0745, 0.0706, 0.0627],\n         ...,\n         [0.2157, 0.1647, 0.1098,  ..., 0.4157, 0.4078, 0.4000],\n         [0.1843, 0.1490, 0.0941,  ..., 0.4235, 0.4118, 0.4000],\n         [0.1843, 0.1686, 0.1137,  ..., 0.4196, 0.4078, 0.3961]],\n\n        [[0.4588, 0.5412, 0.5686,  ..., 0.0667, 0.0549, 0.0392],\n         [0.4275, 0.5098, 0.5686,  ..., 0.0667, 0.0549, 0.0431],\n         [0.3412, 0.4353, 0.5333,  ..., 0.0549, 0.0510, 0.0431],\n         ...,\n         [0.1373, 0.0863, 0.0314,  ..., 0.2118, 0.2039, 0.1961],\n         [0.1059, 0.0706, 0.0157,  ..., 0.1961, 0.1843, 0.1725],\n         [0.1059, 0.0902, 0.0353,  ..., 0.1922, 0.1804, 0.1686]]])\n\nCustom image shape: torch.Size([3, 4032, 3024])\n\nCustom image dtype: torch.float32\n</pre> In\u00a0[67]: Copied! <pre># Plot custom image\nplt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error\nplt.title(f\"Image shape: {custom_image.shape}\")\nplt.axis(False);\n</pre> # Plot custom image plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error plt.title(f\"Image shape: {custom_image.shape}\") plt.axis(False); <p>Two thumbs up!</p> <p>Now how could we get our image to be the same size as the images our model was trained on?</p> <p>One way to do so is with <code>torchvision.transforms.Resize()</code>.</p> <p>Let's compose a transform pipeline to do so.</p> In\u00a0[68]: Copied! <pre># Create transform pipleine to resize image\ncustom_image_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n])\n\n# Transform target image\ncustom_image_transformed = custom_image_transform(custom_image)\n\n# Print out original shape and new shape\nprint(f\"Original shape: {custom_image.shape}\")\nprint(f\"New shape: {custom_image_transformed.shape}\")\n</pre> # Create transform pipleine to resize image custom_image_transform = transforms.Compose([     transforms.Resize((64, 64)), ])  # Transform target image custom_image_transformed = custom_image_transform(custom_image)  # Print out original shape and new shape print(f\"Original shape: {custom_image.shape}\") print(f\"New shape: {custom_image_transformed.shape}\") <pre>Original shape: torch.Size([3, 4032, 3024])\nNew shape: torch.Size([3, 64, 64])\n</pre> <p>Woohoo!</p> <p>Let's finally make a prediction on our own custom image.</p> In\u00a0[69]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed)\n</pre> model_1.eval() with torch.inference_mode():     custom_image_pred = model_1(custom_image_transformed) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [69], in &lt;cell line: 2&gt;()\n      1 model_1.eval()\n      2 with torch.inference_mode():\n----&gt; 3     custom_image_pred = model_1(custom_image_transformed)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [45], in TinyVGG.forward(self, x)\n     39 def forward(self, x: torch.Tensor):\n---&gt; 40     x = self.conv_block_1(x)\n     41     # print(x.shape)\n     42     x = self.conv_block_2(x)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\container.py:141, in Sequential.forward(self, input)\n    139 def forward(self, input):\n    140     for module in self:\n--&gt; 141         input = module(input)\n    142     return input\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447, in Conv2d.forward(self, input)\n    446 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 447     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443, in Conv2d._conv_forward(self, input, weight, bias)\n    439 if self.padding_mode != 'zeros':\n    440     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    441                     weight, bias, self.stride,\n    442                     _pair(0), self.dilation, self.groups)\n--&gt; 443 return F.conv2d(input, weight, bias, self.stride,\n    444 self.padding, self.dilation, self.groups)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)</pre> <p>Oh my goodness...</p> <p>Despite our preparations our custom image and model are on different devices.</p> <p>And we get the error:</p> <p><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)</code></p> <p>Let's fix that by putting our <code>custom_image_transformed</code> on the target device.</p> In\u00a0[70]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed.to(device))\n</pre> model_1.eval() with torch.inference_mode():     custom_image_pred = model_1(custom_image_transformed.to(device)) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [70], in &lt;cell line: 2&gt;()\n      1 model_1.eval()\n      2 with torch.inference_mode():\n----&gt; 3     custom_image_pred = model_1(custom_image_transformed.to(device))\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [45], in TinyVGG.forward(self, x)\n     42 x = self.conv_block_2(x)\n     43 # print(x.shape)\n---&gt; 44 x = self.classifier(x)\n     45 # print(x.shape)\n     46 return x\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\container.py:141, in Sequential.forward(self, input)\n    139 def forward(self, input):\n    140     for module in self:\n--&gt; 141         input = module(input)\n    142     return input\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103, in Linear.forward(self, input)\n    102 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 103     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (10x256 and 2560x3)</pre> <p>What now?</p> <p>It looks like we're getting a shape error.</p> <p>Why might this be?</p> <p>We converted our custom image to be the same size as the images our model was trained on...</p> <p>Oh wait...</p> <p>There's one dimension we forgot about.</p> <p>The batch size.</p> <p>Our model expects image tensors with a batch size dimension at the start (<code>NCHW</code> where <code>N</code> is the batch size).</p> <p>Except our custom image is currently only <code>CHW</code>.</p> <p>We can add a batch size dimension using <code>torch.unsqueeze(dim=0)</code> to add an extra dimension our image and finally make a prediction.</p> <p>Essentially we'll be telling our model to predict on a single image (an image with a <code>batch_size</code> of 1).</p> In\u00a0[71]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    # Add an extra dimension to image\n    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n    \n    # Print out different shapes\n    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n    \n    # Make a prediction on image with an extra dimension\n    custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device))\n</pre> model_1.eval() with torch.inference_mode():     # Add an extra dimension to image     custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)          # Print out different shapes     print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")     print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")          # Make a prediction on image with an extra dimension     custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device)) <pre>Custom image transformed shape: torch.Size([3, 64, 64])\nUnsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n</pre> <p>Yes!!!</p> <p>It looks like it worked!</p> <p>Note: What we've just gone through are three of the classical and most common deep learning and PyTorch issues:</p> <ol> <li>Wrong datatypes - our model expects <code>torch.float32</code> where our original custom image was <code>uint8</code>.</li> <li>Wrong device - our model was on the target <code>device</code> (in our case, the GPU) whereas our target data hadn't been moved to the target <code>device</code> yet.</li> <li>Wrong shapes - our model expected an input image of shape <code>[N, C, H, W]</code> or <code>[batch_size, color_channels, height, width]</code> whereas our custom image tensor was of shape <code>[color_channels, height, width]</code>.</li> </ol> <p>Keep in mind, these errors aren't just for predicting on custom images.</p> <p>They will be present with almost every kind of data type (text, audio, structured data) and problem you work with.</p> <p>Now let's take a look at our model's predictions.</p> In\u00a0[72]: Copied! <pre>custom_image_pred\n</pre> custom_image_pred Out[72]: <pre>tensor([[ 0.1161,  0.0213, -0.1422]], device='cuda:0')</pre> <p>Alright, these are still in logit form (the raw outputs of a model are called logits).</p> <p>Let's convert them from logits -&gt; prediction probabilities -&gt; prediction labels.</p> In\u00a0[73]: Copied! <pre># Print out prediction logits\nprint(f\"Prediction logits: {custom_image_pred}\")\n\n# Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\ncustom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\nprint(f\"Prediction probabilities: {custom_image_pred_probs}\")\n\n# Convert prediction probabilities -&gt; prediction labels\ncustom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\nprint(f\"Prediction label: {custom_image_pred_label}\")\n</pre> # Print out prediction logits print(f\"Prediction logits: {custom_image_pred}\")  # Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification) custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1) print(f\"Prediction probabilities: {custom_image_pred_probs}\")  # Convert prediction probabilities -&gt; prediction labels custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1) print(f\"Prediction label: {custom_image_pred_label}\") <pre>Prediction logits: tensor([[ 0.1161,  0.0213, -0.1422]], device='cuda:0')\nPrediction probabilities: tensor([[0.3729, 0.3392, 0.2880]], device='cuda:0')\nPrediction label: tensor([0], device='cuda:0')\n</pre> <p>Alright!</p> <p>Looking good.</p> <p>But of course our prediction label is still in index/tensor form.</p> <p>We can convert it to a string class name prediction by indexing on the <code>class_names</code> list.</p> In\u00a0[74]: Copied! <pre># Find the predicted label\ncustom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\ncustom_image_pred_class\n</pre> # Find the predicted label custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error custom_image_pred_class Out[74]: <pre>'pizza'</pre> <p>Wow.</p> <p>It looks like the model gets the prediction right, even though it was performing poorly based on our evaluation metrics.</p> <p>Note: The model in its current form will predict \"pizza\", \"steak\" or \"sushi\" no matter what image it's given. If you wanted your model to predict on a different class, you'd have to train it to do so.</p> <p>But if we check the <code>custom_image_pred_probs</code>, we'll notice that the model gives almost equal weight (the values are similar) to every class.</p> In\u00a0[75]: Copied! <pre># The values of the prediction probabilities are quite similar\ncustom_image_pred_probs\n</pre> # The values of the prediction probabilities are quite similar custom_image_pred_probs Out[75]: <pre>tensor([[0.3729, 0.3392, 0.2880]], device='cuda:0')</pre> <p>Having prediction probabilities this similar could mean a couple of things:</p> <ol> <li>The model is trying to predict all three classes at the same time (there may be an image containing pizza, steak and sushi).</li> <li>The model doesn't really know what it wants to predict and is in turn just assigning similar values to each of the classes.</li> </ol> <p>Our case is number 2, since our model is poorly trained, it is basically guessing the prediction.</p> In\u00a0[76]: Copied! <pre>def pred_and_plot_image(model: torch.nn.Module, \n                        image_path: str, \n                        class_names: List[str] = None, \n                        transform=None,\n                        device: torch.device = device):\n\"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n    \n    # 1. Load in image and convert the tensor values to float32\n    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n    \n    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n    target_image = target_image / 255. \n    \n    # 3. Transform if necessary\n    if transform:\n        target_image = transform(target_image)\n    \n    # 4. Make sure the model is on the target device\n    model.to(device)\n    \n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n        # Add an extra dimension to the image\n        target_image = target_image.unsqueeze(dim=0)\n    \n        # Make a prediction on image with an extra dimension and send it to the target device\n        target_image_pred = model(target_image.to(device))\n        \n    # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 7. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n    \n    # 8. Plot the image alongside the prediction and prediction probability\n    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib\n    if class_names:\n        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    else: \n        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    plt.title(title)\n    plt.axis(False);\n</pre> def pred_and_plot_image(model: torch.nn.Module,                          image_path: str,                          class_names: List[str] = None,                          transform=None,                         device: torch.device = device):     \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"          # 1. Load in image and convert the tensor values to float32     target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)          # 2. Divide the image pixel values by 255 to get them between [0, 1]     target_image = target_image / 255.           # 3. Transform if necessary     if transform:         target_image = transform(target_image)          # 4. Make sure the model is on the target device     model.to(device)          # 5. Turn on model evaluation mode and inference mode     model.eval()     with torch.inference_mode():         # Add an extra dimension to the image         target_image = target_image.unsqueeze(dim=0)              # Make a prediction on image with an extra dimension and send it to the target device         target_image_pred = model(target_image.to(device))              # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)     target_image_pred_probs = torch.softmax(target_image_pred, dim=1)      # 7. Convert prediction probabilities -&gt; prediction labels     target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)          # 8. Plot the image alongside the prediction and prediction probability     plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib     if class_names:         title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"     else:          title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"     plt.title(title)     plt.axis(False); <p>What a nice looking function, let's test it out.</p> In\u00a0[77]: Copied! <pre># Pred on our custom image\npred_and_plot_image(model=model_1,\n                    image_path=custom_image_path,\n                    class_names=class_names,\n                    transform=custom_image_transform,\n                    device=device)\n</pre> # Pred on our custom image pred_and_plot_image(model=model_1,                     image_path=custom_image_path,                     class_names=class_names,                     transform=custom_image_transform,                     device=device) <p>Two thumbs up again!</p> <p>Looks like our model got the prediction right just by guessing.</p> <p>This won't always be the case with other images though...</p> <p>The image is pixelated too because we resized it to <code>[64, 64]</code> using <code>custom_image_transform</code>.</p> <p>Exercise: Try making a prediction with one of your own images of pizza, steak or sushi and see what happens.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#04-pytorch-custom-datasets","title":"04. PyTorch Custom Datasets\u00b6","text":"<p>In the last notebook, notebook 03, we looked at how to build computer vision models on an in-built dataset in PyTorch (FashionMNIST).</p> <p>The steps we took are similar across many different problems in machine learning.</p> <p>Find a dataset, turn the dataset into numbers, build a model (or find an existing model) to find patterns in those numbers that can be used for prediction.</p> <p>PyTorch has many built-in datasets used for a wide number of machine learning benchmarks, however, you'll often want to use your own custom dataset.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#what-is-a-custom-dataset","title":"What is a custom dataset?\u00b6","text":"<p>A custom dataset is a collection of data relating to a specific problem you're working on.</p> <p>In essence, a custom dataset can be comprised of almost anything.</p> <p>For example, if we were building a food image classification app like Nutrify, our custom dataset might be images of food.</p> <p>Or if we were trying to build a model to classify whether or not a text-based review on a website was positive or negative, our custom dataset might be examples of existing customer reviews and their ratings.</p> <p>Or if we were trying to build a sound classification app, our custom dataset might be sound samples alongside their sample labels.</p> <p>Or if we were trying to build a recommendation system for customers purchasing things on our website, our custom dataset might be examples of products other people have bought.</p> <p>PyTorch includes many existing functions to load in various custom datasets in the <code>TorchVision</code>, <code>TorchText</code>, <code>TorchAudio</code> and <code>TorchRec</code> domain libraries.</p> <p>But sometimes these existing functions may not be enough.</p> <p>In that case, we can always subclass <code>torch.utils.data.Dataset</code> and customize it to our liking.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to be applying the PyTorch Workflow we covered in notebook 01 and notebook 02 to a computer vision problem.</p> <p>But instead of using an in-built PyTorch dataset, we're going to be using our own dataset of pizza, steak and sushi images.</p> <p>The goal will be to load these images and then build a model to train and predict on them.</p> <p></p> <p>What we're going to build. We'll use <code>torchvision.datasets</code> as well as our own custom <code>Dataset</code> class to load in images of food and then we'll build a PyTorch computer vision model to hopefully be able to classify them.</p> <p>Specifically, we're going to cover:</p> Topic Contents 0. Importing PyTorch and setting up device-agnostic code Let's get PyTorch loaded and then follow best practice to setup our code to be device-agnostic. 1. Get data We're going to be using our own custom dataset of pizza, steak and sushi images. 2. Become one with the data (data preparation) At the beginning of any new machine learning problem, it's paramount to understand the data you're working with. Here we'll take some steps to figure out what data we have. 3. Transforming data Often, the data you get won't be 100% ready to use with a machine learning model, here we'll look at some steps we can take to transform our images so they're ready to be used with a model. 4. Loading data with <code>ImageFolder</code> (option 1) PyTorch has many in-built data loading functions for common types of data. <code>ImageFolder</code> is helpful if our images are in standard image classification format. 5. Loading image data with a custom <code>Dataset</code> What if PyTorch didn't have an in-built function to load data with? This is where we can build our own custom subclass of <code>torch.utils.data.Dataset</code>. 6. Other forms of transforms (data augmentation) Data augmentation is a common technique for expanding the diversity of your training data. Here we'll explore some of <code>torchvision</code>'s in-built data augmentation functions. 7. Model 0: TinyVGG without data augmentation By this stage, we'll have our data ready, let's build a model capable of fitting it. We'll also create some training and testing functions for training and evaluating our model. 8. Exploring loss curves Loss curves are a great way to see how your model is training/improving over time. They're also a good way to see if your model is underfitting or overfitting. 9. Model 1: TinyVGG with data augmentation By now, we've tried a model without, how about we try one with data augmentation? 10. Compare model results Let's compare our different models' loss curves and see which performed better and discuss some options for improving performance. 11. Making a prediction on a custom image Our model is trained to on a dataset of pizza, steak and sushi images. In this section we'll cover how to use our trained model to predict on an image outside of our existing dataset."},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#where-can-can-you-get-help","title":"Where can can you get help?\u00b6","text":"<p>All of the materials for this course live on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page there too.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#0-importing-pytorch-and-setting-up-device-agnostic-code","title":"0. Importing PyTorch and setting up device-agnostic code\u00b6","text":""},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#1-get-data","title":"1. Get data\u00b6","text":"<p>First thing's first we need some data.</p> <p>And like any good cooking show, some data has already been prepared for us.</p> <p>We're going to start small.</p> <p>Because we're not looking to train the biggest model or use the biggest dataset yet.</p> <p>Machine learning is an iterative process, start small, get something working and increase when necessary.</p> <p>The data we're going to be using is a subset of the Food101 dataset.</p> <p>Food101 is popular computer vision benchmark as it contains 1000 images of 101 different kinds of foods, totaling 101,000 images (75,750 train and 25,250 test).</p> <p>Can you think of 101 different foods?</p> <p>Can you think of a computer program to classify 101 foods?</p> <p>I can.</p> <p>A machine learning model!</p> <p>Specifically, a PyTorch computer vision model like we covered in notebook 03.</p> <p>Instead of 101 food classes though, we're going to start with 3: pizza, steak and sushi.</p> <p>And instead of 1,000 images per class, we're going to start with a random 10% (start small, increase when necessary).</p> <p>If you'd like to see where the data came from you see the following resources:</p> <ul> <li>Original Food101 dataset and paper website.</li> <li><code>torchvision.datasets.Food101</code> - the version of the data I downloaded for this notebook.</li> <li><code>extras/04_custom_data_creation.ipynb</code> - a notebook I used to format the Food101 dataset to use for this notebook.</li> <li><code>data/pizza_steak_sushi.zip</code> - the zip archive of pizza, steak and sushi images from Food101, created with the notebook linked above.</li> </ul> <p>Let's write some code to download the formatted data from GitHub.</p> <p>Note: The dataset we're about to use has been pre-formatted for what we'd like to use it for. However, you'll often have to format your own datasets for whatever problem you're working on. This is a regular practice in the machine learning world.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#2-become-one-with-the-data-data-preparation","title":"2. Become one with the data (data preparation)\u00b6","text":"<p>Dataset downloaded!</p> <p>Time to become one with it.</p> <p>This is another important step before building a model.</p> <p>As Abraham Lossfunction said...</p> <p>Data preparation is paramount. Before building a model, become one with the data. Ask: What am I trying to do here? Source: @mrdbourke Twitter.</p> <p>What's inspecting the data and becoming one with it?</p> <p>Before starting a project or building any kind of model, it's important to know what data you're working with.</p> <p>In our case, we have images of pizza, steak and sushi in standard image classification format.</p> <p>Image classification format contains separate classes of images in seperate directories titled with a particular class name.</p> <p>For example, all images of <code>pizza</code> are contained in the <code>pizza/</code> directory.</p> <p>This format is popular across many different image classification benchmarks, including ImageNet (of the most popular computer vision benchmark datasets).</p> <p>You can see an example of the storage format below, the images numbers are arbitrary.</p> <pre><code>pizza_steak_sushi/ &lt;- overall dataset folder\n    train/ &lt;- training images\n        pizza/ &lt;- class name as folder name\n            image01.jpeg\n            image02.jpeg\n            ...\n        steak/\n            image24.jpeg\n            image25.jpeg\n            ...\n        sushi/\n            image37.jpeg\n            ...\n    test/ &lt;- testing images\n        pizza/\n            image101.jpeg\n            image102.jpeg\n            ...\n        steak/\n            image154.jpeg\n            image155.jpeg\n            ...\n        sushi/\n            image167.jpeg\n            ...\n</code></pre> <p>The goal will be to take this data storage structure and turn it into a dataset usable with PyTorch.</p> <p>Note: The structure of the data you work with will vary depending on the problem you're working on. But the premise still remains: become one with the data, then find a way to best turn it into a dataset compatible with PyTorch.</p> <p>We can inspect what's in our data directory by writing a small helper function to walk through each of the subdirectories and count the files present.</p> <p>To do so, we'll use Python's in-built <code>os.walk()</code>.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#21-visualize-an-image","title":"2.1 Visualize an image\u00b6","text":"<p>Okay, we've seen how our directory structure is formatted.</p> <p>Now in the spirit of the data explorer, it's time to visualize, visualize, visualize!</p> <p>Let's write some code to:</p> <ol> <li>Get all of the image paths using <code>pathlib.Path.glob()</code> to find all of the files ending in <code>.jpg</code>.</li> <li>Pick a random image path using Python's <code>random.choice()</code>.</li> <li>Get the image class name using <code>pathlib.Path.parent.stem</code>.</li> <li>And since we're working with images, we'll open the random image path using <code>PIL.Image.open()</code> (PIL stands for Python Image Library).</li> <li>We'll then show the image and print some metadata.</li> </ol>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#3-transforming-data","title":"3. Transforming data\u00b6","text":"<p>Now what if we wanted to load our image data into PyTorch?</p> <p>Before we can use our image data with PyTorch we need to:</p> <ol> <li>Turn it into tensors (numerical representations of our images).</li> <li>Turn it into a <code>torch.utils.data.Dataset</code> and subsequently a <code>torch.utils.data.DataLoader</code>, we'll call these <code>Dataset</code> and <code>DataLoader</code> for short.</li> </ol> <p>There are several different kinds of pre-built datasets and dataset loaders for PyTorch, depending on the problem you're working on.</p> Problem space Pre-built Datasets and Functions Vision <code>torchvision.datasets</code> Audio <code>torchaudio.datasets</code> Text <code>torchtext.datasets</code> Recommendation system <code>torchrec.datasets</code> <p>Since we're working with a vision problem, we'll be looking at <code>torchvision.datasets</code> for our data loading functions as well as <code>torchvision.transforms</code> for preparing our data.</p> <p>Let's import some base libraries.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#31-transforming-data-with-torchvisiontransforms","title":"3.1 Transforming data with <code>torchvision.transforms</code>\u00b6","text":"<p>We've got folders of images but before we can use them with PyTorch, we need to convert them into tensors.</p> <p>One of the ways we can do this is by using the <code>torchvision.transforms</code> module.</p> <p><code>torchvision.transforms</code> contains many pre-built methods for formatting images, turning them into tensors and even manipulating them for data augmentation (the practice of altering data to make it harder for a model to learn, we'll see this later on) purposes .</p> <p>To get experience with <code>torchvision.transforms</code>, let's write a series of transform steps that:</p> <ol> <li>Resize the images using <code>transforms.Resize()</code> (from about 512x512 to 64x64, the same shape as the images on the CNN Explainer website).</li> <li>Flip our images randomly on the horizontal using <code>transforms.RandomHorizontalFlip()</code> (this could be considered a form of data augmentation because it will artificially change our image data).</li> <li>Turn our images from a PIL image to a PyTorch tensor using <code>transforms.ToTensor()</code>.</li> </ol> <p>We can compile all of these steps using <code>torchvision.transforms.Compose()</code>.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#4-option-1-loading-image-data-using-imagefolder","title":"4. Option 1: Loading Image Data Using <code>ImageFolder</code>\u00b6","text":"<p>Alright, time to turn our image data into a <code>Dataset</code> capable of being used with PyTorch.</p> <p>Since our data is in standard image classification format, we can use the class <code>torchvision.datasets.ImageFolder</code>.</p> <p>Where we can pass it the file path of a target image directory as well as a series of transforms we'd like to perform on our images.</p> <p>Let's test it out on our data folders <code>train_dir</code> and <code>test_dir</code> passing in <code>transform=data_transform</code> to turn our images into tensors.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#41-turn-loaded-images-into-dataloaders","title":"4.1 Turn loaded images into <code>DataLoader</code>'s\u00b6","text":"<p>We've got our images as PyTorch <code>Dataset</code>'s but now let's turn them into <code>DataLoader</code>'s.</p> <p>We'll do so using <code>torch.utils.data.DataLoader</code>.</p> <p>Turning our <code>Dataset</code>'s into <code>DataLoader</code>'s makes them iterable so a model can go through learn the relationships between samples and targets (features and labels).</p> <p>To keep things simple, we'll use a <code>batch_size=1</code> and <code>num_workers=1</code>.</p> <p>What's <code>num_workers</code>?</p> <p>Good question.</p> <p>It defines how many subprocesses will be created to load your data.</p> <p>Think of it like this, the higher value <code>num_workers</code> is set to, the more compute power PyTorch will use to load your data.</p> <p>Personally, I usually set it to the total number of CPUs on my machine via Python's <code>os.cpu_count()</code>.</p> <p>This ensures the <code>DataLoader</code> recruits as many cores as possible to load data.</p> <p>Note: There are more parameters you can get familiar with using <code>torch.utils.data.DataLoader</code> in the PyTorch documentation.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#5-option-2-loading-image-data-with-a-custom-dataset","title":"5. Option 2: Loading Image Data with a Custom <code>Dataset</code>\u00b6","text":"<p>What if a pre-built <code>Dataset</code> creator like <code>torchvision.datasets.ImageFolder()</code> didn't exist?</p> <p>Or one for your specific problem didn't exist?</p> <p>Well, you could build your own.</p> <p>But wait, what are the pros and cons of creating your own custom way to load <code>Dataset</code>'s?</p> Pros of creating a custom <code>Dataset</code> Cons of creating a custom <code>Dataset</code> Can create a <code>Dataset</code> out of almost anything. Even though you could create a <code>Dataset</code> out of almost anything, it doesn't mean it will work. Not limited to PyTorch pre-built <code>Dataset</code> functions. Using a custom <code>Dataset</code> often results in writing more code, which could be prone to errors or performance issues. <p>To see this in action, let's work towards replicating <code>torchvision.datasets.ImageFolder()</code> by subclassing <code>torch.utils.data.Dataset</code> (the base class for all <code>Dataset</code>'s in PyTorch).</p> <p>We'll start by importing the modules we need:</p> <ul> <li>Python's <code>os</code> for dealing with directories (our data is stored in directories).</li> <li>Python's <code>pathlib</code> for dealing with filepaths (each of our images has a unique filepath).</li> <li><code>torch</code> for all things PyTorch.</li> <li>PIL's <code>Image</code> class for loading images.</li> <li><code>torch.utils.data.Dataset</code> to subclass and create our own custom <code>Dataset</code>.</li> <li><code>torchvision.transforms</code> to turn our images into tensors.</li> <li>Various types from Python's <code>typing</code> module to add type hints to our code.</li> </ul> <p>Note: You can customize the following steps for your own dataset. The premise remains: write code to load your data in the format you'd like it.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#51-creating-a-helper-function-to-get-class-names","title":"5.1 Creating a helper function to get class names\u00b6","text":"<p>Let's write a helper function capable of creating a list of class names and a dictionary of class names and their indexes given a directory path.</p> <p>To do so, we'll:</p> <ol> <li>Get the class names using <code>os.scandir()</code> to traverse a target directory (ideally the directory is in standard image classification format).</li> <li>Raise an error if the class names aren't found (if this happens, there might be something wrong with the directory structure).</li> <li>Turn the class names into a dictionary of numerical labels, one for each class.</li> </ol> <p>Let's see a small example of step 1 before we write the full function.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#52-create-a-custom-dataset-to-replicate-imagefolder","title":"5.2 Create a custom <code>Dataset</code> to replicate <code>ImageFolder</code>\u00b6","text":"<p>Now we're ready to build our own custom <code>Dataset</code>.</p> <p>We'll build one to replicate the functionality of <code>torchvision.datasets.ImageFolder()</code>.</p> <p>This will be good practice, plus, it'll reveal a few of the required steps to make your own custom <code>Dataset</code>.</p> <p>It'll be a fair bit of a code... but nothing we can't handle!</p> <p>Let's break it down:</p> <ol> <li>Subclass <code>torch.utils.data.Dataset</code>.</li> <li>Initialize our subclass with a <code>targ_dir</code> parameter (the target data directory) and <code>transform</code> parameter (so we have the option to transform our data if needed).</li> <li>Create several attributes for <code>paths</code> (the paths of our target images), <code>transform</code> (the transforms we might like to use, this can be <code>None</code>), <code>classes</code> and <code>class_to_idx</code> (from our <code>find_classes()</code> function).</li> <li>Create a function to load images from file and return them, this could be using <code>PIL</code> or <code>torchvision.io</code> (for input/output of vision data).</li> <li>Overwrite the <code>__len__</code> method of <code>torch.utils.data.Dataset</code> to return the number of samples in the <code>Dataset</code>, this is recommended but not required. This is so you can call <code>len(Dataset)</code>.</li> <li>Overwrite the <code>__getitem__</code> method of <code>torch.utils.data.Dataset</code> to return a single sample from the <code>Dataset</code>, this is required.</li> </ol> <p>Let's do it!</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#53-create-a-function-to-display-random-images","title":"5.3 Create a function to display random images\u00b6","text":"<p>You know what time it is!</p> <p>Time to put on our data explorer's hat and visualize, visualize, visualize!</p> <p>Let's create a helper function called <code>display_random_images()</code> that helps us visualize images in our <code>Dataset'</code>s.</p> <p>Specifically, it'll:</p> <ol> <li>Take in a <code>Dataset</code> and a number of other parameters such as <code>classes</code> (the names of our target classes), the number of images to display (<code>n</code>) and a random seed.</li> <li>To prevent the display getting out of hand, we'll cap <code>n</code> at 10 images.</li> <li>Set the random seed for reproducible plots (if <code>seed</code> is set).</li> <li>Get a list of random sample indexes (we can use Python's <code>random.sample()</code> for this) to plot.</li> <li>Setup a <code>matplotlib</code> plot.</li> <li>Loop through the random sample indexes found in step 4 and plot them with <code>matplotlib</code>.</li> <li>Make sure the sample images are of shape <code>HWC</code> (height, width, color channels) so we can plot them.</li> </ol>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#54-turn-custom-loaded-images-into-dataloaders","title":"5.4 Turn custom loaded images into <code>DataLoader</code>'s\u00b6","text":"<p>We've got a way to turn our raw images into <code>Dataset</code>'s (features mapped to labels or <code>X</code>'s mapped to <code>y</code>'s) through our <code>ImageFolderCustom</code> class.</p> <p>Now how could we turn our custom <code>Dataset</code>'s into <code>DataLoader</code>'s?</p> <p>If you guessed by using <code>torch.utils.data.DataLoader()</code>, you'd be right!</p> <p>Because our custom <code>Dataset</code>'s subclass <code>torch.utils.data.Dataset</code>, we can use them directly with <code>torch.utils.data.DataLoader()</code>.</p> <p>And we can do using very similar steps to before except this time we'll be using our custom created <code>Dataset</code>'s.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation","title":"6. Other forms of transforms (data augmentation)\u00b6","text":"<p>We've seen a couple of transforms on our data already but there's plenty more.</p> <p>You can see them all in the <code>torchvision.transforms</code> documentation.</p> <p>The purpose of tranforms is to alter your images in some way.</p> <p>That may be turning your images into a tensor (as we've seen before).</p> <p>Or cropping it or randomly erasing a portion or randomly rotating them.</p> <p>Doing this kinds of transforms is often referred to as data augmentation.</p> <p>Data augmentation is the process of altering your data in such a way that you artificially increase the diversity of your training set.</p> <p>Training a model on this artificially altered dataset hopefully results in a model that is capable of better generalization (the patterns it learns are more robust to future unseen examples).</p> <p>You can see many different examples of data augmentation performed on images using <code>torchvision.transforms</code> in PyTorch's Illustration of Transforms example.</p> <p>But let's try one out ourselves.</p> <p>Machine learning is all about harnessing the power of randomness and research shows that random transforms (like <code>transforms.RandAugment()</code> and <code>transforms.TrivialAugmentWide()</code>) generally perform better than hand-picked transforms.</p> <p>The idea behind TrivialAugment is... well, trivial.</p> <p>You have a set of transforms and you randomly pick a number of them to perform on an image and at a random magnitude between a given range (a higher magnitude means more instense).</p> <p>The PyTorch team even used TrivialAugment it to train their latest state-of-the-art vision models.</p> <p></p> <p>TrivialAugment was one of the ingredients used in a recent state of the art training upgrade to various PyTorch vision models.</p> <p>How about we test it out on some of our own images?</p> <p>The main parameter to pay attention to in <code>transforms.TrivialAugmentWide()</code> is <code>num_magnitude_bins=31</code>.</p> <p>It defines how much of a range an intensity value will be picked to apply a certain transform, <code>0</code> being no range and <code>31</code> being maximum range (highest chance for highest intensity).</p> <p>We can incorporate <code>transforms.TrivialAugmentWide()</code> into <code>transforms.Compose()</code>.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#7-model-0-tinyvgg-without-data-augmentation","title":"7. Model 0: TinyVGG without data augmentation\u00b6","text":"<p>Alright, we've seen how to turn our data from images in folders to transformed tensors.</p> <p>Now let's construct a computer vision model to see if we can classify if an image is of pizza, steak or sushi.</p> <p>To begin, we'll start with a simple transform, only resizing the images to <code>(64, 64)</code> and turning them into tensors.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#71-creating-transforms-and-loading-data-for-model-0","title":"7.1 Creating transforms and loading data for Model 0\u00b6","text":""},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#72-create-tinyvgg-model-class","title":"7.2 Create TinyVGG model class\u00b6","text":"<p>In notebook 03, we used the TinyVGG model from the CNN Explainer website.</p> <p>Let's recreate the same model, except this time we'll be using color images instead of grayscale (<code>in_channels=3</code> instead of <code>in_channels=1</code> for RGB pixels).</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#73-try-a-forward-pass-on-a-single-image-to-test-the-model","title":"7.3 Try a forward pass on a single image (to test the model)\u00b6","text":"<p>A good way to test a model is to do a forward pass on a single piece of data.</p> <p>It's also handy way to test the input and output shapes of our different layers.</p> <p>To do a forward pass on a single image, let's:</p> <ol> <li>Get a batch of images and labels from the <code>DataLoader</code>.</li> <li>Get a single image from the batch and <code>unsqueeze()</code> the image so it has a batch size of <code>1</code> (so its shape fits the model).</li> <li>Perform inference on a single image (making sure to send the image to the target <code>device</code>).</li> <li>Print out what's happening and convert the model's raw output logits to prediction probabilities with <code>torch.softmax()</code> (since we're working with multi-class data) and convert the prediction probabilities to prediction labels with <code>torch.argmax()</code>.</li> </ol>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#74-use-torchinfo-to-get-an-idea-of-the-shapes-going-through-our-model","title":"7.4 Use <code>torchinfo</code> to get an idea of the shapes going through our model\u00b6","text":"<p>Printing out our model with <code>print(model)</code> gives us an idea of what's going on with our model.</p> <p>And we can print out the shapes of our data throughout the <code>forward()</code> method.</p> <p>However, a helpful way to get information from our model is to use <code>torchinfo</code>.</p> <p><code>torchinfo</code> comes with a <code>summary()</code> method that takes a PyTorch model as well as an <code>input_shape</code> and returns what happens as a tensor moves through your model.</p> <p>Note: If you're using Google Colab, you'll need to install <code>torchinfo</code>.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#75-create-train-test-loop-functions","title":"7.5 Create train &amp; test loop functions\u00b6","text":"<p>We've got data and we've got a model.</p> <p>Now let's make some training and test loop functions to train our model on the training data and evaluate our model on the testing data.</p> <p>And to make sure we can use these the training and testing loops again, we'll functionize them.</p> <p>Specifically, we're going to make three functions:</p> <ol> <li><code>train_step()</code> - takes in a model, a <code>DataLoader</code>, a loss function and an optimizer and trains the model on the <code>DataLoader</code>.</li> <li><code>test_step()</code> - takes in a model, a <code>DataLoader</code> and a loss function and evaluates the model on the <code>DataLoader</code>.</li> <li><code>train()</code> - performs 1. and 2. together for a given number of epochs and returns a results dictionary.</li> </ol> <p>Note: We covered the steps in a PyTorch opimization loop in notebook 01, as well as the Unofficial PyTorch Optimization Loop Song and we've built similar functions in notebook 03.</p> <p>Let's start by building <code>train_step()</code>.</p> <p>Because we're dealing with batches in the <code>DataLoader</code>'s, we'll accumulate the model loss and accuracy values during training (by adding them up for each batch) and then adjust them at the end before we return them.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#76-creating-a-train-function-to-combine-train_step-and-test_step","title":"7.6 Creating a <code>train()</code> function to combine <code>train_step()</code> and <code>test_step()</code>\u00b6","text":"<p>Now we need a way to put our <code>train_step()</code> and <code>test_step()</code> functions together.</p> <p>To do so, we'll package them up in a <code>train()</code> function.</p> <p>This function will train the model as well as evaluate it.</p> <p>Specificially, it'll:</p> <ol> <li>Take in a model, a <code>DataLoader</code> for training and test sets, an optimizer, a loss function and how many epochs to perform each train and test step for.</li> <li>Create an empty results dictionary for <code>train_loss</code>, <code>train_acc</code>, <code>test_loss</code> and <code>test_acc</code> values (we can fill this up as training goes on).</li> <li>Loop through the training and test step functions for a number of epochs.</li> <li>Print out what's happening at the end of each epoch.</li> <li>Update the empty results dictionary with the updated metrics each epoch.</li> <li>Return the filled</li> </ol> <p>To keep track of the number of epochs we've been through, let's import <code>tqdm</code> from <code>tqdm.auto</code> (<code>tqdm</code> is one of the most popular progress bar libraries for Python and <code>tqdm.auto</code> automatically decides what kind of progress bar is best for your computing environment, e.g. Jupyter Notebook vs. Python script).</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#77-train-and-evaluate-model-0","title":"7.7 Train and Evaluate Model 0\u00b6","text":"<p>Alright, alright, alright we've got all of the ingredients we need to train and evaluate our model.</p> <p>Time to put our <code>TinyVGG</code> model, <code>DataLoader</code>'s and <code>train()</code> function together to see if we can build a model capable of discerning between pizza, steak and sushi!</p> <p>Let's recreate <code>model_0</code> (we don't need to but we will for completeness) then call our <code>train()</code> function passing in the necessary parameters.</p> <p>To keep our experiments quick, we'll train our model for 5 epochs (though you could increase this if you want).</p> <p>As for an optimizer and loss function, we'll use <code>torch.nn.CrossEntropyLoss()</code> (since we're working with multi-class classification data) and <code>torch.optim.Adam()</code> with a learning rate of <code>1e-3</code> respecitvely.</p> <p>To see how long things take, we'll import Python's <code>timeit.default_timer()</code> method to calculate the training time.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0","title":"7.8 Plot the loss curves of Model 0\u00b6","text":"<p>From the print outs of our <code>model_0</code> training, it didn't look like it did too well.</p> <p>But we can further evaluate it by plotting the model's loss curves.</p> <p>Loss curves show the model's results over time.</p> <p>And they're a great way to see how your model performs on different datasets (e.g. training and test).</p> <p>Let's create a function to plot the values in our <code>model_0_results</code> dictionary.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like","title":"8. What should an ideal loss curve look like?\u00b6","text":"<p>Looking at training and test loss curves is a great way to see if your model is overfitting.</p> <p>An overfitting model is one that performs better (often by a considerable margin) on the training set than the validation/test set.</p> <p>If your training loss is far lower than your test loss, your model is overfitting.</p> <p>As in, it's learning the patterns in the training too well and those patterns aren't generalizing to the test data.</p> <p>The other side is when your training and test loss are not as low as you'd like, this is considered underfitting.</p> <p>The ideal position for a training and test loss curve is for them to line up closely with each other.</p> <p></p> <p>Left: If your training and test loss curves aren't as low as you'd like, this is considered underfitting. *Middle: When your test/validation loss is higher than your training loss this is considered overfitting. Right: The ideal scenario is when your training and test loss curves line up over time. This means your model is generalizing well. There are more combinations and different things loss curves can do, for more on these, see Google's Interpreting Loss Curves guide.*</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#81-how-to-deal-with-overfitting","title":"8.1 How to deal with overfitting\u00b6","text":"<p>Since the main problem with overfitting is that you're model is fitting the training data too well, you'll want to use techniques to \"reign it in\".</p> <p>A common technique of preventing overfitting is known as regularization.</p> <p>I like to think of this as \"making our models more regular\", as in, capable of fitting more kinds of data.</p> <p>Let's discuss a few methods to prevent overfitting.</p> Method to prevent overfitting What is it? Get more data Having more data gives the model more opportunities to learn patterns, patterns which may be more generalizable to new examples. Simplify your model If the current model is already overfitting the training data, it may be too complicated of a model. This means it's learning the patterns of the data too well and isn't able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer. Use data augmentation Data augmentation manipulates the training data in a way so that's harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data. Use transfer learning Transfer learning involves leveraging the patterns (also called pretrained weights) one model has learned to use as the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety of images and then tweak it slightly to be more specialized for food images. Use dropout layers Dropout layers randomly remove connections between hidden layers in neural networks, effectively simplifying a model but also making the remaining connections better. See <code>torch.nn.Dropout()</code> for more. Use learning rate decay The idea here is to slowly decrease the learning rate as a model trains. This is akin to reaching for a coin at the back of a couch. The closer you get, the smaller your steps. The same with the learning rate, the closer you get to convergence, the smaller you'll want your weight updates to be. Use early stopping Early stopping stops model training before it begins to overfit. As in, say the model's loss has stopped decreasing for the past 10 epochs (this number is arbitrary), you may want to stop the model training here and go with the model weights that had the lowest loss (10 epochs prior). <p>There are more methods for dealing with overfitting but these are some of the main ones.</p> <p>As you start to build more and more deep models, you'll find because deep learnings are so good at learning patterns in data, dealing with overfitting is one of the primary problems of deep learning.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#82-how-to-deal-with-underfitting","title":"8.2 How to deal with underfitting\u00b6","text":"<p>When a model is underfitting it is considered to have poor predictive power on the training and test sets.</p> <p>In essence, an underfitting model will fail to reduce the loss values to a desired level.</p> <p>Right now, looking at our current loss curves, I'd considered our <code>TinyVGG</code> model, <code>model_0</code>, to be underfitting the data.</p> <p>The main idea behind dealing with underfitting is to increase your model's predictive power.</p> <p>There are several ways to do this.</p> Method to prevent underfitting What is it? Add more layers/units to your model If your model is underfitting, it may not have enough capability to learn the required patterns/weights/representations of the data to be predictive. One way to add more predictive power to your model is to increase the number of hidden layers/units within those layers. Tweak the learning rate Perhaps your model's learning rate is too high to begin with. And it's trying to update its weights each epoch too much, in turn not learning anything. In this case, you might lower the learning rate and see what happens. Use transfer learning Transfer learning is capable of preventing overfitting and underfitting. It involves using the patterns from a previously working model and adjusting them to your own problem. Train for longer Sometimes a model just needs more time to learn representations of data. If you find in your smaller experiments your model isn't learning anything, perhaps leaving it train for a more epochs may result in better performance. Use less regularization Perhaps your model is underfitting because you're trying to prevent overfitting too much. Holding back on regularization techniques can help your model fit the data better."},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#83-the-balance-between-overfitting-and-underfitting","title":"8.3 The balance between overfitting and underfitting\u00b6","text":"<p>None of the methods discussed above are silver bullets, meaning, they don't always work.</p> <p>And preventing overfitting and underfitting is possibly the most active area of machine learning research.</p> <p>Since everone wants their models to fit better (less underfitting) but not so good they don't generalize well and perform in the real world (less overfitting).</p> <p>There's a fine line between overfitting and underfitting.</p> <p>Because too much of each can cause the other.</p> <p>Transfer learning is perhaps one of the most powerful techniques when it comes to dealing with both overfitting and underfitting on your own problems.</p> <p>Rather than handcraft different overfitting and underfitting techniques, transfer learning enables you to take an already working model in a similar problem space to yours (say one from paperswithcode.com/sota or Hugging Face models) and apply it to your own dataset.</p> <p>We'll see the power of transfer learning in a later notebook.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#9-model-1-tinyvgg-with-data-augmentation","title":"9. Model 1: TinyVGG with Data Augmentation\u00b6","text":"<p>Time to try out another model!</p> <p>This time, let's load in the data and use data augmentation to see if it improves our results in anyway.</p> <p>First, we'll compose a training transform to include <code>transforms.TrivialAugmentWide()</code> as well as resize and turn our images into tensors.</p> <p>We'll do the same for a testing transform except without the data augmentation.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#91-create-transform-with-data-augmentation","title":"9.1 Create transform with data augmentation\u00b6","text":""},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#92-create-train-and-test-datasets-and-dataloaders","title":"9.2 Create train and test <code>Dataset</code>'s and <code>DataLoader</code>'s\u00b6","text":"<p>We'll make sure the train <code>Dataset</code> uses the <code>train_transform_trivial_augment</code> and the test <code>Dataset</code> uses the <code>test_transform</code>.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#93-construct-and-train-model-1","title":"9.3 Construct and train Model 1\u00b6","text":"<p>Data loaded!</p> <p>Now to build our next model, <code>model_1</code>, we can reuse our <code>TinyVGG</code> class from before.</p> <p>We'll make sure to send it to the target device.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#94-plot-the-loss-curves-of-model-1","title":"9.4 Plot the loss curves of Model 1\u00b6","text":"<p>Since we've got the results of <code>model_1</code> saved in a results dictionary, <code>model_1_results</code>, we can plot them using <code>plot_loss_curves()</code>.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#10-compare-model-results","title":"10. Compare model results\u00b6","text":"<p>Even though our models our performing quite poorly, we can still write code to compare them.</p> <p>Let's first turn our model results in pandas DataFrames.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#11-make-a-prediction-on-a-custom-image","title":"11. Make a prediction on a custom image\u00b6","text":"<p>If you've trained a model on a certain dataset, chances are you'd like to make a prediction on on your own custom data.</p> <p>In our case, since we've trained a model on pizza, steak and sushi images, how could we use our model to make a prediction on one of our own images?</p> <p>To do so, we can load an image and then preprocess it in a way that matches the type of data our model was trained on.</p> <p>In other words, we'll have to convert our own custom image to a tensor and make sure it's in the right datatype before passing it to our model.</p> <p>Let's start by downloading a custom image.</p> <p>Since our model predicts whether an image contains pizza, steak or sushi, let's download a photo of my Dad giving two thumbs up to a big pizza from the Learn PyTorch for Deep Learning GitHub.</p> <p>We download the image using Python's <code>requests</code> module.</p> <p>Note: If you're using Google Colab, you can also upload an image to the current session by going to the left hand side menu -&gt; Files -&gt; Upload to session storage. Beware though, this image will delete when your Google Colab session ends.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#111-loading-in-a-custom-image-with-pytorch","title":"11.1 Loading in a custom image with PyTorch\u00b6","text":"<p>Excellent!</p> <p>Looks like we've got a custom image downloaded and ready to go at <code>data/04-pizza-dad.jpeg</code>.</p> <p>Time to load it in.</p> <p>PyTorch's <code>torchvision</code> has several input and output (\"IO\" or \"io\" for short) methods for reading and writing images and video in <code>torchvision.io</code>.</p> <p>Since we want to load in an image, we'll use <code>torchvision.io.read_image()</code>.</p> <p>This method will read a JPEG or PNG image and turn it into a 3 dimensional RGB or grayscale <code>torch.Tensor</code> with values of datatype <code>uint8</code> in range <code>[0, 255]</code>.</p> <p>Let's try it out.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#112-predicting-on-custom-images-with-a-trained-pytorch-model","title":"11.2 Predicting on custom images with a trained PyTorch model\u00b6","text":"<p>Beautiful, it looks like our image data is now in the same format our model was trained on.</p> <p>Except for one thing...</p> <p>It's <code>shape</code>.</p> <p>Our model was trained on images with shape <code>[3, 64, 64]</code>, whereas our custom image is currently <code>[3, 4032, 3024]</code>.</p> <p>How could we make sure our custom image is the same shape as the images our model was trained on?</p> <p>Are there any <code>torchvision.transforms</code> that could help?</p> <p>Before we answer that question, let's plot the image with <code>matplotlib</code> to make sure it looks okay, remember we'll have to permute the dimensions from <code>CHW</code> to <code>HWC</code> to suit <code>matplotlib</code>'s requirements.</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function","title":"11.3 Putting custom image prediction together: building a function\u00b6","text":"<p>Doing all of the above steps every time you'd like to make a prediction on a custom image would quickly become tedious.</p> <p>So let's put them all together in a function we can easily use over and over again.</p> <p>Specifically, let's make a function that:</p> <ol> <li>Takes in a target image path and converts to the right datatype for our model (<code>torch.float32</code>).</li> <li>Makes sure the target image pixel values are in the range <code>[0, 1]</code>.</li> <li>Transforms the target image if necessary.</li> <li>Makes sure the model is on the target device.</li> <li>Makes a prediction on the target image with a trained model (ensuring the image is the right size and on the same device as the model).</li> <li>Converts the model's output logits to prediction probabilities.</li> <li>Converts the prediction probabilities to prediction labels.</li> <li>Plots the target image alongside the model prediction and prediction probability.</li> </ol> <p>A fair few steps but we've got this!</p>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#main-takeaways","title":"Main takeaways\u00b6","text":"<p>We've covered a fair bit in this module.</p> <p>Let's summarise it with a few dot points.</p> <ul> <li>PyTorch has many in-built functions to deal with all kinds of data, from vision to text to audio to recommendation systems.</li> <li>If PyTorch's built-in data loading functions don't suit your requirements, you can write code to create your own custom datasets by subclassing <code>torch.utils.data.Dataset</code>.</li> <li><code>torch.utils.data.DataLoader</code>'s in PyTorch help turn your <code>Dataset</code>'s into iterables that can be used when training and testing a model.</li> <li>A lot of machine learning is dealing with the balance between overfitting and underfitting (we discussed different methods for each above, so a good exercise would be to research more and writing code to try out the different techniques).</li> <li>Predicting on your own custom data with a trained model is possible, as long as you format the data into a similar format to what the model was trained on. Make sure you take care of the three big PyTorch and deep learning errors:<ol> <li>Wrong datatypes - Your model expected <code>torch.float32</code> when your data is <code>torch.uint8</code>.</li> <li>Wrong data shapes - Your model expected <code>[batch_size, color_channels, height, width]</code> when your data is <code>[color_channels, height, width]</code>.</li> <li>Wrong devices - Your model is on the GPU but your data is on the CPU.</li> </ol> </li> </ul>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code in the sections above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 04</li> <li>Example solutions notebook for 04 (try the exercises before looking at this)</li> </ul> <ol> <li>Our models are underperforming (not fitting the data well). What are 3 methods for preventing underfitting? Write them down and explain each with a sentence.</li> <li>Recreate the data loading functions we built in sections 1, 2, 3 and 4. You should have train and test <code>DataLoader</code>'s ready to use.</li> <li>Recreate <code>model_0</code> we built in section 7.</li> <li>Create training and testing functions for <code>model_0</code>.</li> <li>Try training the model you made in exercise 3 for 5, 20 and 50 epochs, what happens to the results?<ul> <li>Use <code>torch.optim.Adam()</code> with a learning rate of 0.001 as the optimizer.</li> </ul> </li> <li>Double the number of hidden units in your model and train it for 20 epochs, what happens to the results?</li> <li>Double the data you're using with your model and train it for 20 epochs, what happens to the results?<ul> <li>Note: You can use the custom data creation notebook to scale up your Food101 dataset.</li> <li>You can also find the already formatted double data (20% instead of 10% subset) dataset on GitHub, you will need to write download code like in exercise 2 to get it into this notebook.</li> </ul> </li> <li>Make a prediction on your own custom image of pizza/steak/sushi (you could even download one from the internet) and share your prediction.<ul> <li>Does the model you trained in exercise 7 get it right?</li> <li>If not, what do you think you could do to improve it?</li> </ul> </li> </ol>"},{"location":"Learn/z2m-pytorch/04_pytorch_custom_datasets/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>To practice your knowledge of PyTorch <code>Dataset</code>'s and <code>DataLoader</code>'s through PyTorch datasets and dataloaders tutorial notebook.</li> <li>Spend 10-minutes reading the PyTorch <code>torchvision.transforms</code> documentation.<ul> <li>You can see demos of transforms in action in the illustrations of transforms tutorial.</li> </ul> </li> <li>Spend 10-minutes reading the PyTorch <code>torchvision.datasets</code> documentation.<ul> <li>What are some datasets that stand out to you?</li> <li>How could you try building a model on these?</li> </ul> </li> <li>TorchData is currently in beta (as of April 2022), it'll be a future way of loading data in PyTorch, but you can start to check it out now.</li> <li>To speed up deep learning models, you can do a few tricks to improve compute, memory and overhead computations, for more read the post Making Deep Learning Go Brrrr From First Principles by Horace He.</li> </ul>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/","title":"05. PyTorch Going Modular","text":"<p>This section answers the question, \"how do I turn my notebook code into Python scripts?\"</p> <p>To do so, we're going to turn the most useful code cells in notebook 04. PyTorch Custom Datasets into a series of Python scripts saved to a directory called <code>going_modular</code>.</p>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#what-is-going-modular","title":"What is going modular?","text":"<p>Going modular involves turning notebook code (from a Jupyter Notebook or Google Colab notebook) into a series of different Python scripts that offer similar functionality.</p> <p>For example, we could turn our notebook code from a series of cells into the following Python files:</p> <ul> <li><code>data_setup.py</code> - a file to prepare and download data if needed.</li> <li><code>engine.py</code> - a file containing various training functions.</li> <li><code>model_builder.py</code> or <code>model.py</code> - a file to create a PyTorch model.</li> <li><code>train.py</code> - a file to leverage all other files and train a target PyTorch model.</li> <li><code>utils.py</code> - a file dedicated to helpful utility functions.</li> </ul> <p>Note: The naming and layout of the above files will depend on your use case and code requirements. Python scripts are as general as individual notebook cells, meaning, you could create one for almost any kind of functionality.</p>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#why-would-you-want-to-go-modular","title":"Why would you want to go modular?","text":"<p>Notebooks are fantastic for iteratively exploring and running experiments quickly.</p> <p>However, for larger scale projects you may find Python scripts more reproducible and easier to run.</p> <p>Though this is a debated topic, as companies like Netflix have shown how they use notebooks for production code.</p> <p>Production code is code that runs to offer a service to someone or something.</p> <p>For example, if you have an app running online that other people can access and use, the code running that app is considered production code.</p> <p>And libraries like fast.ai's <code>nb-dev</code> (short for notebook development) enable you to write whole Python libraries (including documentation) with Jupyter Notebooks.</p>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#pros-and-cons-of-notebooks-vs-python-scripts","title":"Pros and cons of notebooks vs Python scripts","text":"<p>There's arguments for both sides.</p> <p>But this list sums up a few of the main topics.</p> Pros Cons Notebooks Easy to experiment/get started Versioning can be hard Easy to share (e.g. a link to a Google Colab notebook) Hard to use only specific parts Very visual Text and graphics can get in the way of code Pros Cons Python scripts Can package code together (saves rewriting similar code across different notebooks) Experimenting isn't as visual (usually have to run the whole script rather than one cell) Can use git for versioning Many open source projects use scripts Larger projects can be run on cloud vendors (not as much support for notebooks)"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#my-workflow","title":"My workflow","text":"<p>I usually start machine learning projects in Jupyter/Google Colab notebooks for quick experimentation and visualization.</p> <p>Then when I've got something working, I move the most useful pieces of code to Python scripts.</p> <p></p> <p>There are many possible workflows for writing machine learning code. Some prefer to start with scripts, others (like me) prefer to start with notebooks and go to scripts later on.</p>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#pytorch-in-the-wild","title":"PyTorch in the wild","text":"<p>In your travels, you'll see many code repositories for PyTorch-based ML projects have instructions on how to run the PyTorch code in the form of Python scripts.</p> <p>For example, you might be instructed to run code like the following in a terminal/command line to train a model:</p> <pre><code>python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS\n</code></pre> <p> </p> <p>Running a PyTorch <code>train.py</code> script on the command line with various hyperparameter settings.</p> <p>In this case, <code>train.py</code> is the target Python script, it'll likely contain functions to train a PyTorch model.</p> <p>And <code>--model</code>, <code>--batch_size</code>, <code>--lr</code> and <code>--num_epochs</code> are known as argument flags.</p> <p>You can set these to whatever values you like and if they're compatible with <code>train.py</code>, they'll work, if not, they'll error.</p> <p>For example, let's say we wanted to train our TinyVGG model from notebook 04 for 10 epochs with a batch size of 32 and a learning rate of 0.001:</p> <pre><code>python train.py --model tinyvgg --batch_size 32 --lr 0.001 --num_epochs 10\n</code></pre> <p>You could setup any number of these argument flags in your <code>train.py</code> script to suit your needs.</p> <p>The PyTorch blog post for training state-of-the-art computer vision models uses this style.</p> <p></p> <p>PyTorch command line training script recipe for training state-of-the-art computer vision models with 8 GPUs. Source: PyTorch blog.</p>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#what-were-going-to-cover","title":"What we're going to cover","text":"<p>The main concept of this section is: turn useful notebook code cells into reusable Python files.</p> <p>Doing this will save us writing the same code over and over again.</p> <p>There are two notebooks for this section:</p> <ol> <li>05. Going Modular: Part 1 (cell mode) - this notebook is run as a traditional Jupyter Notebook/Google Colab notebook and is a condensed version of notebook 04.</li> <li>05. Going Modular: Part 2 (script mode) - this notebook is the same as number 1 but with added functionality to turn each of the major sections into Python scripts, such as, <code>data_setup.py</code> and <code>train.py</code>. </li> </ol> <p>The text in this document focuses on the code cells 05. Going Modular: Part 2 (script mode), the ones with <code>%%writefile ...</code> at the top.</p>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#why-two-parts","title":"Why two parts?","text":"<p>Because sometimes the best way to learn something is to see how it differs from something else.</p> <p>If you run each notebook side-by-side you'll see how they differ and that's where the key learnings are.</p> <p></p> <p>Running the two notebooks for section 05 side-by-side. You'll notice that the script mode notebook has extra code cells to turn code from the cell mode notebook into Python scripts.</p>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#what-were-working-towards","title":"What we're working towards","text":"<p>By the end of this section we want to have two things:</p> <ol> <li>The ability to train the model we built in notebook 04 (Food Vision Mini) with one line of code on the command line: <code>python train.py</code>.</li> <li>A directory structure of reusable Python scripts, such as: </li> </ol> <pre><code>going_modular/\n\u251c\u2500\u2500 going_modular/\n\u2502   \u251c\u2500\u2500 data_setup.py\n\u2502   \u251c\u2500\u2500 engine.py\n\u2502   \u251c\u2500\u2500 model_builder.py\n\u2502   \u251c\u2500\u2500 train.py\n\u2502   \u2514\u2500\u2500 utils.py\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 05_going_modular_cell_mode_tinyvgg_model.pth\n\u2502   \u2514\u2500\u2500 05_going_modular_script_mode_tinyvgg_model.pth\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 pizza_steak_sushi/\n        \u251c\u2500\u2500 train/\n        \u2502   \u251c\u2500\u2500 pizza/\n        \u2502   \u2502   \u251c\u2500\u2500 image01.jpeg\n        \u2502   \u2502   \u2514\u2500\u2500 ...\n        \u2502   \u251c\u2500\u2500 steak/\n        \u2502   \u2514\u2500\u2500 sushi/\n        \u2514\u2500\u2500 test/\n            \u251c\u2500\u2500 pizza/\n            \u251c\u2500\u2500 steak/\n            \u2514\u2500\u2500 sushi/\n</code></pre>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#things-to-note","title":"Things to note","text":"<ul> <li>Docstrings - Writing reproducible and understandable code is important. And with this in mind, each of the functions/classes we'll be putting into scripts has been created with Google's Python docstring style in mind.</li> <li>Imports at the top of scripts - Since all of the Python scripts we're going to create could be considered a small program on their own, all of the scripts require their input modules be imported at the start of the script for example:</li> </ul> <pre><code># Import modules required for train.py\nimport os\nimport torch\nimport data_setup, engine, model_builder, utils\n\nfrom torchvision import transforms\n</code></pre>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#where-can-you-get-help","title":"Where can you get help?","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch. </p>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#0-cell-mode-vs-script-mode","title":"0. Cell mode vs. script mode","text":"<p>A cell mode notebook such as 05. Going Modular Part 1 (cell mode) is a notebook run normally, each cell in the notebook is either code or markdown.</p> <p>A script mode notebook such as 05. Going Modular Part 2 (script mode) is very similar to a cell mode notebook, however, many of the code cells may be turned into Python scripts.</p> <p>Note: You don't need to create Python scripts via a notebook, you can create them directly through an IDE (integrated developer environment) such as VS Code. Having the script mode notebook as part of this section is just to demonstrate one way of going from notebooks to Python scripts.</p>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#1-get-data","title":"1. Get data","text":"<p>Getting the data in each of the 05 notebooks happens the same as in notebook 04.</p> <p>A call is made to GitHub via Python's <code>requests</code> module to download a <code>.zip</code> file and unzip it.</p> <pre><code>import os\nimport requests\nimport zipfile\nfrom pathlib import Path\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n\n# Download pizza, steak, sushi data\nwith open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n    print(\"Downloading pizza, steak, sushi data...\")\n    f.write(request.content)\n\n# Unzip pizza, steak, sushi data\nwith zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n    print(\"Unzipping pizza, steak, sushi data...\") \n    zip_ref.extractall(image_path)\n\n# Remove zip file\nos.remove(data_path / \"pizza_steak_sushi.zip\")\n</code></pre> <p>This results in having a file called <code>data</code> that contains another directory called <code>pizza_steak_sushi</code> with images of pizza, steak and sushi in standard image classification format.</p> <pre><code>data/\n\u2514\u2500\u2500 pizza_steak_sushi/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 pizza/\n    \u2502   \u2502   \u251c\u2500\u2500 train_image01.jpeg\n    \u2502   \u2502   \u251c\u2500\u2500 test_image02.jpeg\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u251c\u2500\u2500 steak/\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u2514\u2500\u2500 sushi/\n    \u2502       \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 test/\n        \u251c\u2500\u2500 pizza/\n        \u2502   \u251c\u2500\u2500 test_image01.jpeg\n        \u2502   \u2514\u2500\u2500 test_image02.jpeg\n        \u251c\u2500\u2500 steak/\n        \u2514\u2500\u2500 sushi/\n</code></pre>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy","title":"2. Create Datasets and DataLoaders (<code>data_setup.py</code>)","text":"<p>Once we've got data, we can then turn it into PyTorch <code>Dataset</code>'s and <code>DataLoader</code>'s (one for training data and one for testing data).</p> <p>We convert the useful <code>Dataset</code> and <code>DataLoader</code> creation code into a function called <code>create_dataloaders()</code>.</p> <p>And we write it to file using the line <code>%%writefile going_modular/data_setup.py</code>. </p> data_setup.py<pre><code>%%writefile going_modular/data_setup.py\n\"\"\"\nContains functionality for creating PyTorch DataLoaders for \nimage classification data.\n\"\"\"\nimport os\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nNUM_WORKERS = os.cpu_count()\n\ndef create_dataloaders(\n    train_dir: str, \n    test_dir: str, \n    transform: transforms.Compose, \n    batch_size: int, \n    num_workers: int=NUM_WORKERS\n):\n\"\"\"Creates training and testing DataLoaders.\n\n  Takes in a training directory and testing directory path and turns\n  them into PyTorch Datasets and then into PyTorch DataLoaders.\n\n  Args:\n    train_dir: Path to training directory.\n    test_dir: Path to testing directory.\n    transform: torchvision transforms to perform on training and testing data.\n    batch_size: Number of samples per batch in each of the DataLoaders.\n    num_workers: An integer for number of workers per DataLoader.\n\n  Returns:\n    A tuple of (train_dataloader, test_dataloader, class_names).\n    Where class_names is a list of the target classes.\n    Example usage:\n      train_dataloader, test_dataloader, class_names = \\\n        = create_dataloaders(train_dir=path/to/train_dir,\n                             test_dir=path/to/test_dir,\n                             transform=some_transform,\n                             batch_size=32,\n                             num_workers=4)\n  \"\"\"\n  # Use ImageFolder to create dataset(s)\n  train_data = datasets.ImageFolder(train_dir, transform=transform)\n  test_data = datasets.ImageFolder(test_dir, transform=transform)\n\n  # Get class names\n  class_names = train_data.classes\n\n  # Turn images into data loaders\n  train_dataloader = DataLoader(\n      train_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n  test_dataloader = DataLoader(\n      test_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n\n  return train_dataloader, test_dataloader, class_names\n</code></pre> <p>If we'd like to make <code>DataLoader</code>'s we can now use the function within <code>data_setup.py</code> like so:</p> <pre><code># Import data_setup.py\nfrom going_modular import data_setup\n\n# Create train/test dataloader and get class names as a list\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(...)\n</code></pre>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#3-making-a-model-model_builderpy","title":"3. Making a model (<code>model_builder.py</code>)","text":"<p>Over the past few notebooks (notebook 03 and notebook 04), we've built the TinyVGG model a few times.</p> <p>So it makes sense to put the model into its file so we can reuse it again and again.</p> <p>Let's put our <code>TinyVGG()</code> model class into a script with the line <code>%%writefile going_modular/model_builder.py</code>:</p> model_builder.py<pre><code>%%writefile going_modular/model_builder.py\n\"\"\"\nContains PyTorch model code to instantiate a TinyVGG model.\n\"\"\"\nimport torch\nfrom torch import nn \n\nclass TinyVGG(nn.Module):\n\"\"\"Creates the TinyVGG architecture.\n\n  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n\n  Args:\n    input_shape: An integer indicating number of input channels.\n    hidden_units: An integer indicating number of hidden units between layers.\n    output_shape: An integer indicating number of output units.\n  \"\"\"\n  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n      super().__init__()\n      self.conv_block_1 = nn.Sequential(\n          nn.Conv2d(in_channels=input_shape, \n                    out_channels=hidden_units, \n                    kernel_size=3, \n                    stride=1, \n                    padding=0),  \n          nn.ReLU(),\n          nn.Conv2d(in_channels=hidden_units, \n                    out_channels=hidden_units,\n                    kernel_size=3,\n                    stride=1,\n                    padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2,\n                        stride=2)\n      )\n      self.conv_block_2 = nn.Sequential(\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(2)\n      )\n      self.classifier = nn.Sequential(\n          nn.Flatten(),\n          # Where did this in_features shape come from? \n          # It's because each layer of our network compresses and changes the shape of our inputs data.\n          nn.Linear(in_features=hidden_units*13*13,\n                    out_features=output_shape)\n      )\n\n  def forward(self, x: torch.Tensor):\n      x = self.conv_block_1(x)\n      x = self.conv_block_2(x)\n      x = self.classifier(x)\n      return x\n      # return self.classifier(self.block_2(self.block_1(x))) # &lt;- leverage the benefits of operator fusion\n</code></pre> <p>Now instead of coding the TinyVGG model from scratch every time, we can import it using:</p> <pre><code>import torch\n# Import model_builder.py\nfrom going_modular import model_builder\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Instantiate an instance of the model from the \"model_builder.py\" script\ntorch.manual_seed(42)\nmodel = model_builder.TinyVGG(input_shape=3,\n                              hidden_units=10, \n                              output_shape=len(class_names)).to(device)\n</code></pre>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them","title":"4. Creating <code>train_step()</code> and <code>test_step()</code> functions and <code>train()</code> to combine them","text":"<p>We wrote several training functions in notebook 04:</p> <ol> <li><code>train_step()</code> - takes in a model, a <code>DataLoader</code>, a loss function and an optimizer and trains the model on the <code>DataLoader</code>.</li> <li><code>test_step()</code> - takes in a model, a <code>DataLoader</code> and a loss function and evaluates the model on the <code>DataLoader</code>.</li> <li><code>train()</code> - performs 1. and 2. together for a given number of epochs and returns a results dictionary.</li> </ol> <p>Since these will be the engine of our model training, we can put them all into a Python script called <code>engine.py</code> with the line <code>%%writefile going_modular/engine.py</code>:</p> engine.py<pre><code>%%writefile going_modular/engine.py\n\"\"\"\nContains functions for training and testing a PyTorch model.\n\"\"\"\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -&gt; Tuple[float, float]:\n\"\"\"Trains a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to training mode and then\n  runs through all of the required training steps (forward\n  pass, loss calculation, optimizer step).\n\n  Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n  \"\"\"\n  # Put model in train mode\n  model.train()\n\n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n\n  # Loop through data loader data batches\n  for batch, (X, y) in enumerate(dataloader):\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      y_pred = model(X)\n\n      # 2. Calculate  and accumulate loss\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -&gt; Tuple[float, float]:\n\"\"\"Tests a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to \"eval\" mode and then performs\n  a forward pass on a testing dataset.\n\n  Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n  \"\"\"\n  # Put model in eval mode\n  model.eval() \n\n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n\n  # Turn on inference context manager\n  with torch.inference_mode():\n      # Loop through DataLoader batches\n      for batch, (X, y) in enumerate(dataloader):\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n\n          # 1. Forward pass\n          test_pred_logits = model(X)\n\n          # 2. Calculate and accumulate loss\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n\n          # Calculate and accumulate accuracy\n          test_pred_labels = test_pred_logits.argmax(dim=1)\n          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List]:\n\"\"\"Trains and tests a PyTorch model.\n\n  Passes a target PyTorch models through train_step() and test_step()\n  functions for a number of epochs, training and testing the model\n  in the same epoch loop.\n\n  Calculates, prints and stores evaluation metrics throughout.\n\n  Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n  \"\"\"\n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": []\n  }\n\n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs)):\n      train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n      test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n      # Print out what's happening\n      print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n      )\n\n      # Update results dictionary\n      results[\"train_loss\"].append(train_loss)\n      results[\"train_acc\"].append(train_acc)\n      results[\"test_loss\"].append(test_loss)\n      results[\"test_acc\"].append(test_acc)\n\n  # Return the filled results at the end of the epochs\n  return results\n</code></pre> <p>Now we've got the <code>engine.py</code> script, we can import functions from it via:</p> <pre><code># Import engine.py\nfrom going_modular import engine\n\n# Use train() by calling it from engine.py\nengine.train(...)\n</code></pre>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy","title":"5. Creating a function to save the model (<code>utils.py</code>)","text":"<p>Often you'll want to save a model whilst it's training or after training.</p> <p>Since we've written the code to save a model a few times now in previous notebooks, it makes sense to turn it into a function and save it to file.</p> <p>It's common practice to store helper functions in a file called <code>utils.py</code> (short for utilities).</p> <p>Let's save our <code>save_model()</code> function to a file called <code>utils.py</code> with the line <code>%%writefile going_modular/utils.py</code>: </p> utils.py<pre><code>%%writefile going_modular/utils.py\n\"\"\"\nContains various utility functions for PyTorch model training and saving.\n\"\"\"\nimport torch\nfrom pathlib import Path\n\ndef save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n\"\"\"Saves a PyTorch model to a target directory.\n\n  Args:\n    model: A target PyTorch model to save.\n    target_dir: A directory for saving the model to.\n    model_name: A filename for the saved model. Should include\n      either \".pth\" or \".pt\" as the file extension.\n\n  Example usage:\n    save_model(model=model_0,\n               target_dir=\"models\",\n               model_name=\"05_going_modular_tingvgg_model.pth\")\n  \"\"\"\n  # Create target directory\n  target_dir_path = Path(target_dir)\n  target_dir_path.mkdir(parents=True,\n                        exist_ok=True)\n\n  # Create model save path\n  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n  model_save_path = target_dir_path / model_name\n\n  # Save the model state_dict()\n  print(f\"[INFO] Saving model to: {model_save_path}\")\n  torch.save(obj=model.state_dict(),\n             f=model_save_path)\n</code></pre> <p>Now if we wanted to use our <code>save_model()</code> function, instead of writing it all over again, we can import it and use it via:</p> <pre><code># Import utils.py\nfrom going_modular import utils\n\n# Save a model to file\nsave_model(model=...\n           target_dir=...,\n           model_name=...)\n</code></pre>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#6-train-evaluate-and-save-the-model-trainpy","title":"6. Train, evaluate and save the model (<code>train.py</code>)","text":"<p>As previously discussed, you'll often come across PyTorch repositories that combine all of their functionality together in a <code>train.py</code> file.</p> <p>This file is essentially saying \"train the model using whatever data is available\".</p> <p>In our <code>train.py</code> file, we'll combine all of the functionality of the other Python scripts we've created and use it to train a model.</p> <p>This way we can train a PyTorch model using a single line of code on the command line:</p> <pre><code>python train.py\n</code></pre> <p>To create <code>train.py</code> we'll go through the following steps:</p> <ol> <li>Import the various dependencies, namely <code>torch</code>, <code>os</code>, <code>torchvision.transforms</code> and all of the scripts from the <code>going_modular</code> directory, <code>data_setup</code>, <code>engine</code>, <code>model_builder</code>, <code>utils</code>.</li> <li>Note: Since <code>train.py</code> will be inside the <code>going_modular</code> directory, we can import the other modules via <code>import ...</code> rather than <code>from going_modular import ...</code>.</li> <li>Setup various hyperparameters such as batch size, number of epochs, learning rate and number of hidden units (these could be set in the future via Python's <code>argparse</code>).</li> <li>Setup the training and test directories.</li> <li>Setup device-agnostic code.</li> <li>Create the necessary data transforms.</li> <li>Create the DataLoaders using <code>data_setup.py</code>.</li> <li>Create the model using <code>model_builder.py</code>.</li> <li>Setup the loss function and optimizer.</li> <li>Train the model using <code>engine.py</code>.</li> <li>Save the model using <code>utils.py</code>. </li> </ol> <p>And we can create the file from a notebook cell using the line <code>%%writefile going_modular/train.py</code>:</p> train.py<pre><code>%%writefile going_modular/train.py\n\"\"\"\nTrains a PyTorch image classification model using device-agnostic code.\n\"\"\"\n\nimport os\nimport torch\nimport data_setup, engine, model_builder, utils\n\nfrom torchvision import transforms\n\n# Setup hyperparameters\nNUM_EPOCHS = 5\nBATCH_SIZE = 32\nHIDDEN_UNITS = 10\nLEARNING_RATE = 0.001\n\n# Setup directories\ntrain_dir = \"data/pizza_steak_sushi/train\"\ntest_dir = \"data/pizza_steak_sushi/test\"\n\n# Setup target device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Create transforms\ndata_transform = transforms.Compose([\n  transforms.Resize((64, 64)),\n  transforms.ToTensor()\n])\n\n# Create DataLoaders with help from data_setup.py\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=data_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Create model with help from model_builder.py\nmodel = model_builder.TinyVGG(\n    input_shape=3,\n    hidden_units=HIDDEN_UNITS,\n    output_shape=len(class_names)\n).to(device)\n\n# Set loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr=LEARNING_RATE)\n\n# Start training with help from engine.py\nengine.train(model=model,\n             train_dataloader=train_dataloader,\n             test_dataloader=test_dataloader,\n             loss_fn=loss_fn,\n             optimizer=optimizer,\n             epochs=NUM_EPOCHS,\n             device=device)\n\n# Save the model with help from utils.py\nutils.save_model(model=model,\n                 target_dir=\"models\",\n                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")\n</code></pre> <p>Woohoo!</p> <p>Now we can train a PyTorch model by running the following line on the command line:</p> <pre><code>python train.py\n</code></pre> <p>Doing this will leverage all of the other code scripts we've created.</p> <p>And if we wanted to, we could adjust our <code>train.py</code> file to use argument flag inputs with Python's <code>argparse</code> module, this would allow us to provide different hyperparameter settings like previously discussed:</p> <pre><code>python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS\n</code></pre>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#exercises","title":"Exercises","text":"<p>Resources:</p> <ul> <li>Exercise template notebook for 05</li> <li>Example solutions notebook for 05<ul> <li>Live coding run through of solutions notebook for 05 on YouTube</li> </ul> </li> </ul> <p>Exercises:</p> <ol> <li>Turn the code to get the data (from section 1. Get Data above) into a Python script, such as <code>get_data.py</code>.<ul> <li>When you run the script using <code>python get_data.py</code> it should check if the data already exists and skip downloading if it does.</li> <li>If the data download is successful, you should be able to access the <code>pizza_steak_sushi</code> images from the <code>data</code> directory.</li> </ul> </li> <li>Use Python's <code>argparse</code> module to be able to send the <code>train.py</code> custom hyperparameter values for training procedures.<ul> <li>Add an argument for using a different:<ul> <li>Training/testing directory</li> <li>Learning rate</li> <li>Batch size</li> <li>Number of epochs to train for</li> <li>Number of hidden units in the TinyVGG model</li> </ul> </li> <li>Keep the default values for each of the above arguments as what they already are (as in notebook 05).</li> <li>For example, you should be able to run something similar to the following line to train a TinyVGG model with a learning rate of 0.003 and a batch size of 64 for 20 epochs: <code>python train.py --learning_rate 0.003 --batch_size 64 --num_epochs 20</code>.</li> <li>Note: Since <code>train.py</code> leverages the other scripts we created in section 05, such as, <code>model_builder.py</code>, <code>utils.py</code> and <code>engine.py</code>, you'll have to make sure they're available to use too. You can find these in the <code>going_modular</code> folder on the course GitHub. </li> </ul> </li> <li>Create a script to predict (such as <code>predict.py</code>) on a target image given a file path with a saved model.<ul> <li>For example, you should be able to run the command <code>python predict.py some_image.jpeg</code> and have a trained PyTorch model predict on the image and return its prediction.</li> <li>To see example prediction code, check out the predicting on a custom image section in notebook 04. </li> <li>You may also have to write code to load in a trained model.</li> </ul> </li> </ol>"},{"location":"Learn/z2m-pytorch/05_pytorch_going_modular/#extra-curriculum","title":"Extra-curriculum","text":"<ul> <li>To learn more about structuring a Python project, check out Real Python's guide on Python Application Layouts. </li> <li>For ideas on styling your PyTorch code, check out the PyTorch style guide by Igor Susmelj (much of styling in this chapter is based off this guide + various similar PyTorch repositories).</li> <li>For an example <code>train.py</code> script and various other PyTorch scripts written by the PyTorch team to train state-of-the-art image classification models, check out their <code>classification</code> repository on GitHub. </li> </ul>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/","title":"06. PyTorch Transfer Learning","text":"In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>torch version: 1.13.0.dev20220620+cu113\ntorchvision version: 0.14.0.dev20220620+cu113\n</pre> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine <p>Now let's setup device agnostic code.</p> <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>.</p> In\u00a0[3]: Copied! <pre># Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Setup device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre>import os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n\n    # Remove .zip file\n    os.remove(data_path / \"pizza_steak_sushi.zip\")\n</pre> import os import zipfile  from pathlib import Path  import requests  # Setup path to data folder data_path = Path(\"data/\") image_path = data_path / \"pizza_steak_sushi\"  # If the image folder doesn't exist, download it and prepare it...  if image_path.is_dir():     print(f\"{image_path} directory exists.\") else:     print(f\"Did not find {image_path} directory, creating one...\")     image_path.mkdir(parents=True, exist_ok=True)          # Download pizza, steak, sushi data     with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:         request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")         print(\"Downloading pizza, steak, sushi data...\")         f.write(request.content)      # Unzip pizza, steak, sushi data     with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:         print(\"Unzipping pizza, steak, sushi data...\")          zip_ref.extractall(image_path)      # Remove .zip file     os.remove(data_path / \"pizza_steak_sushi.zip\") <pre>data/pizza_steak_sushi directory exists.\n</pre> <p>Excellent!</p> <p>Now we've got the same dataset we've been using previously, a series of images of pizza, steak and sushi in standard image classification format.</p> <p>Let's now create paths to our training and test directories.</p> In\u00a0[5]: Copied! <pre># Setup Dirs\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n</pre> # Setup Dirs train_dir = image_path / \"train\" test_dir = image_path / \"test\" In\u00a0[6]: Copied! <pre># Create a transforms pipeline manually (required for torchvision &lt; 0.13)\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n    transforms.ToTensor(), # 2. Turn image values to between 0 &amp; 1 \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n])\n</pre> # Create a transforms pipeline manually (required for torchvision &lt; 0.13) manual_transforms = transforms.Compose([     transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)     transforms.ToTensor(), # 2. Turn image values to between 0 &amp; 1      transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)                          std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel), ]) <p>Wonderful!</p> <p>Now we've got a manually created series of transforms ready to prepare our images, let's create training and testing DataLoaders.</p> <p>We can create these using the <code>create_dataloaders</code> function from the <code>data_setup.py</code> script we created in 05. PyTorch Going Modular Part 2.</p> <p>We'll set <code>batch_size=32</code> so our model see's mini-batches of 32 samples at a time.</p> <p>And we can transform our images using the transform pipeline we created above by setting <code>transform=simple_transform</code>.</p> <p>Note: I've included this manual creation of transforms in this notebook because you may come across resources that use this style. It's also important to note that because these transforms are manually created, they're also infinitely customizable. So if you wanted to included data augmentation techniques in your transforms pipeline, you could.</p> In\u00a0[7]: Copied! <pre># Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=manual_transforms, # resize, convert images to between 0 &amp; 1 and normalize them\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Create training and testing DataLoaders as well as get a list of class names train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                test_dir=test_dir,                                                                                transform=manual_transforms, # resize, convert images to between 0 &amp; 1 and normalize them                                                                                batch_size=32) # set mini-batch size to 32  train_dataloader, test_dataloader, class_names Out[7]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa9429a3a60&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fa9429a37c0&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[8]: Copied! <pre># Get a set of pretrained model weights\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\nweights\n</pre> # Get a set of pretrained model weights weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet weights Out[8]: <pre>EfficientNet_B0_Weights.IMAGENET1K_V1</pre> <p>And now to access the transforms assosciated with our <code>weights</code>, we can use the <code>transforms()</code> method.</p> <p>This is essentially saying \"get the data transforms that were used to train the <code>EfficientNet_B0_Weights</code> on ImageNet\".</p> In\u00a0[9]: Copied! <pre># Get the transforms used to create our pretrained weights\nauto_transforms = weights.transforms()\nauto_transforms\n</pre> # Get the transforms used to create our pretrained weights auto_transforms = weights.transforms() auto_transforms Out[9]: <pre>ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)</pre> <p>Notice how <code>auto_transforms</code> is very similar to <code>manual_transforms</code>, the only difference is that <code>auto_transforms</code> came with the model architecture we chose, where as we had to create <code>manual_transforms</code> by hand.</p> <p>The benefit of automatically creating a transform through <code>weights.transforms()</code> is that you ensure you're using the same data transformation as the pretrained model used when it was trained.</p> <p>However, the tradeoff of using automatically created transforms is a lack of customization.</p> <p>We can use <code>auto_transforms</code> to create DataLoaders with <code>create_dataloaders()</code> just as before.</p> In\u00a0[10]: Copied! <pre># Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Create training and testing DataLoaders as well as get a list of class names train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                test_dir=test_dir,                                                                                transform=auto_transforms, # perform same data transforms on our own data as the pretrained model                                                                                batch_size=32) # set mini-batch size to 32  train_dataloader, test_dataloader, class_names Out[10]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa942951460&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fa942951550&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[11]: Copied! <pre># OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n\n# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights \nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n#model # uncomment to output (it's very long)\n</pre> # OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13) # model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)  # NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+) weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights  model = torchvision.models.efficientnet_b0(weights=weights).to(device)  #model # uncomment to output (it's very long) <p>Note: In previous versions of <code>torchvision</code>, you'd create a prertained model with code like:</p> <p><code>model = torchvision.models.efficientnet_b0(pretrained=True).to(device)</code></p> <p>However, running this using <code>torchvision</code> v0.13+ will result in errors such as the following:</p> <p><code>UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.</code></p> <p>And...</p> <p><code>UserWarning: Arguments other than a weight enum or None for weights are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing weights=EfficientNet_B0_Weights.IMAGENET1K_V1. You can also use weights=EfficientNet_B0_Weights.DEFAULT to get the most up-to-date weights.</code></p> <p>If we print the model, we get something similar to the following:</p> <p>Lots and lots and lots of layers.</p> <p>This is one of the benefits of transfer learning, taking an existing model, that's been crafted by some of the best engineers in the world and applying to your own problem.</p> <p>Our <code>efficientnet_b0</code> comes in three main parts:</p> <ol> <li><code>features</code> - A collection of convolutional layers and other various activation layers to learn a base representation of vision data (this base representation/collection of layers is often referred to as features or feature extractor, \"the base layers of the model learn the different features of images\").</li> <li><code>avgpool</code> - Takes the average of the output of the <code>features</code> layer(s) and turns it into a feature vector.</li> <li><code>classifier</code> - Turns the feature vector into a vector with the same dimensionality as the number of required output classes (since <code>efficientnet_b0</code> is pretrained on ImageNet and because ImageNet has 1000 classes, <code>out_features=1000</code> is the default).</li> </ol> In\u00a0[12]: Copied! <pre># Print a summary using torchinfo (uncomment for actual output)\nsummary(model=model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n</pre> # Print a summary using torchinfo (uncomment for actual output) summary(model=model,          input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"         # col_names=[\"input_size\"], # uncomment for smaller output         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"] )  Out[12]: <pre>============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   True\n\u251c\u2500Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   True\n\u2502    \u2514\u2500Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   True\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   864                  True\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   64                   True\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n\u2502    \u2514\u2500Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   1,448                True\n\u2502    \u2514\u2500Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     6,004                True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\n\u2502    \u2514\u2500Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     15,350               True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     31,290               True\n\u2502    \u2514\u2500Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     37,130               True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n\u2502    \u2514\u2500Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    126,004              True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n\u2502    \u2514\u2500Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      262,492              True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n\u2502    \u2502    \u2514\u2500MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n\u2502    \u2514\u2500Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      717,232              True\n\u2502    \u2514\u2500Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   True\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     409,600              True\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     2,560                True\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n\u251c\u2500AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n\u251c\u2500Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   True\n\u2502    \u2514\u2500Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n\u2502    \u2514\u2500Linear (1)                                            [32, 1280]           [32, 1000]           1,281,000            True\n============================================================================================================================================\nTotal params: 5,288,548\nTrainable params: 5,288,548\nNon-trainable params: 0\nTotal mult-adds (G): 12.35\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3452.35\nParams size (MB): 21.15\nEstimated Total Size (MB): 3492.77\n============================================================================================================================================</pre> <p>Woah!</p> <p>Now that's a big model!</p> <p>From the output of the summary, we can see all of the various input and output shape changes as our image data goes through the model.</p> <p>And there are a whole bunch more total parameters (pretrained weights) to recognize different patterns in our data.</p> <p>For reference, our model from previous sections, TinyVGG had 8,083 parameters vs. 5,288,548 parameters for <code>efficientnet_b0</code>, an increase of ~654x!</p> <p>What do you think, will this mean better performance?</p> In\u00a0[13]: Copied! <pre># Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\nfor param in model.features.parameters():\n    param.requires_grad = False\n</pre> # Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False for param in model.features.parameters():     param.requires_grad = False <p>Feature extractor layers frozen!</p> <p>Let's now adjust the output layer or the <code>classifier</code> portion of our pretrained model to our needs.</p> <p>Right now our pretrained model has <code>out_features=1000</code> because there are 1000 classes in ImageNet.</p> <p>However, we don't have 1000 classes, we only have three, pizza, steak and sushi.</p> <p>We can change the <code>classifier</code> portion of our model by creating a new series of layers.</p> <p>The current <code>classifier</code> consists of:</p> <pre><code>(classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=True)\n    (1): Linear(in_features=1280, out_features=1000, bias=True)\n</code></pre> <p>We'll keep the <code>Dropout</code> layer the same using <code>torch.nn.Dropout(p=0.2, inplace=True)</code>.</p> <p>Note: Dropout layers randomly remove connections between two neural network layers with a probability of <code>p</code>. For example, if <code>p=0.2</code>, 20% of connections between neural network layers will be removed at random each pass. This practice is meant to help regularize (prevent overfitting) a model by making sure the connections that remain learn features to compensate for the removal of the other connections (hopefully these remaining features are more general).</p> <p>And we'll keep <code>in_features=1280</code> for our <code>Linear</code> output layer but we'll change the <code>out_features</code> value to the length of our <code>class_names</code> (<code>len(['pizza', 'steak', 'sushi']) = 3</code>).</p> <p>Our new <code>classifier</code> layer should be on the same device as our <code>model</code>.</p> In\u00a0[14]: Copied! <pre># Set the manual seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Get the length of class_names (one output unit for each class)\noutput_shape = len(class_names)\n\n# Recreate the classifier layer and seed it to the target device\nmodel.classifier = torch.nn.Sequential(\n    torch.nn.Dropout(p=0.2, inplace=True), \n    torch.nn.Linear(in_features=1280, \n                    out_features=output_shape, # same number of output units as our number of classes\n                    bias=True)).to(device)\n</pre> # Set the manual seeds torch.manual_seed(42) torch.cuda.manual_seed(42)  # Get the length of class_names (one output unit for each class) output_shape = len(class_names)  # Recreate the classifier layer and seed it to the target device model.classifier = torch.nn.Sequential(     torch.nn.Dropout(p=0.2, inplace=True),      torch.nn.Linear(in_features=1280,                      out_features=output_shape, # same number of output units as our number of classes                     bias=True)).to(device) <p>Nice!</p> <p>Output layer updated, let's get another summary of our model and see what's changed.</p> In\u00a0[15]: Copied! <pre># # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\nsummary(model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n        verbose=0,\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n</pre> # # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output) summary(model,          input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)         verbose=0,         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"] ) Out[15]: <pre>============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n\u251c\u2500Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n\u2502    \u2514\u2500Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n\u2502    \u2514\u2500Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n\u2502    \u2514\u2500Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n\u2502    \u2514\u2500Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n\u2502    \u2514\u2500Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n\u2502    \u2514\u2500Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n\u2502    \u2514\u2500Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n\u2502    \u2502    \u2514\u2500MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n\u2502    \u2514\u2500Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n\u2502    \u2514\u2500Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n\u251c\u2500AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n\u251c\u2500Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n\u2502    \u2514\u2500Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n\u2502    \u2514\u2500Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n============================================================================================================================================\nTotal params: 4,011,391\nTrainable params: 3,843\nNon-trainable params: 4,007,548\nTotal mult-adds (G): 12.31\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3452.09\nParams size (MB): 16.05\nEstimated Total Size (MB): 3487.41\n============================================================================================================================================</pre> <p>Ho, ho! There's a fair few changes here!</p> <p>Let's go through them:</p> <ul> <li>Trainable column - You'll see that many of the base layers (the ones in the <code>features</code> portion) have their Trainable value as <code>False</code>. This is because we set their attribute <code>requires_grad=False</code>. Unless we change this, these layers won't be updated during furture training.</li> <li>Output shape of <code>classifier</code> - The <code>classifier</code> portion of the model now has an Output Shape value of <code>[32, 3]</code> instead of <code>[32, 1000]</code>. It's Trainable value is also <code>True</code>. This means its parameters will be updated during training. In essence, we're using the <code>features</code> portion to feed our <code>classifier</code> portion a base representation of an image and then our <code>classifier</code> layer is going to learn how to base representation aligns with our problem.</li> <li>Less trainable parameters - Previously there was 5,288,548 trainable parameters. But since we froze many of the layers of the model and only left the <code>classifier</code> as trainable, there's now only 3,843 trainable parameters (even less than our TinyVGG model). Though there's also 4,007,548 non-trainable parameters, these will create a base representation of our input images to feed into our <code>classifier</code> layer.</li> </ul> <p>Note: The more trainable parameters a model has, the more compute power/longer it takes to train. Freezing the base layers of our model and leaving it with less trainable parameters means our model should train quite quickly. This is one huge benefit of transfer learning, taking the already learned parameters of a model trained on a problem similar to yours and only tweaking the outputs slightly to suit your problem.</p> In\u00a0[16]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) <p>Wonderful!</p> <p>To train our model, we can use <code>train()</code> function we defined in the 05. PyTorch Going Modular section 04.</p> <p>The <code>train()</code> function is in the <code>engine.py</code> script inside the <code>going_modular</code> directory.</p> <p>Let's see how long it takes to train our model for 5 epochs.</p> <p>Note: We're only going to be training the parameters <code>classifier</code> here as all of the other parameters in our model have been frozen.</p> In\u00a0[17]: Copied! <pre># Set the random seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Setup training and save the results\nresults = engine.train(model=model,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=5,\n                       device=device)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Set the random seeds torch.manual_seed(42) torch.cuda.manual_seed(42)  # Start the timer from timeit import default_timer as timer  start_time = timer()  # Setup training and save the results results = engine.train(model=model,                        train_dataloader=train_dataloader,                        test_dataloader=test_dataloader,                        optimizer=optimizer,                        loss_fn=loss_fn,                        epochs=5,                        device=device)  # End the timer and print out how long it took end_time = timer() print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9133 | test_acc: 0.5398\nEpoch: 2 | train_loss: 0.8717 | train_acc: 0.7773 | test_loss: 0.7912 | test_acc: 0.8153\nEpoch: 3 | train_loss: 0.7648 | train_acc: 0.7930 | test_loss: 0.7463 | test_acc: 0.8561\nEpoch: 4 | train_loss: 0.7108 | train_acc: 0.7539 | test_loss: 0.6372 | test_acc: 0.8655\nEpoch: 5 | train_loss: 0.6254 | train_acc: 0.7852 | test_loss: 0.6260 | test_acc: 0.8561\n[INFO] Total training time: 8.977 seconds\n</pre> <p>Wow!</p> <p>Our model trained quite fast (~5 seconds on my local machine with a NVIDIA TITAN RTX GPU/about 15 seconds on Google Colab with a NVIDIA P100 GPU).</p> <p>And it looks like it smashed our previous model results out of the park!</p> <p>With an <code>efficientnet_b0</code> backbone, our model achieves almost 85%+ accuracy on the test dataset, almost double what we were able to achieve with TinyVGG.</p> <p>Not bad for a model we downloaded with a few lines of code.</p> In\u00a0[18]: Copied! <pre># Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it\ntry:\n    from helper_functions import plot_loss_curves\nexcept:\n    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n    with open(\"helper_functions.py\", \"wb\") as f:\n        import requests\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n        f.write(request.content)\n    from helper_functions import plot_loss_curves\n\n# Plot the loss curves of our model\nplot_loss_curves(results)\n</pre> # Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it try:     from helper_functions import plot_loss_curves except:     print(\"[INFO] Couldn't find helper_functions.py, downloading...\")     with open(\"helper_functions.py\", \"wb\") as f:         import requests         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")         f.write(request.content)     from helper_functions import plot_loss_curves  # Plot the loss curves of our model plot_loss_curves(results) <p>Those are some excellent looking loss curves!</p> <p>It looks like the loss for both datasets (train and test) is heading in the right direction.</p> <p>The same with the accuracy values, trending upwards.</p> <p>That goes to show the power of transfer learning. Using a pretrained model often leads to pretty good results with a small amount of data in less time.</p> <p>I wonder what would happen if you tried to train the model for longer? Or if we added more data?</p> <p>Question: Looking at the loss curves, does our model look like it's overfitting or underfitting? Or perhaps neither? Hint: Check out notebook 04. PyTorch Custom Datasets part 8. What should an ideal loss curve look like? for ideas.</p> In\u00a0[19]: Copied! <pre>from typing import List, Tuple\n\nfrom PIL import Image\n\n# 1. Take in a trained model, class names, image path, image size, a transform and target device\ndef pred_and_plot_image(model: torch.nn.Module,\n                        image_path: str, \n                        class_names: List[str],\n                        image_size: Tuple[int, int] = (224, 224),\n                        transform: torchvision.transforms = None,\n                        device: torch.device=device):\n    \n    \n    # 2. Open image\n    img = Image.open(image_path)\n\n    # 3. Create transformation for image (if one doesn't exist)\n    if transform is not None:\n        image_transform = transform\n    else:\n        image_transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n        ])\n\n    ### Predict on image ### \n\n    # 4. Make sure the model is on the target device\n    model.to(device)\n\n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n      transformed_image = image_transform(img).unsqueeze(dim=0)\n\n      # 7. Make a prediction on image with an extra dimension and send it to the target device\n      target_image_pred = model(transformed_image.to(device))\n\n    # 8. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 9. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    # 10. Plot image with predicted label and probability \n    plt.figure()\n    plt.imshow(img)\n    plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n    plt.axis(False);\n</pre> from typing import List, Tuple  from PIL import Image  # 1. Take in a trained model, class names, image path, image size, a transform and target device def pred_and_plot_image(model: torch.nn.Module,                         image_path: str,                          class_names: List[str],                         image_size: Tuple[int, int] = (224, 224),                         transform: torchvision.transforms = None,                         device: torch.device=device):               # 2. Open image     img = Image.open(image_path)      # 3. Create transformation for image (if one doesn't exist)     if transform is not None:         image_transform = transform     else:         image_transform = transforms.Compose([             transforms.Resize(image_size),             transforms.ToTensor(),             transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225]),         ])      ### Predict on image ###       # 4. Make sure the model is on the target device     model.to(device)      # 5. Turn on model evaluation mode and inference mode     model.eval()     with torch.inference_mode():       # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])       transformed_image = image_transform(img).unsqueeze(dim=0)        # 7. Make a prediction on image with an extra dimension and send it to the target device       target_image_pred = model(transformed_image.to(device))      # 8. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)     target_image_pred_probs = torch.softmax(target_image_pred, dim=1)      # 9. Convert prediction probabilities -&gt; prediction labels     target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)      # 10. Plot image with predicted label and probability      plt.figure()     plt.imshow(img)     plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")     plt.axis(False); <p>What a good looking function!</p> <p>Let's test it out by making predictions on a few random images from the test set.</p> <p>We can get a list of all the test image paths using <code>list(Path(test_dir).glob(\"*/*.jpg\"))</code>, the stars in the <code>glob()</code> method say \"any file matching this pattern\", in other words, any file ending in <code>.jpg</code> (all of our images).</p> <p>And then we can randomly sample a number of these using Python's <code>random.sample(populuation, k)</code> where <code>population</code> is the sequence to sample and <code>k</code> is the number of samples to retrieve.</p> In\u00a0[20]: Copied! <pre># Get a random list of image paths from test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data \ntest_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n\n# Make predictions on and plot the images\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=model, \n                        image_path=image_path,\n                        class_names=class_names,\n                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n                        image_size=(224, 224))\n</pre> # Get a random list of image paths from test set import random num_images_to_plot = 3 test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data  test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths                                        k=num_images_to_plot) # randomly select 'k' image paths to pred and plot  # Make predictions on and plot the images for image_path in test_image_path_sample:     pred_and_plot_image(model=model,                          image_path=image_path,                         class_names=class_names,                         # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights                         image_size=(224, 224)) <p>Woohoo!</p> <p>Those predictions look far better than the ones our TinyVGG model was previously making.</p> In\u00a0[21]: Copied! <pre># Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> # Download custom image import requests  # Setup custom image path custom_image_path = data_path / \"04-pizza-dad.jpeg\"  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predict on custom image pred_and_plot_image(model=model,                     image_path=custom_image_path,                     class_names=class_names) <pre>data/04-pizza-dad.jpeg already exists, skipping download.\n</pre> <p>Two thumbs up!</p> <p>Looks like our model go it right again!</p> <p>But this time the prediction probability is higher than the one from TinyVGG (<code>0.373</code>) in 04. PyTorch Custom Datasets section 11.3.</p> <p>This indicates our <code>efficientnet_b0</code> model is more confident in its prediction where as our TinyVGG model was par with just guessing.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#06-pytorch-transfer-learning","title":"06. PyTorch Transfer Learning\u00b6","text":"<p>Note: This notebook uses <code>torchvision</code>'s upcoming multi-weight support API (coming in <code>torchvision</code> v0.13).</p> <p>As of June 2022, it requires the nightly versions of PyTorch and <code>torchvision</code> be installed.</p> <p>We've built a few models by hand so far.</p> <p>But their performance has been poor.</p> <p>You might be thinking, is there a well-performing model that already exists for our problem?</p> <p>And in the world of deep learning, the answer is often yes.</p> <p>We'll see how by using a powerful technique called transfer learning.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#what-is-transfer-learning","title":"What is transfer learning?\u00b6","text":"<p>Transfer learning allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.</p> <p>For example, we can take the patterns a computer vision model has learned from datasets such as ImageNet (millions of images of different objects) and use them to power our FoodVision Mini model.</p> <p>Or we could take the patterns from a language model (a model that's been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.</p> <p>The premise remains: find a well-performing existing model and apply it to your own problem.</p> <p>Example of transfer learning being applied to computer vision and natural language processing (NLP). In the case of computer vision, a computer vision model might learn patterns on millions of images in ImageNet and then use those patterns to infer on another problem. And for NLP, a language model may learn the structure of language by reading all of Wikipedia (and perhaps more) and then apply that knowledge to a different problem.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#why-use-transfer-learning","title":"Why use transfer learning?\u00b6","text":"<p>There are two main benefits to using transfer learning:</p> <ol> <li>Can leverage an existing model (usually a neural network architecture) proven to work on problems similar to our own.</li> <li>Can leverage a working model which has already learned patterns on similar data to our own. This often results in achieving great results with less custom data.</li> </ol> <p>We'll be putting these to the test for our FoodVision Mini problem, we'll take a computer vision model pretrained on ImageNet and try to leverage its underlying learned representations for classifying images of pizza, steak and sushi.</p> <p>Both research and practice support the use of transfer learning too.</p> <p>A finding from a recent machine learning research paper recommended practioner's use transfer learning wherever possible.</p> <p></p> <p>A study into the effects of whether training from scratch or using transfer learning was better from a practioner's point of view, found transfer learning to be far more beneficial in terms of cost and time. Source: How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers paper section 6 (conclusion).</p> <p>And Jeremy Howard (founder of fastai) is a big proponent of transfer learning.</p> <p>The things that really make a difference (transfer learning), if we can do better at transfer learning, it\u2019s this world changing thing. Suddenly lots more people can do world-class work with less resources and less data. \u2014 Jeremy Howard on the Lex Fridman Podcast</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#where-to-find-pretrained-models","title":"Where to find pretrained models\u00b6","text":"<p>The world of deep learning is an amazing place.</p> <p>So amazing that many people around the world share their work.</p> <p>Often, code and pretrained models for the latest state-of-the-art research is released within a few days of publishing.</p> <p>And there are several places you can find pretrained models to use for your own problems.</p> Location What's there? Link(s) PyTorch domain libraries Each of the PyTorch domain libraries (<code>torchvision</code>, <code>torchtext</code>) come with pretrained models of some form. The models there work right within PyTorch. <code>torchvision.models</code>, <code>torchtext.models</code>, <code>torchaudio.models</code>, <code>torchrec.models</code> HuggingFace Hub A series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There's plenty of different datasets too. https://huggingface.co/models, https://huggingface.co/datasets <code>timm</code> (PyTorch Image Models) library Almost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features. https://github.com/rwightman/pytorch-image-models Paperswithcode A collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks. https://paperswithcode.com/ <p>With access to such high-quality resources as above, it should be common practice at the start of every deep learning problem you take on to ask, \"Does a pretrained model exist for my problem?\"</p> <p>Exercise: Spend 5-minutes going through <code>torchvision.models</code> as well as the HuggingFace Hub Models page, what do you find? (there's no right answers here, it's just to practice exploring)</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to take a pretrained model from <code>torchvision.models</code> and customise it to work on (and hopefully improve) our FoodVision Mini problem.</p> Topic Contents 0. Getting setup We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. 1. Get data Let's get the pizza, steak and sushi image classification dataset we've been using to try and improve our model's results. 2. Create Datasets and DataLoaders We'll use the <code>data_setup.py</code> script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. 3. Get and customise a pretrained model Here we'll download a pretrained model from <code>torchvision.models</code> and customise it to our own problem. 4. Train model Let's see how the new pretrained model goes on our pizza, steak, sushi dataset. We'll use the training functions we created in the previous chapter. 5. Evaluate the model by plotting loss curves How did our first transfer learning model go? Did it overfit or underfit? 6. Make predictions on images from the test set It's one thing to check out a model's evaluation metrics but it's another thing to view its predictions on test samples, let's visualize, visualize, visualize!"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#0-getting-setup","title":"0. Getting setup\u00b6","text":"<p>Let's get started by importing/downloading the required modules for this section.</p> <p>To save us writing extra code, we're going to be leveraging some of the Python scripts (such as <code>data_setup.py</code> and <code>engine.py</code>) we created in the previous section, 05. PyTorch Going Modular.</p> <p>Specifically, we're going to download the <code>going_modular</code> directory from the <code>pytorch-deep-learning</code> repository (if we don't already have it).</p> <p>We'll also get the <code>torchinfo</code> package if it's not available.</p> <p><code>torchinfo</code> will help later on to give us a visual representation of our model.</p> <p>Note: As of June 2022, this notebook uses the nightly versions of <code>torch</code> and <code>torchvision</code> as <code>torchvision</code> v0.13+ is required for using the updated multi-weights API. You can install these using the command below.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#1-get-data","title":"1. Get data\u00b6","text":"<p>Before we can start to use transfer learning, we'll need a dataset.</p> <p>To see how transfer learning compares to our previous attempts at model building, we'll download the same dataset we've been using for FoodVision Mini.</p> <p>Let's write some code to download the <code>pizza_steak_sushi.zip</code> dataset from the course GitHub and then unzip it.</p> <p>We can also make sure if we've already got the data, it doesn't redownload.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#2-create-datasets-and-dataloaders","title":"2. Create Datasets and DataLoaders\u00b6","text":"<p>Since we've downloaded the <code>going_modular</code> directory, we can use the <code>data_setup.py</code> script we created in section 05. PyTorch Going Modular to prepare and setup our DataLoaders.</p> <p>But since we'll be using a pretrained model from <code>torchvision.models</code>, there's a specific transform we need to prepare our images first.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#21-creating-a-transform-for-torchvisionmodels-manual-creation","title":"2.1 Creating a transform for <code>torchvision.models</code> (manual creation)\u00b6","text":"<p>Note: As of <code>torchvision</code> v0.13+, there's an update to how data transforms can be created using <code>torchvision.models</code>. I've called the previous method \"manual creation\" and the new method \"auto creation\". This notebook showcases both.</p> <p>When using a pretrained model, it's important that your custom data going into the model is prepared in the same way as the original training data that went into the model.</p> <p>Prior to <code>torchvision</code> v0.13+, to create a transform for a pretrained model in <code>torchvision.models</code>, the documentation stated:</p> <p>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.</p> <p>The images have to be loaded in to a range of <code>[0, 1]</code> and then normalized using <code>mean = [0.485, 0.456, 0.406]</code> and <code>std = [0.229, 0.224, 0.225]</code>.</p> <p>You can use the following transform to normalize:</p> <pre><code>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n</code></pre> <p>The good news is, we can achieve the above transformations with a combination of:</p> Transform number Transform required Code to perform transform 1 Mini-batches of size <code>[batch_size, 3, height, width]</code> where height and width are at least 224x224^. <code>torchvision.transforms.Resize()</code> to resize images into <code>[3, 224, 224]</code>^ and <code>torch.utils.data.DataLoader()</code> to create batches of images. 2 Values between 0 &amp; 1. <code>torchvision.transforms.ToTensor()</code> 3 A mean of <code>[0.485, 0.456, 0.406]</code> (values across each colour channel). <code>torchvision.transforms.Normalize(mean=...)</code> to adjust the mean of our images. 4 A standard deviation of <code>[0.229, 0.224, 0.225]</code> (values across each colour channel). <code>torchvision.transforms.Normalize(std=...)</code> to adjust the standard deviation of our images. <p>Note: ^some pretrained models from <code>torchvision.models</code> in different sizes to <code>[3, 224, 224]</code>, for example, some might take them in <code>[3, 240, 240]</code>. For specific input image sizes, see the documentation.</p> <p>Question: Where did the mean and standard deviation values come from? Why do we need to do this?</p> <p>These were calculated from the data. Specifically, the ImageNet dataset by taking the means and standard deviations across a subset of images.</p> <p>We also don't need to do this. Neural networks are usually quite capable of figuring out appropriate data distributions (they'll calculate where the mean and standard deviations need to be on their own) but setting them at the start can help our networks achieve better performance quicker.</p> <p>Let's compose a series of <code>torchvision.transforms</code> to perform the above steps.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#22-creating-a-transform-for-torchvisionmodels-auto-creation","title":"2.2 Creating a transform for <code>torchvision.models</code> (auto creation)\u00b6","text":"<p>As previously stated, when using a pretrained model, it's important that your custom data going into the model is prepared in the same way as the original training data that went into the model.</p> <p>Above we saw how to manually create a transform for a pretrained model.</p> <p>But as of <code>torchvision</code> v0.13+, an automatic transform creation feature has been added.</p> <p>When you setup a model from <code>torchvision.models</code> and select the pretrained model weights you'd like to use, for example, say we'd like to use:</p> <pre>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n</pre> <p>Where,</p> <ul> <li><code>EfficientNet_B0_Weights</code> is the model architecture weights we'd like to use (there are many differnt model architecture options in <code>torchvision.models</code>).</li> <li><code>DEFAULT</code> means the best available weights (the best performance in ImageNet).<ul> <li>Note: Depending on the model architecture you choose, you may also see other options such as <code>IMAGENET_V1</code> and <code>IMAGENET_V2</code> where generally the higher version number the better. Though if you want the best available, <code>DEFAULT</code> is the easiest option. See the <code>torchvision.models</code> documentation for more.</li> </ul> </li> </ul> <p>Let's try it out.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#3-getting-a-pretrained-model","title":"3. Getting a pretrained model\u00b6","text":"<p>Alright, here comes the fun part!</p> <p>Over the past few notebooks we've been building PyTorch neural networks from scratch.</p> <p>And while that's a good skill to have, our models haven't been performing as well as we'd like.</p> <p>That's where transfer learning comes in.</p> <p>The whole idea of transfer learning is to take an already well-performing model on a problem-space similar to yours and then customising it to your use case.</p> <p>Since we're working on a computer vision problem (image classification with FoodVision Mini), we can find pretrained classification models in <code>torchvision.models</code>.</p> <p>Exploring the documentation, you'll find plenty of common computer vision architecture backbones such as:</p> Architecuture backbone Code ResNet's <code>torchvision.models.resnet18()</code>, <code>torchvision.models.resnet50()</code>... VGG (similar to what we used for TinyVGG) <code>torchvision.models.vgg16()</code> EfficientNet's <code>torchvision.models.efficientnet_b0()</code>, <code>torchvision.models.efficientnet_b1()</code>... VisionTransformer (ViT's) <code>torchvision.models.vit_b_16()</code>, <code>torchvision.models.vit_b_32()</code>... ConvNeXt <code>torchvision.models.convnext_tiny()</code>,  <code>torchvision.models.convnext_small()</code>... More available in <code>torchvision.models</code> <code>torchvision.models...</code>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#31-which-pretrained-model-should-you-use","title":"3.1 Which pretrained model should you use?\u00b6","text":"<p>It depends on your problem/the device you're working with.</p> <p>Generally, the higher number in the model name (e.g. <code>efficientnet_b0()</code> -&gt; <code>efficientnet_b1()</code> -&gt; <code>efficientnet_b7()</code>) means better performance but a larger model.</p> <p>You might think better performance is always better, right?</p> <p>That's true but some better performing models are too big for some devices.</p> <p>For example, say you'd like to run your model on a mobile-device, you'll have to take into account the limited compute resources on the device, thus you'd be looking for a smaller model.</p> <p>But if you've got unlimited compute power, as The Bitter Lesson states, you'd likely take the biggest, most compute hungry model you can.</p> <p>Understanding this performance vs. speed vs. size tradeoff will come with time and practice.</p> <p>For me, I've found a nice balance in the <code>efficientnet_bX</code> models.</p> <p>As of May 2022, Nutrify (the machine learning powered app I'm working on) is powered by an <code>efficientnet_b0</code>.</p> <p>Comma.ai (a company that makes open source self-driving car software) uses an <code>efficientnet_b2</code> to learn a representation of the road.</p> <p>Note: Even though we're using <code>efficientnet_bX</code>, it's important not to get too attached to any one architecture, as they are always changing as new research gets released. Best to experiment, experiment, experiment and see what works for your problem.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#32-setting-up-a-pretrained-model","title":"3.2 Setting up a pretrained model\u00b6","text":"<p>The pretrained model we're going to be using is <code>torchvision.models.efficientnet_b0()</code>.</p> <p>The architecture is from the paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.</p> <p>Example of what we're going to create, a pretrained <code>EfficientNet_B0</code> model from <code>torchvision.models</code> with the output layer adjusted for our use case of classifying pizza, steak and sushi images.</p> <p>We can setup the <code>EfficientNet_B0</code> pretrained ImageNet weights using the same code as we used to create the transforms.</p> <pre>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights for ImageNet\n</pre> <p>This means the model has already been trained on millions of images and has a good base representation of image data.</p> <p>The PyTorch version of this pretrained model is capable of achieving ~77.7% accuracy across ImageNet's 1000 classes.</p> <p>We'll also send it to the target device.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#33-getting-a-summary-of-our-model-with-torchinfosummary","title":"3.3 Getting a summary of our model with <code>torchinfo.summary()</code>\u00b6","text":"<p>To learn more about our model, let's use <code>torchinfo</code>'s <code>summary()</code> method.</p> <p>To do so, we'll pass in:</p> <ul> <li><code>model</code> - the model we'd like to get a summary of.</li> <li><code>input_size</code> - the shape of the data we'd like to pass to our model, for the case of <code>efficientnet_b0</code>, the input size is <code>(batch_size, 3, 224, 224)</code>, though other variants of <code>efficientnet_bX</code> have different input sizes.<ul> <li>Note: Many modern models can handle input images of varying sizes thanks to <code>torch.nn.AdaptiveAvgPool2d()</code>, this layer adaptively adjusts the <code>output_size</code> of a given input as required. You can try this out by passing different size input images to <code>summary()</code> or your models.</li> </ul> </li> <li><code>col_names</code> - the various information columns we'd like to see about our model.</li> <li><code>col_width</code> - how wide the columns should be for the summary.</li> <li><code>row_settings</code> - what features to show in a row.</li> </ul>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs","title":"3.4 Freezing the base model and changing the output layer to suit our needs\u00b6","text":"<p>The process of transfer learning usually goes: freeze some base layers of a pretrained model (typically the <code>features</code> section) and then adjust the output layers (also called head/classifier layers) to suit your needs.</p> <p>You can customise the outputs of a pretrained model by changing the output layer(s) to suit your problem. The original <code>torchvision.models.efficientnet_b0()</code> comes with <code>out_features=1000</code> because there are 1000 classes in ImageNet, the dataset it was trained on. However, for our problem, classifying images of pizza, steak and sushi we only need <code>out_features=3</code>.</p> <p>Let's freeze all of the layers/parameters in the <code>features</code> section of our <code>efficientnet_b0</code> model.</p> <p>Note: To freeze layers means to keep them how they are during training. For example, if your model has pretrained layers, to freeze them would be to say, \"don't change any of the patterns in these layers during training, keep them how they are.\" In essence, we'd like to keep the pretrained weights/patterns our model has learned from ImageNet as a backbone and then only change the output layers.</p> <p>We can freeze all of the layers/parameters in the <code>features</code> section by setting the attribute <code>requires_grad=False</code>.</p> <p>For parameters with <code>requires_grad=False</code>, PyTorch doesn't track gradient updates and in turn, these parameters won't be changed by our optimizer during training.</p> <p>In essence, a parameter with <code>requires_grad=False</code> is \"untrainable\" or \"frozen\" in place.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#4-train-model","title":"4. Train model\u00b6","text":"<p>Now we've got a pretraiend model that's semi-frozen and has a customised <code>classifier</code>, how about we see transfer learning in action?</p> <p>To begin training, let's create a loss function and an optimizer.</p> <p>Because we're still working with multi-class classification, we'll use <code>nn.CrossEntropyLoss()</code> for the loss function.</p> <p>And we'll stick with <code>torch.optim.Adam()</code> as our optimizer with <code>lr=0.001</code>.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#5-evaluate-model-by-plotting-loss-curves","title":"5. Evaluate model by plotting loss curves\u00b6","text":"<p>Our model looks like it's performing pretty well.</p> <p>Let's plot it's loss curves to see what the training looks like over time.</p> <p>We can plot the loss curves using the function <code>plot_loss_curves()</code> we created in 04. PyTorch Custom Datasets section 7.8.</p> <p>The function is stored in the <code>helper_functions.py</code> script so we'll try to import it and download the script if we don't have it.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set","title":"6. Make predictions on images from the test set\u00b6","text":"<p>It looks like our model performs well quantitatively but how about qualitatively?</p> <p>Let's find out by making some predictions with our model on images from the test set (these aren't seen during training) and plotting them.</p> <p>Visualize, visualize, visualize!</p> <p>One thing we'll have to remember is that for our model to make predictions on an image, the image has to be in same format as the images our model was trained on.</p> <p>This means we'll need to make sure our images have:</p> <ul> <li>Same shape - If our images are different shapes to what our model was trained on, we'll get shape errors.</li> <li>Same datatype - If our images are a different datatype (e.g. <code>torch.int8</code> vs. <code>torch.float32</code>) we'll get datatype errors.</li> <li>Same device - If our images are on a different device to our model, we'll get device errors.</li> <li>Same transformations - If our model is trained on images that have been transformed in certain way (e.g. normalized with a specific mean and standard deviation) and we try and make preidctions on images transformed in a different way, these predictions may be off.</li> </ul> <p>Note: These requirements go for all kinds of data if you're trying to make predictions with a trained model. Data you'd like to predict on should be in the same format as your model was trained on.</p> <p>To do all of this, we'll create a function <code>pred_and_plot_image()</code> to:</p> <ol> <li>Take in a trained model, a list of class names, a filepath to a target image, an image size, a transform and a target device.</li> <li>Open an image with <code>PIL.Image.open()</code>.</li> <li>Create a transform for the image (this will default to the <code>manual_transforms</code> we created above or it could use a transform generated from <code>weights.transforms()</code>).</li> <li>Make sure the model is on the target device.</li> <li>Turn on model eval mode with <code>model.eval()</code> (this turns off layers like <code>nn.Dropout()</code>, so they aren't used for inference) and the inference mode context manager.</li> <li>Transform the target image with the transform made in step 3 and add an extra batch dimension with <code>torch.unsqueeze(dim=0)</code> so our input image has shape <code>[batch_size, color_channels, height, width]</code>.</li> <li>Make a prediction on the image by passing it to the model ensuring it's on the target device.</li> <li>Convert the model's output logits to prediction probabilities with <code>torch.softmax()</code>.</li> <li>Convert model's prediction probabilities to prediction labels with <code>torch.argmax()</code>.</li> <li>Plot the image with <code>matplotlib</code> and set the title to the prediction label from step 9 and prediction probability from step 8.</li> </ol> <p>Note: This is a similar function to 04. PyTorch Custom Datasets section 11.3's <code>pred_and_plot_image()</code> with a few tweaked steps.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#61-making-predictions-on-a-custom-image","title":"6.1 Making predictions on a custom image\u00b6","text":"<p>It looks like our model does well qualitatively on data from the test set.</p> <p>But how about on our own custom image?</p> <p>That's where the real fun of machine learning is!</p> <p>Predicting on your own custom data, outisde of any training or test set.</p> <p>To test our model on a custom image, let's import the old faithful <code>pizza-dad.jpeg</code> image (an image of my dad eating pizza).</p> <p>We'll then pass it to the <code>pred_and_plot_image()</code> function we created above and see what happens.</p>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#main-takeaways","title":"Main takeaways\u00b6","text":"<ul> <li>Transfer learning often allows to you get good results with a relatively small amount of custom data.</li> <li>Knowing the power of transfer learning, it's a good idea to ask at the start of every problem, \"does an existing well-performing model exist for my problem?\"</li> <li>When using a pretrained model, it's important that your custom data be formatted/preprocessed in the same way that the original model was trained on, otherwise you may get degraded performance.</li> <li>The same goes for predicting on custom data, ensure your custom data is in the same format as the data your model was trained on.</li> <li>There are several different places to find pretrained models from the PyTorch domain libraries, HuggingFace Hub and libraries such as <code>timm</code> (PyTorch Image Models).</li> </ul>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 06</li> <li>Example solutions notebook for 06 (try the exercises before looking at this)<ul> <li>See a live video walkthrough of the solutions on YouTube (errors and all)</li> </ul> </li> </ul> <ol> <li>Make predictions on the entire test dataset and plot a confusion matrix for the results of our model compared to the truth labels. Check out 03. PyTorch Computer Vision section 10 for ideas.</li> <li>Get the \"most wrong\" of the predictions on the test dataset and plot the 5 \"most wrong\" images. You can do this by:<ul> <li>Predicting across all of the test dataset, storing the labels and predicted probabilities.</li> <li>Sort the predictions by wrong prediction and then descending predicted probabilities, this will give you the wrong predictions with the highest prediction probabilities, in other words, the \"most wrong\".</li> <li>Plot the top 5 \"most wrong\" images, why do you think the model got these wrong?</li> </ul> </li> <li>Predict on your own image of pizza/steak/sushi - how does the model go? What happens if you predict on an image that isn't pizza/steak/sushi?</li> <li>Train the model from section 4 above for longer (10 epochs should do), what happens to the performance?</li> <li>Train the model from section 4 above with more data, say 20% of the images from Food101 of Pizza, Steak and Sushi images.<ul> <li>You can find the 20% Pizza, Steak, Sushi dataset on the course GitHub. It was created with the notebook <code>extras/04_custom_data_creation.ipynb</code>.</li> </ul> </li> <li>Try a different model from <code>torchvision.models</code> on the Pizza, Steak, Sushi data, how does this model perform?<ul> <li>You'll have to change the size of the classifier layer to suit our problem.</li> <li>You may want to try an EfficientNet with a higher number than our B0, perhaps <code>torchvision.models.efficientnet_b2()</code>?</li> </ul> </li> </ol>"},{"location":"Learn/z2m-pytorch/06_pytorch_transfer_learning/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Look up what \"model fine-tuning\" is and spend 30-minutes researching different methods to perform it with PyTorch. How would we change our code to fine-tine? Tip: fine-tuning usually works best if you have lots of custom data, where as, feature extraction is typically better if you have less custom data.</li> <li>Check out the new/upcoming PyTorch multi-weights API (still in beta at time of writing, May 2022), it's a new way to perform transfer learning in PyTorch. What changes to our code would need to made to use the new API?</li> <li>Try to create your own classifier on two classes of images, for example, you could collect 10 photos of your dog and your friends dog and train a model to classify the two dogs. This would be a good way to practice creating a dataset as well as building a model on that dataset.</li> </ul>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/","title":"07. PyTorch Experiment Tracking","text":"In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>torch version: 1.13.0.dev20220620+cu113\ntorchvision version: 0.14.0.dev20220620+cu113\n</pre> <p>Note: If you're using Google Colab, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of <code>torch</code> (0.12+) and <code>torchvision</code> (0.13+).</p> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine <p>Now let's setup device agnostic code.</p> <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>.</p> In\u00a0[3]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre># Set seeds\ndef set_seeds(seed: int=42):\n\"\"\"Sets random sets for torch operations.\n\n    Args:\n        seed (int, optional): Random seed to set. Defaults to 42.\n    \"\"\"\n    # Set the seed for general torch operations\n    torch.manual_seed(seed)\n    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n    torch.cuda.manual_seed(seed)\n</pre> # Set seeds def set_seeds(seed: int=42):     \"\"\"Sets random sets for torch operations.      Args:         seed (int, optional): Random seed to set. Defaults to 42.     \"\"\"     # Set the seed for general torch operations     torch.manual_seed(seed)     # Set the seed for CUDA torch operations (ones that happen on the GPU)     torch.cuda.manual_seed(seed) In\u00a0[5]: Copied! <pre>import os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\ndef download_data(source: str, \n                  destination: str,\n                  remove_source: bool = True) -&gt; Path:\n\"\"\"Downloads a zipped dataset from source and unzips to destination.\n\n    Args:\n        source (str): A link to a zipped file containing data.\n        destination (str): A target directory to unzip data to.\n        remove_source (bool): Whether to remove the source after downloading and extracting.\n    Returns:\n        pathlib.Path to downloaded data.\n    Example usage:\n        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                      destination=\"pizza_steak_sushi\")\n    \"\"\"\n    # Setup path to data folder\n    data_path = Path(\"data/\")\n    image_path = data_path / destination\n\n    # If the image folder doesn't exist, download it and prepare it... \n    if image_path.is_dir():\n        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n    else:\n        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n        image_path.mkdir(parents=True, exist_ok=True)\n        \n        # Download pizza, steak, sushi data\n        target_file = Path(source).name\n        with open(data_path / target_file, \"wb\") as f:\n            request = requests.get(source)\n            print(f\"[INFO] Downloading {target_file} from {source}...\")\n            f.write(request.content)\n\n        # Unzip pizza, steak, sushi data\n        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n            print(f\"[INFO] Unzipping {target_file} data...\") \n            zip_ref.extractall(image_path)\n\n        # Remove .zip file\n        if remove_source:\n            os.remove(data_path / target_file)\n    \n    return image_path\n\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> import os import zipfile  from pathlib import Path  import requests  def download_data(source: str,                    destination: str,                   remove_source: bool = True) -&gt; Path:     \"\"\"Downloads a zipped dataset from source and unzips to destination.      Args:         source (str): A link to a zipped file containing data.         destination (str): A target directory to unzip data to.         remove_source (bool): Whether to remove the source after downloading and extracting.          Returns:         pathlib.Path to downloaded data.          Example usage:         download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                       destination=\"pizza_steak_sushi\")     \"\"\"     # Setup path to data folder     data_path = Path(\"data/\")     image_path = data_path / destination      # If the image folder doesn't exist, download it and prepare it...      if image_path.is_dir():         print(f\"[INFO] {image_path} directory exists, skipping download.\")     else:         print(f\"[INFO] Did not find {image_path} directory, creating one...\")         image_path.mkdir(parents=True, exist_ok=True)                  # Download pizza, steak, sushi data         target_file = Path(source).name         with open(data_path / target_file, \"wb\") as f:             request = requests.get(source)             print(f\"[INFO] Downloading {target_file} from {source}...\")             f.write(request.content)          # Unzip pizza, steak, sushi data         with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:             print(f\"[INFO] Unzipping {target_file} data...\")              zip_ref.extractall(image_path)          # Remove .zip file         if remove_source:             os.remove(data_path / target_file)          return image_path  image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n</pre> Out[5]: <pre>PosixPath('data/pizza_steak_sushi')</pre> <p>Excellent! Looks like we've got our pizza, steak and sushi images in standard image classification format ready to go.</p> In\u00a0[6]: Copied! <pre># Setup directories\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Create transform pipeline manually\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \nprint(f\"Manually created transforms: {manual_transforms}\")\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup directories train_dir = image_path / \"train\" test_dir = image_path / \"test\"  # Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet) normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225])  # Create transform pipeline manually manual_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     normalize ])            print(f\"Manually created transforms: {manual_transforms}\")  # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=manual_transforms, # use manually created transforms     batch_size=32 )  train_dataloader, test_dataloader, class_names <pre>Manually created transforms: Compose(\n    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n    ToTensor()\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)\n</pre> Out[6]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d218e0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d216a0&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[7]: Copied! <pre># Setup dirs\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Setup pretrained weights (plenty of these available in torchvision.models)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n\n# Get transforms from weights (these are the transforms that were used to obtain the weights)\nautomatic_transforms = weights.transforms() \nprint(f\"Automatically created transforms: {automatic_transforms}\")\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=automatic_transforms, # use automatic created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup dirs train_dir = image_path / \"train\" test_dir = image_path / \"test\"  # Setup pretrained weights (plenty of these available in torchvision.models) weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT  # Get transforms from weights (these are the transforms that were used to obtain the weights) automatic_transforms = weights.transforms()  print(f\"Automatically created transforms: {automatic_transforms}\")  # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=automatic_transforms, # use automatic created transforms     batch_size=32 )  train_dataloader, test_dataloader, class_names <pre>Automatically created transforms: ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n</pre> Out[7]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d213a0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d21490&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[8]: Copied! <pre># Note: This is how a pretrained model would be created in torchvision &gt; 0.13, it will be deprecated in future versions.\n# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD \n\n# Download the pretrained weights for EfficientNet_B0\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"\n\n# Setup the model with the pretrained weights and send it to the target device\nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n# View the output of the model\n# model\n</pre> # Note: This is how a pretrained model would be created in torchvision &gt; 0.13, it will be deprecated in future versions. # model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD   # Download the pretrained weights for EfficientNet_B0 weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"  # Setup the model with the pretrained weights and send it to the target device model = torchvision.models.efficientnet_b0(weights=weights).to(device)  # View the output of the model # model <p>Wonderful!</p> <p>Now we've got a pretrained model let's turn into a feature extractor model.</p> <p>In essence, we'll freeze the base layers of the model (we'll use these to extract features from our input images) and we'll change the classifier head (output layer) to suit the number of classes we're working with (we've got 3 classes: pizza, steak, sushi).</p> <p>Note: The idea of creating a feature extractor model (what we're doing here) was covered in more depth in 06. PyTorch Transfer Learning section 3.2: Setting up a pretrained model.</p> In\u00a0[9]: Copied! <pre># Freeze all base layers by setting requires_grad attribute to False\nfor param in model.features.parameters():\n    param.requires_grad = False\n    \n# Since we're creating a new layer with random weights (torch.nn.Linear), \n# let's set the seeds\nset_seeds() \n\n# Update the classifier head to suit our problem\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2, inplace=True),\n    nn.Linear(in_features=1280, \n              out_features=len(class_names),\n              bias=True).to(device))\n</pre> # Freeze all base layers by setting requires_grad attribute to False for param in model.features.parameters():     param.requires_grad = False      # Since we're creating a new layer with random weights (torch.nn.Linear),  # let's set the seeds set_seeds()   # Update the classifier head to suit our problem model.classifier = torch.nn.Sequential(     nn.Dropout(p=0.2, inplace=True),     nn.Linear(in_features=1280,                out_features=len(class_names),               bias=True).to(device)) <p>Base layers frozen, classifier head changed, let's get a summary of our model with <code>torchinfo.summary()</code>.</p> In\u00a0[10]: Copied! <pre>from torchinfo import summary\n\n# # Get a summary of the model (uncomment for full output)\n# summary(model, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n#         verbose=0,\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> from torchinfo import summary  # # Get a summary of the model (uncomment for full output) # summary(model,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width) #         verbose=0, #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # ) <p>Output of <code>torchinfo.summary()</code> with our feature extractor EffNetB0 model, notice how the base layers are frozen (not trainable) and the output layers are customized to our own problem.</p> In\u00a0[11]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) In\u00a0[12]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\n\n# Create a writer with all default settings\nwriter = SummaryWriter()\n</pre> from torch.utils.tensorboard import SummaryWriter  # Create a writer with all default settings writer = SummaryWriter() <p>Now to use the writer, we could write a new training loop or we could adjust the existing <code>train()</code> function we created in 05. PyTorch Going Modular section 4.</p> <p>Let's take the latter option.</p> <p>We'll get the <code>train()</code> function from <code>engine.py</code> and adjust it to use <code>writer</code>.</p> <p>Specifically, we'll add the ability for our <code>train()</code> function to log our model's training and test loss and accuracy values.</p> <p>We can do this with <code>writer.add_scalars(main_tag, tag_scalar_dict)</code>, where:</p> <ul> <li><code>main_tag</code> (string) - the name for the scalars being tracked (e.g. \"Accuracy\")</li> <li><code>tag_scalar_dict</code> (dict) - a dictionary of the values being tracked (e.g. <code>{\"train_loss\": 0.3454}</code>)<ul> <li> <p>Note: The method is called <code>add_scalars()</code> because our loss and accuracy values are generally scalars (single values).</p> </li> </ul> </li> </ul> <p>Once we've finished tracking values, we'll call <code>writer.close()</code> to tell the <code>writer</code> to stop looking for values to track.</p> <p>To start modifying <code>train()</code> we'll also import <code>train_step()</code> and <code>test_step()</code> from <code>engine.py</code>.</p> <p>Note: You can track information about your model almost anywhere in your code. But quite often experiments will be tracked while a model is training (inside a training/testing loop).</p> <p>The <code>torch.utils.tensorboard.SummaryWriter()</code> class also has many different methods to track different things about your model/data, such as <code>add_graph()</code> which tracks the computation graph of your model. For more options, check the <code>SummaryWriter()</code> documentation.</p> In\u00a0[13]: Copied! <pre>from typing import Dict, List\nfrom tqdm.auto import tqdm\n\nfrom going_modular.going_modular.engine import train_step, test_step\n\n# Import train() function from: \n# https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List]:\n\"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer,\n                                           device=device)\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        ### New: Experiment tracking ###\n        # Add loss results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n\n        # Add accuracy results to SummaryWriter\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n        \n        # Track the PyTorch model architecture\n        writer.add_graph(model=model, \n                         # Pass in an example input\n                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n    \n    # Close the writer\n    writer.close()\n    \n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results\n</pre> from typing import Dict, List from tqdm.auto import tqdm  from going_modular.going_modular.engine import train_step, test_step  # Import train() function from:  # https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module,           epochs: int,           device: torch.device) -&gt; Dict[str, List]:     \"\"\"Trains and tests a PyTorch model.      Passes a target PyTorch models through train_step() and test_step()     functions for a number of epochs, training and testing the model     in the same epoch loop.      Calculates, prints and stores evaluation metrics throughout.      Args:       model: A PyTorch model to be trained and tested.       train_dataloader: A DataLoader instance for the model to be trained on.       test_dataloader: A DataLoader instance for the model to be tested on.       optimizer: A PyTorch optimizer to help minimize the loss function.       loss_fn: A PyTorch loss function to calculate loss on both datasets.       epochs: An integer indicating how many epochs to train for.       device: A target device to compute on (e.g. \"cuda\" or \"cpu\").            Returns:       A dictionary of training and testing loss as well as training and       testing accuracy metrics. Each metric has a value in a list for        each epoch.       In the form: {train_loss: [...],                 train_acc: [...],                 test_loss: [...],                 test_acc: [...]}        For example if training for epochs=2:                {train_loss: [2.0616, 1.0537],                 train_acc: [0.3945, 0.3945],                 test_loss: [1.2641, 1.5706],                 test_acc: [0.3400, 0.2973]}      \"\"\"     # Create empty results dictionary     results = {\"train_loss\": [],                \"train_acc\": [],                \"test_loss\": [],                \"test_acc\": []     }      # Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                            dataloader=train_dataloader,                                            loss_fn=loss_fn,                                            optimizer=optimizer,                                            device=device)         test_loss, test_acc = test_step(model=model,                                         dataloader=test_dataloader,                                         loss_fn=loss_fn,                                         device=device)          # Print out what's happening         print(           f\"Epoch: {epoch+1} | \"           f\"train_loss: {train_loss:.4f} | \"           f\"train_acc: {train_acc:.4f} | \"           f\"test_loss: {test_loss:.4f} | \"           f\"test_acc: {test_acc:.4f}\"         )          # Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)          ### New: Experiment tracking ###         # Add loss results to SummaryWriter         writer.add_scalars(main_tag=\"Loss\",                             tag_scalar_dict={\"train_loss\": train_loss,                                             \"test_loss\": test_loss},                            global_step=epoch)          # Add accuracy results to SummaryWriter         writer.add_scalars(main_tag=\"Accuracy\",                             tag_scalar_dict={\"train_acc\": train_acc,                                             \"test_acc\": test_acc},                             global_step=epoch)                  # Track the PyTorch model architecture         writer.add_graph(model=model,                           # Pass in an example input                          input_to_model=torch.randn(32, 3, 224, 224).to(device))          # Close the writer     writer.close()          ### End new ###      # Return the filled results at the end of the epochs     return results <p>Woohoo!</p> <p>Our <code>train()</code> function is now updated to use a <code>SummaryWriter()</code> instance to track our model's results.</p> <p>How about we try it out for 5 epochs?</p> In\u00a0[14]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated to use writer\nset_seeds()\nresults = train(model=model,\n                train_dataloader=train_dataloader,\n                test_dataloader=test_dataloader,\n                optimizer=optimizer,\n                loss_fn=loss_fn,\n                epochs=5,\n                device=device)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated to use writer set_seeds() results = train(model=model,                 train_dataloader=train_dataloader,                 test_dataloader=test_dataloader,                 optimizer=optimizer,                 loss_fn=loss_fn,                 epochs=5,                 device=device) <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9133 | test_acc: 0.5398\nEpoch: 2 | train_loss: 0.8975 | train_acc: 0.6562 | test_loss: 0.7838 | test_acc: 0.8561\nEpoch: 3 | train_loss: 0.8037 | train_acc: 0.7461 | test_loss: 0.6723 | test_acc: 0.8864\nEpoch: 4 | train_loss: 0.6769 | train_acc: 0.8516 | test_loss: 0.6698 | test_acc: 0.8049\nEpoch: 5 | train_loss: 0.7065 | train_acc: 0.7188 | test_loss: 0.6746 | test_acc: 0.7737\n</pre> <p>Note: You might notice the results here are slightly different to what our model got in 06. PyTorch Transfer Learning. The difference comes from using the <code>engine.train()</code> and our modified <code>train()</code> function. Can you guess why? The PyTorch documentation on randomness may help more.</p> <p>Running the cell above we get similar outputs we got in 06. PyTorch Transfer Learning section 4: Train model but the difference is behind the scenes our <code>writer</code> instance has created a <code>runs/</code> directory storing our model's results.</p> <p>For example, the save location might look like:</p> <pre><code>runs/Jun21_00-46-03_daniels_macbook_pro\n</code></pre> <p>Where the default format is <code>runs/CURRENT_DATETIME_HOSTNAME</code>.</p> <p>We'll check these out in a second but just as a reminder, we were previously tracking our model's results in a dictionary.</p> In\u00a0[15]: Copied! <pre># Check out the model results\nresults\n</pre> # Check out the model results results Out[15]: <pre>{'train_loss': [1.0923754647374153,\n  0.8974628075957298,\n  0.803724929690361,\n  0.6769256368279457,\n  0.7064960040152073],\n 'train_acc': [0.3984375, 0.65625, 0.74609375, 0.8515625, 0.71875],\n 'test_loss': [0.9132757981618246,\n  0.7837507526079813,\n  0.6722926497459412,\n  0.6698453426361084,\n  0.6746167540550232],\n 'test_acc': [0.5397727272727273,\n  0.8560606060606061,\n  0.8863636363636364,\n  0.8049242424242425,\n  0.7736742424242425]}</pre> <p>Hmmm, we could format this to be a nice plot but could you image keeping track of a bunch of these dictionaries?</p> <p>There has to be a better way...</p> In\u00a0[16]: Copied! <pre># Example code to run in Jupyter or Google Colab Notebook (uncomment to try it out)\n# %load_ext tensorboard\n# %tensorboard --logdir runs\n</pre> # Example code to run in Jupyter or Google Colab Notebook (uncomment to try it out) # %load_ext tensorboard # %tensorboard --logdir runs <p>If all went correctly, you should see something like the following:</p> <p>Viewing a single modelling experiment's results for accuracy and loss in TensorBoard.</p> <p>Note: For more information on running TensorBoard in notebooks or in other locations, see the following:</p> <ul> <li>Using TensorBoard in Notebooks guide by TensorFlow</li> <li>Get started with TensorBoard.dev (helpful for uploading your TensorBoard logs to a shareable link)</li> </ul> In\u00a0[17]: Copied! <pre>def create_writer(experiment_name: str, \n                  model_name: str, \n                  extra: str=None) -&gt; torch.utils.tensorboard.writer.SummaryWriter():\n\"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n\n    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n\n    Where timestamp is the current date in YYYY-MM-DD format.\n\n    Args:\n        experiment_name (str): Name of experiment.\n        model_name (str): Name of model.\n        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n\n    Returns:\n        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n\n    Example usage:\n        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n        writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb2\",\n                               extra=\"5_epochs\")\n        # The above is the same as:\n        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n    \"\"\"\n    from datetime import datetime\n    import os\n\n    # Get timestamp of current date (all experiments on certain day live in same folder)\n    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n\n    if extra:\n        # Create log directory path\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n    else:\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n        \n    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n    return SummaryWriter(log_dir=log_dir)\n</pre> def create_writer(experiment_name: str,                    model_name: str,                    extra: str=None) -&gt; torch.utils.tensorboard.writer.SummaryWriter():     \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.      log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.      Where timestamp is the current date in YYYY-MM-DD format.      Args:         experiment_name (str): Name of experiment.         model_name (str): Name of model.         extra (str, optional): Anything extra to add to the directory. Defaults to None.      Returns:         torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.      Example usage:         # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"         writer = create_writer(experiment_name=\"data_10_percent\",                                model_name=\"effnetb2\",                                extra=\"5_epochs\")         # The above is the same as:         writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")     \"\"\"     from datetime import datetime     import os      # Get timestamp of current date (all experiments on certain day live in same folder)     timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format      if extra:         # Create log directory path         log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)     else:         log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)              print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")     return SummaryWriter(log_dir=log_dir) <p>Beautiful!</p> <p>Now we've got a <code>create_writer()</code> function, let's try it out.</p> In\u00a0[18]: Copied! <pre># Create an example writer\nexample_writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb0\",\n                               extra=\"5_epochs\")\n</pre> # Create an example writer example_writer = create_writer(experiment_name=\"data_10_percent\",                                model_name=\"effnetb0\",                                extra=\"5_epochs\") <pre>[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...\n</pre> <p>Looking good, now we've got a way to log and trace back our various experiments.</p> In\u00a0[19]: Copied! <pre>from typing import Dict, List\nfrom tqdm.auto import tqdm\n\n# Add writer parameter to train()\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device, \n          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n          ) -&gt; Dict[str, List]:\n\"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Stores metrics to specified writer log_dir if present.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n      writer: A SummaryWriter() instance to log model results to.\n\n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n\n        ### New: Use the writer parameter to track experiments ###\n        # See if there's a writer, if so, log to it\n        if writer:\n            # Add results to SummaryWriter\n            writer.add_scalars(main_tag=\"Loss\", \n                               tag_scalar_dict={\"train_loss\": train_loss,\n                                                \"test_loss\": test_loss},\n                               global_step=epoch)\n            writer.add_scalars(main_tag=\"Accuracy\", \n                               tag_scalar_dict={\"train_acc\": train_acc,\n                                                \"test_acc\": test_acc}, \n                               global_step=epoch)\n\n            # Close the writer\n            writer.close()\n        else:\n            pass\n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results\n</pre> from typing import Dict, List from tqdm.auto import tqdm  # Add writer parameter to train() def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module,           epochs: int,           device: torch.device,            writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer           ) -&gt; Dict[str, List]:     \"\"\"Trains and tests a PyTorch model.      Passes a target PyTorch models through train_step() and test_step()     functions for a number of epochs, training and testing the model     in the same epoch loop.      Calculates, prints and stores evaluation metrics throughout.      Stores metrics to specified writer log_dir if present.      Args:       model: A PyTorch model to be trained and tested.       train_dataloader: A DataLoader instance for the model to be trained on.       test_dataloader: A DataLoader instance for the model to be tested on.       optimizer: A PyTorch optimizer to help minimize the loss function.       loss_fn: A PyTorch loss function to calculate loss on both datasets.       epochs: An integer indicating how many epochs to train for.       device: A target device to compute on (e.g. \"cuda\" or \"cpu\").       writer: A SummaryWriter() instance to log model results to.      Returns:       A dictionary of training and testing loss as well as training and       testing accuracy metrics. Each metric has a value in a list for        each epoch.       In the form: {train_loss: [...],                 train_acc: [...],                 test_loss: [...],                 test_acc: [...]}        For example if training for epochs=2:                {train_loss: [2.0616, 1.0537],                 train_acc: [0.3945, 0.3945],                 test_loss: [1.2641, 1.5706],                 test_acc: [0.3400, 0.2973]}      \"\"\"     # Create empty results dictionary     results = {\"train_loss\": [],                \"train_acc\": [],                \"test_loss\": [],                \"test_acc\": []     }      # Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                           dataloader=train_dataloader,                                           loss_fn=loss_fn,                                           optimizer=optimizer,                                           device=device)         test_loss, test_acc = test_step(model=model,           dataloader=test_dataloader,           loss_fn=loss_fn,           device=device)          # Print out what's happening         print(           f\"Epoch: {epoch+1} | \"           f\"train_loss: {train_loss:.4f} | \"           f\"train_acc: {train_acc:.4f} | \"           f\"test_loss: {test_loss:.4f} | \"           f\"test_acc: {test_acc:.4f}\"         )          # Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)           ### New: Use the writer parameter to track experiments ###         # See if there's a writer, if so, log to it         if writer:             # Add results to SummaryWriter             writer.add_scalars(main_tag=\"Loss\",                                 tag_scalar_dict={\"train_loss\": train_loss,                                                 \"test_loss\": test_loss},                                global_step=epoch)             writer.add_scalars(main_tag=\"Accuracy\",                                 tag_scalar_dict={\"train_acc\": train_acc,                                                 \"test_acc\": test_acc},                                 global_step=epoch)              # Close the writer             writer.close()         else:             pass     ### End new ###      # Return the filled results at the end of the epochs     return results In\u00a0[20]: Copied! <pre># Download 10 percent and 20 percent training data (if necessary)\ndata_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                                     destination=\"pizza_steak_sushi\")\n\ndata_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\n</pre> # Download 10 percent and 20 percent training data (if necessary) data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                                      destination=\"pizza_steak_sushi\")  data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",                                      destination=\"pizza_steak_sushi_20_percent\") <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n</pre> <p>Data downloaded!</p> <p>Now let's setup the filepaths to data we'll be using for the different experiments.</p> <p>We'll create different training directory paths but we'll only need one testing directory path since all experiments will be using the same test dataset (the test dataset from pizza, steak, sushi 10%).</p> In\u00a0[21]: Copied! <pre># Setup training directory paths\ntrain_dir_10_percent = data_10_percent_path / \"train\"\ntrain_dir_20_percent = data_20_percent_path / \"train\"\n\n# Setup testing directory paths (note: use the same test dataset for both to compare the results)\ntest_dir = data_10_percent_path / \"test\"\n\n# Check the directories\nprint(f\"Training directory 10%: {train_dir_10_percent}\")\nprint(f\"Training directory 20%: {train_dir_20_percent}\")\nprint(f\"Testing directory: {test_dir}\")\n</pre> # Setup training directory paths train_dir_10_percent = data_10_percent_path / \"train\" train_dir_20_percent = data_20_percent_path / \"train\"  # Setup testing directory paths (note: use the same test dataset for both to compare the results) test_dir = data_10_percent_path / \"test\"  # Check the directories print(f\"Training directory 10%: {train_dir_10_percent}\") print(f\"Training directory 20%: {train_dir_20_percent}\") print(f\"Testing directory: {test_dir}\") <pre>Training directory 10%: data/pizza_steak_sushi/train\nTraining directory 20%: data/pizza_steak_sushi_20_percent/train\nTesting directory: data/pizza_steak_sushi/test\n</pre> In\u00a0[22]: Copied! <pre>from torchvision import transforms\n\n# Create a transform to normalize data distribution to be inline with ImageNet\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n                                 std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]\n\n# Compose transforms into a pipeline\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Resize the images\n    transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 &amp; 1\n    normalize # 3. Normalize the images so their distributions match the ImageNet dataset \n])\n</pre> from torchvision import transforms  # Create a transform to normalize data distribution to be inline with ImageNet normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]                                  std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]  # Compose transforms into a pipeline simple_transform = transforms.Compose([     transforms.Resize((224, 224)), # 1. Resize the images     transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 &amp; 1     normalize # 3. Normalize the images so their distributions match the ImageNet dataset  ]) <p>Transform ready!</p> <p>Now let's create our DataLoaders using the <code>create_dataloaders()</code> function from <code>data_setup.py</code> we created in 05. PyTorch Going Modular section 2.</p> <p>We'll create the DataLoaders with a batch size of 32.</p> <p>For all of our experiments we'll be using the same <code>test_dataloader</code> (to keep comparisons consistent).</p> In\u00a0[23]: Copied! <pre>BATCH_SIZE = 32\n\n# Create 10% training and test DataLoaders\ntrain_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n    test_dir=test_dir, \n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Create 20% training and test data DataLoders\ntrain_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\nprint(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\nprint(f\"Number of classes: {len(class_names)}, class names: {class_names}\")\n</pre> BATCH_SIZE = 32  # Create 10% training and test DataLoaders train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,     test_dir=test_dir,      transform=simple_transform,     batch_size=BATCH_SIZE )  # Create 20% training and test data DataLoders train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,     test_dir=test_dir,     transform=simple_transform,     batch_size=BATCH_SIZE )  # Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments) print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\") print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\") print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\") print(f\"Number of classes: {len(class_names)}, class names: {class_names}\") <pre>Number of batches of size 32 in 10 percent training data: 8\nNumber of batches of size 32 in 20 percent training data: 15\nNumber of batches of size 32 in testing data: 8 (all experiments will use the same test set)\nNumber of classes: 3, class names: ['pizza', 'steak', 'sushi']\n</pre> In\u00a0[24]: Copied! <pre>import torchvision\nfrom torchinfo import summary\n\n# 1. Create an instance of EffNetB2 with pretrained weights\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n\n# # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output)\n# summary(model=effnetb2, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# ) \n\n# 3. Get the number of in_features of the EfficientNetB2 classifier layer\nprint(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")\n</pre> import torchvision from torchinfo import summary  # 1. Create an instance of EffNetB2 with pretrained weights effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)  # # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output) # summary(model=effnetb2,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # )   # 3. Get the number of in_features of the EfficientNetB2 classifier layer print(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\") <pre>Number of in_features to final layer of EfficientNetB2: 1408\n</pre> <p>Model summary of EffNetB2 feature extractor model with all layers unfrozen (trainable) and default classifier head from ImageNet pretraining.</p> <p>Now we know the required number of <code>in_features</code> for the EffNetB2 model, let's create a couple of helper functions to setup our EffNetB0 and EffNetB2 feature extractor models.</p> <p>We want these functions to:</p> <ol> <li>Get the base model from <code>torchvision.models</code></li> <li>Freeze the base layers in the model (set <code>requires_grad=False</code>)</li> <li>Set the random seeds (we don't need to do this but since we're running a series of experiments and initalizing a new layer with random weights, we want the randomness to be similar for each experiment)</li> <li>Change the classifier head (to suit our problem)</li> <li>Give the model a name (e.g. \"effnetb0\" for EffNetB0)</li> </ol> In\u00a0[25]: Copied! <pre>import torchvision\nfrom torch import nn\n\n# Get num out features (one for each class pizza, steak, sushi)\nOUT_FEATURES = len(class_names)\n\n# Create an EffNetB0 feature extractor\ndef create_effnetb0():\n    # 1. Get the base mdoel with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.2),\n        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb0\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n\n# Create an EffNetB2 feature extractor\ndef create_effnetb2():\n    # 1. Get the base model with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3),\n        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb2\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n</pre> import torchvision from torch import nn  # Get num out features (one for each class pizza, steak, sushi) OUT_FEATURES = len(class_names)  # Create an EffNetB0 feature extractor def create_effnetb0():     # 1. Get the base mdoel with pretrained weights and send to target device     weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT     model = torchvision.models.efficientnet_b0(weights=weights).to(device)      # 2. Freeze the base model layers     for param in model.features.parameters():         param.requires_grad = False      # 3. Set the seeds     set_seeds()      # 4. Change the classifier head     model.classifier = nn.Sequential(         nn.Dropout(p=0.2),         nn.Linear(in_features=1280, out_features=OUT_FEATURES)     ).to(device)      # 5. Give the model a name     model.name = \"effnetb0\"     print(f\"[INFO] Created new {model.name} model.\")     return model  # Create an EffNetB2 feature extractor def create_effnetb2():     # 1. Get the base model with pretrained weights and send to target device     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     model = torchvision.models.efficientnet_b2(weights=weights).to(device)      # 2. Freeze the base model layers     for param in model.features.parameters():         param.requires_grad = False      # 3. Set the seeds     set_seeds()      # 4. Change the classifier head     model.classifier = nn.Sequential(         nn.Dropout(p=0.3),         nn.Linear(in_features=1408, out_features=OUT_FEATURES)     ).to(device)      # 5. Give the model a name     model.name = \"effnetb2\"     print(f\"[INFO] Created new {model.name} model.\")     return model <p>Those are some nice looking functions!</p> <p>Let's test them out by creating an instance of EffNetB0 and EffNetB2 and checking out their <code>summary()</code>.</p> In\u00a0[26]: Copied! <pre>effnetb0 = create_effnetb0() \n\n# Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output)\n# summary(model=effnetb0, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> effnetb0 = create_effnetb0()   # Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output) # summary(model=effnetb0,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # )  <pre>[INFO] Created new effnetb0 model.\n</pre> <p>Model summary of EffNetB0 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).</p> In\u00a0[27]: Copied! <pre>effnetb2 = create_effnetb2()\n\n# Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output)\n# summary(model=effnetb2, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> effnetb2 = create_effnetb2()  # Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output) # summary(model=effnetb2,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # )  <pre>[INFO] Created new effnetb2 model.\n</pre> <p>Model summary of EffNetB2 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).</p> <p>Looking at the outputs of the summaries, it seems the EffNetB2 backbone has nearly double the amount of parameters as EffNetB0.</p> Model Total parameters (before freezing/changing head) Total parameters (after freezing/changing head) Total trainable parameters (after freezing/changing head) EfficientNetB0 5,288,548 4,011,391 3,843 EfficientNetB2 9,109,994 7,705,221 4,227 <p>This gives the backbone of the EffNetB2 model more opportunities to form a representation of our pizza, steak and sushi data.</p> <p>However, the trainable parameters for each model (the classifier heads) aren't very different.</p> <p>Will these extra parameters lead to better results?</p> <p>We'll have to wait and see...</p> <p>Note: In the spirit of experimenting, you really could try almost any model from <code>torchvision.models</code> in a similar fashion to what we're doing here. I've only chosen EffNetB0 and EffNetB2 as examples. Perhaps you might want to throw something like <code>torchvision.models.convnext_tiny()</code> or <code>torchvision.models.convnext_small()</code> into the mix.</p> In\u00a0[28]: Copied! <pre># 1. Create epochs list\nnum_epochs = [5, 10]\n\n# 2. Create models list (need to create a new model for each experiment)\nmodels = [\"effnetb0\", \"effnetb2\"]\n\n# 3. Create dataloaders dictionary for various dataloaders\ntrain_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n                     \"data_20_percent\": train_dataloader_20_percent}\n</pre> # 1. Create epochs list num_epochs = [5, 10]  # 2. Create models list (need to create a new model for each experiment) models = [\"effnetb0\", \"effnetb2\"]  # 3. Create dataloaders dictionary for various dataloaders train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,                      \"data_20_percent\": train_dataloader_20_percent} <p>Lists and dictionary created!</p> <p>Now we can write code to iterate through each of the different options and try out each of the different combinations.</p> <p>We'll also save the model at the end of each experiment so later on we can load back in the best model and use it for making predictions.</p> <p>Specifically, let's go through the following steps:</p> <ol> <li>Set the random seeds (so our experiment results are reproducible, in practice, you might run the same experiment across ~3 different seeds and average the results).</li> <li>Keep track of different experiment numbers (this is mostly for pretty print outs).</li> <li>Loop through the <code>train_dataloaders</code> dictionary items for each of the different training DataLoaders.</li> <li>Loop through the list of epoch numbers.</li> <li>Loop through the list of different model names.</li> <li>Create information print outs for the current running experiment (so we know what's happening).</li> <li>Check which model is the target model and create a new EffNetB0 or EffNetB2 instance (we create a new model instance each experiment so all models start from the same standpoint).</li> <li>Create a new loss function (<code>torch.nn.CrossEntropyLoss()</code>) and optimizer (<code>torch.optim.Adam(params=model.parameters(), lr=0.001)</code>) for each new experiment.</li> <li>Train the model with the modified <code>train()</code> function passing the appropriate details to the <code>writer</code> parameter.</li> <li>Save the trained model with an appropriate file name to file with <code>save_model()</code> from <code>utils.py</code>.</li> </ol> <p>We can also use the <code>%%time</code> magic to see how long all of our experiments take together in a single Jupyter/Google Colab cell.</p> <p>Let's do it!</p> In\u00a0[29]: Copied! <pre>%%time\nfrom going_modular.going_modular.utils import save_model\n\n# 1. Set the random seeds\nset_seeds(seed=42)\n\n# 2. Keep track of experiment numbers\nexperiment_number = 0\n\n# 3. Loop through each DataLoader\nfor dataloader_name, train_dataloader in train_dataloaders.items():\n\n    # 4. Loop through each number of epochs\n    for epochs in num_epochs: \n\n        # 5. Loop through each model name and create a new model based on the name\n        for model_name in models:\n\n            # 6. Create information print outs\n            experiment_number += 1\n            print(f\"[INFO] Experiment number: {experiment_number}\")\n            print(f\"[INFO] Model: {model_name}\")\n            print(f\"[INFO] DataLoader: {dataloader_name}\")\n            print(f\"[INFO] Number of epochs: {epochs}\")  \n\n            # 7. Select the model\n            if model_name == \"effnetb0\":\n                model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n            else:\n                model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n            \n            # 8. Create a new loss and optimizer for every model\n            loss_fn = nn.CrossEntropyLoss()\n            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n\n            # 9. Train target model with target dataloaders and track experiments\n            train(model=model,\n                  train_dataloader=train_dataloader,\n                  test_dataloader=test_dataloader, \n                  optimizer=optimizer,\n                  loss_fn=loss_fn,\n                  epochs=epochs,\n                  device=device,\n                  writer=create_writer(experiment_name=dataloader_name,\n                                       model_name=model_name,\n                                       extra=f\"{epochs}_epochs\"))\n            \n            # 10. Save the model to file so we can get back the best model\n            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n            save_model(model=model,\n                       target_dir=\"models\",\n                       model_name=save_filepath)\n            print(\"-\"*50 + \"\\n\")\n</pre> %%time from going_modular.going_modular.utils import save_model  # 1. Set the random seeds set_seeds(seed=42)  # 2. Keep track of experiment numbers experiment_number = 0  # 3. Loop through each DataLoader for dataloader_name, train_dataloader in train_dataloaders.items():      # 4. Loop through each number of epochs     for epochs in num_epochs:           # 5. Loop through each model name and create a new model based on the name         for model_name in models:              # 6. Create information print outs             experiment_number += 1             print(f\"[INFO] Experiment number: {experiment_number}\")             print(f\"[INFO] Model: {model_name}\")             print(f\"[INFO] DataLoader: {dataloader_name}\")             print(f\"[INFO] Number of epochs: {epochs}\")                # 7. Select the model             if model_name == \"effnetb0\":                 model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)             else:                 model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)                          # 8. Create a new loss and optimizer for every model             loss_fn = nn.CrossEntropyLoss()             optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)              # 9. Train target model with target dataloaders and track experiments             train(model=model,                   train_dataloader=train_dataloader,                   test_dataloader=test_dataloader,                    optimizer=optimizer,                   loss_fn=loss_fn,                   epochs=epochs,                   device=device,                   writer=create_writer(experiment_name=dataloader_name,                                        model_name=model_name,                                        extra=f\"{epochs}_epochs\"))                          # 10. Save the model to file so we can get back the best model             save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"             save_model(model=model,                        target_dir=\"models\",                        model_name=save_filepath)             print(\"-\"*50 + \"\\n\") <pre>[INFO] Experiment number: 1\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678\nEpoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203\nEpoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258\nEpoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864\nEpoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864\n[INFO] Saving model to: models/07_effnetb0_data_10_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 2\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610\nEpoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144\nEpoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873\n[INFO] Saving model to: models/07_effnetb2_data_10_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 3\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678\nEpoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203\nEpoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258\nEpoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864\nEpoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864\nEpoch: 6 | train_loss: 0.5611 | train_acc: 0.8984 | test_loss: 0.5949 | test_acc: 0.8864\nEpoch: 7 | train_loss: 0.5573 | train_acc: 0.7930 | test_loss: 0.5566 | test_acc: 0.8864\nEpoch: 8 | train_loss: 0.4702 | train_acc: 0.9492 | test_loss: 0.5176 | test_acc: 0.8759\nEpoch: 9 | train_loss: 0.5728 | train_acc: 0.7773 | test_loss: 0.5095 | test_acc: 0.8873\nEpoch: 10 | train_loss: 0.4794 | train_acc: 0.8242 | test_loss: 0.4640 | test_acc: 0.9072\n[INFO] Saving model to: models/07_effnetb0_data_10_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 4\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610\nEpoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144\nEpoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873\nEpoch: 6 | train_loss: 0.6111 | train_acc: 0.7812 | test_loss: 0.6325 | test_acc: 0.9280\nEpoch: 7 | train_loss: 0.6127 | train_acc: 0.8008 | test_loss: 0.6404 | test_acc: 0.8769\nEpoch: 8 | train_loss: 0.5202 | train_acc: 0.9336 | test_loss: 0.6200 | test_acc: 0.8977\nEpoch: 9 | train_loss: 0.5425 | train_acc: 0.8008 | test_loss: 0.6227 | test_acc: 0.8466\nEpoch: 10 | train_loss: 0.4908 | train_acc: 0.8125 | test_loss: 0.5870 | test_acc: 0.8873\n[INFO] Saving model to: models/07_effnetb2_data_10_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 5\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655\nEpoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176\nEpoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176\nEpoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176\n[INFO] Saving model to: models/07_effnetb0_data_20_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 6\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153\nEpoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873\nEpoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280\nEpoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186\nEpoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489\n[INFO] Saving model to: models/07_effnetb2_data_20_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 7\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655\nEpoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176\nEpoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176\nEpoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176\nEpoch: 6 | train_loss: 0.3705 | train_acc: 0.8854 | test_loss: 0.3568 | test_acc: 0.9072\nEpoch: 7 | train_loss: 0.3551 | train_acc: 0.9250 | test_loss: 0.3187 | test_acc: 0.9072\nEpoch: 8 | train_loss: 0.3745 | train_acc: 0.8938 | test_loss: 0.3349 | test_acc: 0.8873\nEpoch: 9 | train_loss: 0.2972 | train_acc: 0.9396 | test_loss: 0.3092 | test_acc: 0.9280\nEpoch: 10 | train_loss: 0.3620 | train_acc: 0.8479 | test_loss: 0.2780 | test_acc: 0.9072\n[INFO] Saving model to: models/07_effnetb0_data_20_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 8\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153\nEpoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873\nEpoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280\nEpoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186\nEpoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489\nEpoch: 6 | train_loss: 0.3889 | train_acc: 0.9104 | test_loss: 0.4555 | test_acc: 0.8977\nEpoch: 7 | train_loss: 0.3483 | train_acc: 0.9271 | test_loss: 0.4227 | test_acc: 0.9384\nEpoch: 8 | train_loss: 0.3862 | train_acc: 0.8771 | test_loss: 0.4344 | test_acc: 0.9280\nEpoch: 9 | train_loss: 0.3308 | train_acc: 0.8979 | test_loss: 0.4242 | test_acc: 0.9384\nEpoch: 10 | train_loss: 0.3383 | train_acc: 0.8896 | test_loss: 0.3906 | test_acc: 0.9384\n[INFO] Saving model to: models/07_effnetb2_data_20_percent_10_epochs.pth\n--------------------------------------------------\n\nCPU times: user 29.5 s, sys: 1min 28s, total: 1min 58s\nWall time: 2min 33s\n</pre> In\u00a0[30]: Copied! <pre># Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)\n# %load_ext tensorboard\n# %tensorboard --logdir runs\n</pre> # Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance) # %load_ext tensorboard # %tensorboard --logdir runs <p>Running the cell above we should get an output similar to the following.</p> <p>Note: Depending on the random seeds you used/hardware you used there's a chance your numbers aren't exactly the same as what's here. This is okay. It's due to the inheret randomness of deep learning. What matters most is the trend. Where your numbers are heading. If they're off by a large amount, perhaps there's something wrong and best to go back and check the code. But if they're off by a small amount (say a couple of decimal places or so), that's okay.</p> <p>Visualizing the test loss values for the different modelling experiments in TensorBoard, you can see that the EffNetB2 model trained for 10 epochs and with 20% of the data achieves the lowest loss. This sticks with the overall trend of the experiments that: more data, larger model and longer training time is generally better.</p> <p>You can also upload your TensorBoard experiment results to tensorboard.dev to host them publically for free.</p> <p>For example, running code similiar to the following:</p> In\u00a0[31]: Copied! <pre># # Upload the results to TensorBoard.dev (uncomment to try it out)\n# !tensorboard dev upload --logdir runs \\\n#     --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\\n#     --description \"Comparing results of different model size, training data amount and training time.\"\n</pre> # # Upload the results to TensorBoard.dev (uncomment to try it out) # !tensorboard dev upload --logdir runs \\ #     --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\ #     --description \"Comparing results of different model size, training data amount and training time.\" <p>Running the cell above results in the experiments from this notebook being publically viewable at: https://tensorboard.dev/experiment/VySxUYY7Rje0xREYvCvZXA/</p> <p>Note: Beware that anything you upload to tensorboard.dev is publically available for anyone to see. So if you do upload your experiments, be careful they don't contain sensitive information.</p> In\u00a0[32]: Copied! <pre># Setup the best model filepath\nbest_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"\n\n# Instantiate a new instance of EffNetB2 (to load the saved state_dict() to)\nbest_model = create_effnetb2()\n\n# Load the saved best model state_dict()\nbest_model.load_state_dict(torch.load(best_model_path))\n</pre> # Setup the best model filepath best_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"  # Instantiate a new instance of EffNetB2 (to load the saved state_dict() to) best_model = create_effnetb2()  # Load the saved best model state_dict() best_model.load_state_dict(torch.load(best_model_path)) <pre>[INFO] Created new effnetb2 model.\n</pre> Out[32]: <pre>&lt;All keys matched successfully&gt;</pre> <p>Best model loaded!</p> <p>While we're here, let's check its filesize.</p> <p>This is an important consideration later on when deploying the model (incorporating it in an app).</p> <p>If the model is too large, it can be hard to deploy.</p> In\u00a0[33]: Copied! <pre># Check the model file size\nfrom pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\neffnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024)\nprint(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\")\n</pre> # Check the model file size from pathlib import Path  # Get the model size in bytes then convert to megabytes effnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024) print(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\") <pre>EfficientNetB2 feature extractor model size: 29 MB\n</pre> <p>Looks like our best model so far is 29 MB in size. We'll keep this in mind if we wanted to deploy it later on.</p> <p>Time to make and visualize some predictions.</p> <p>We created a <code>pred_and_plot_image()</code> function use a trained model to make predictions on an image in 06. PyTorch Transfer Learning section 6.</p> <p>And we can reuse this function by importing it from <code>going_modular.going_modular.predictions.py</code> (I put the <code>pred_and_plot_image()</code> function in a script so we could reuse it).</p> <p>So to make predictions on various images the model hasn't seen before, we'll first get a list of all the image filepaths from the 20% pizza, steak, sushi testing dataset and then we'll randomly select a subset of these filepaths to pass to our <code>pred_and_plot_image()</code> function.</p> In\u00a0[34]: Copied! <pre># Import function to make predictions on images and plot them \n# See the function previously created in section: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Get a random list of 3 images from 20% test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset\ntest_image_path_sample = random.sample(population=test_image_path_list,\n                                       k=num_images_to_plot) # randomly select k number of images\n\n# Iterate through random test image paths, make predictions on them and plot them\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=best_model,\n                        image_path=image_path,\n                        class_names=class_names,\n                        image_size=(224, 224))\n</pre> # Import function to make predictions on images and plot them  # See the function previously created in section: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set from going_modular.going_modular.predictions import pred_and_plot_image  # Get a random list of 3 images from 20% test set import random num_images_to_plot = 3 test_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset test_image_path_sample = random.sample(population=test_image_path_list,                                        k=num_images_to_plot) # randomly select k number of images  # Iterate through random test image paths, make predictions on them and plot them for image_path in test_image_path_sample:     pred_and_plot_image(model=best_model,                         image_path=image_path,                         class_names=class_names,                         image_size=(224, 224)) <p>Nice!</p> <p>Running the cell above a few times we can see our model performs quite well and often has higher prediction probabilities than previous models we've built.</p> <p>This suggests the model is more confident in the decisions it's making.</p> In\u00a0[35]: Copied! <pre># Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = Path(\"data/04-pizza-dad.jpeg\")\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> # Download custom image import requests  # Setup custom image path custom_image_path = Path(\"data/04-pizza-dad.jpeg\")  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predict on custom image pred_and_plot_image(model=model,                     image_path=custom_image_path,                     class_names=class_names) <pre>data/04-pizza-dad.jpeg already exists, skipping download.\n</pre> <p>Woah!</p> <p>Two thumbs again!</p> <p>Our best model predicts \"pizza\" correctly and this time with an even higher prediction probability (0.978) than the first feature extraction model we trained and used in 06. PyTorch Transfer Learning section 6.1.</p> <p>This again suggests our current best model (EffNetB2 feature extractor trained on 20% of the pizza, steak, sushi training data and for 10 epochs) has learned patterns to make it more confident of its decision to predict pizza.</p> <p>I wonder what could improve our model's performance even further?</p> <p>I'll leave that as a challenge for you to investigate.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#07-pytorch-experiment-tracking","title":"07. PyTorch Experiment Tracking\u00b6","text":"<p>Note: This notebook uses <code>torchvision</code>'s upcoming multi-weight support API (coming in <code>torchvision</code> v0.13).</p> <p>As of June 2022, it requires the nightly versions of PyTorch and <code>torchvision</code> be installed.</p> <p>Once <code>torchvision</code> v0.13 becomes the standard (not nightly), the nightly version of <code>torchvision</code> will no longer need to be installed.</p> <p>We've trained a fair few models now on the journey to making FoodVision Mini (an image classification model to classify images of pizza, steak or sushi).</p> <p>And so far we've keep track of them via Python dictionaries.</p> <p>Or just comparing them by the metric print outs during training.</p> <p>What if you wanted to run a dozen (or more) different models at once?</p> <p>Surely there's a better way...</p> <p>There is.</p> <p>Experiment tracking.</p> <p>And since experiment tracking is so important and integral to machine learning, you can consider this notebook your first milestone project.</p> <p>So welcome to Milestone Project 1: FoodVision Mini Experiment Tracking.</p> <p>We're going to answer the question: how do I track my machine learning experiments?</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#what-is-experiment-tracking","title":"What is experiment tracking?\u00b6","text":"<p>Machine learning and deep learning are very experimental.</p> <p>You have to put on your artist's beret/chef's hat to cook up lots of different models.</p> <p>And you have to put on your scientist's coat to track the results of various combinations of data, model architectures and training regimes.</p> <p>That's where experiment tracking comes in.</p> <p>If you're running lots of different experiments, experiment tracking helps you figure out what works and what doesn't.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#why-track-experiments","title":"Why track experiments?\u00b6","text":"<p>If you're only running a handful of models (like we've done so far), it might be okay just to track their results in print outs and a few dictionaries.</p> <p>However, as the number of experiments you run starts to increase, this naive way of tracking could get out of hand.</p> <p>So if you're following the machine learning practitioner's motto of experiment, experiment, experiment!, you'll want a way to track them.</p> <p>After building a few models and tracking their results, you'll start to notice how quickly it can get out of hand.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#different-ways-to-track-machine-learning-experiments","title":"Different ways to track machine learning experiments\u00b6","text":"<p>There are as many different ways to track machine learning experiments as there is experiments to run.</p> <p>This table covers a few.</p> Method Setup Pros Cons Cost Python dictionaries, CSV files, print outs None Easy to setup, runs in pure Python Hard to keep track of large numbers of experiments Free TensorBoard Minimal, install <code>tensorboard</code> Extensions built into PyTorch, widely recognized and used, easily scales. User-experience not as nice as other options. Free Weights &amp; Biases Experiment Tracking Minimal, install <code>wandb</code>, make an account Incredible user experience, make experiments public, tracks almost anything. Requires external resource outside of PyTorch. Free for personal use MLFlow Minimal, install <code>mlflow</code> and starting tracking Fully open-source MLOps lifecycle management, many integrations. Little bit harder to setup a remote tracking server than other services. Free <p>Various places and techniques you can use to track your machine learning experiments. Note: There are various other options similar to Weights &amp; Biases and open-source options similar to MLflow but I've left them out for brevity. You can find more by searching \"machine learning experiment tracking\".</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to be running several different modelling experiments with various levels of data, model size and training time to try and improve on FoodVision Mini.</p> <p>And due to its tight integration with PyTorch and widespread use, this notebook focuses on using TensorBoard to track our experiments.</p> <p>However, the principles we're going to cover are similar across all of the other tools for experiment tracking.</p> Topic Contents 0. Getting setup We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. 1. Get data Let's get the pizza, steak and sushi image classification dataset we've been using to try and improve our FoodVision Mini model's results. 2. Create Datasets and DataLoaders We'll use the <code>data_setup.py</code> script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. 3. Get and customise a pretrained model Just like the last section, 06. PyTorch Transfer Learning we'll download a pretrained model from <code>torchvision.models</code> and customise it to our own problem. 4. Train model amd track results Let's see what it's like to train and track the training results of a single model using TensorBoard. 5. View our model's results in TensorBoard Previously we visualized our model's loss curves with a helper function, now let's see what they look like in TensorBoard. 6. Creating a helper function to track experiments If we're going to be adhering to the machine learner practitioner's motto of experiment, experiment, experiment!, we best create a function that will help us save our modelling experiment results. 7. Setting up a series of modelling experiments Instead of running experiments one by one, how about we write some code to run several experiments at once, with different models, different amounts of data and different training times. 8. View modelling experiments in TensorBoard By this stage we'll have run eight modelling experiments in one go, a fair bit to keep track of, let's what their results look like in TensorBoard. 9. Load in the best model and make predictions with it The point of experiment tracking is to figure out which model performs the best, let's load in the best performing model and make some predictions with it to visualize, visualize, visualize!."},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#0-getting-setup","title":"0. Getting setup\u00b6","text":"<p>Let's start by downloading all of the modules we'll need for this section.</p> <p>To save us writing extra code, we're going to be leveraging some of the Python scripts (such as <code>data_setup.py</code> and <code>engine.py</code>) we created in section, 05. PyTorch Going Modular.</p> <p>Specifically, we're going to download the <code>going_modular</code> directory from the <code>pytorch-deep-learning</code> repository (if we don't already have it).</p> <p>We'll also get the <code>torchinfo</code> package if it's not available.</p> <p><code>torchinfo</code> will help later on to give us visual summaries of our model(s).</p> <p>And since we're using a newer version of the <code>torchvision</code> package (v0.13 as of June 2022), we'll make sure we've got the latest versions.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#create-a-helper-function-to-set-seeds","title":"Create a helper function to set seeds\u00b6","text":"<p>Since we've been setting random seeds a whole bunch throughout previous sections, how about we functionize it?</p> <p>Let's create a function to \"set the seeds\" called <code>set_seeds()</code>.</p> <p>Note: Recall a random seed is a way of flavouring the randomness generated by a computer. They aren't necessary to always set when running machine learning code, however, they help ensure there's an element of reproducibility (the numbers I get with my code are similar to the numbers you get with your code). Outside of an education or experimental setting, random seeds generally aren't required.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#1-get-data","title":"1. Get data\u00b6","text":"<p>As always, before we can run machine learning experiments, we'll need a dataset.</p> <p>We're going to continue trying to improve upon the results we've been getting on FoodVision Mini.</p> <p>In the previous section, 06. PyTorch Transfer Learning, we saw how powerful using a pretrained model and transfer learning could be when classifying images of pizza, steak and sushi.</p> <p>So how about we run some experiments and try to further improve our results?</p> <p>To do so, we'll use similar code to the previous section to download the <code>pizza_steak_sushi.zip</code> (if the data doesn't already exist) except this time its been functionised.</p> <p>This will allow us to use it again later.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#2-create-datasets-and-dataloaders","title":"2. Create Datasets and DataLoaders\u00b6","text":"<p>Now we've got some data, let's turn it into PyTorch DataLoaders.</p> <p>We can do so using the <code>create_dataloaders()</code> function we created in 05. PyTorch Going Modular part 2.</p> <p>And since we'll be using transfer learning and specifically pretrained models from <code>torchvision.models</code>, we'll create a transform to prepare our images correctly.</p> <p>To transform our images in tensors, we can use:</p> <ol> <li>Manually created transforms using <code>torchvision.transforms</code>.</li> <li>Automatically created transforms using <code>torchvision.models.MODEL_NAME.MODEL_WEIGHTS.DEFAULT.transforms()</code>.<ul> <li>Where <code>MODEL_NAME</code> is a specific <code>torchvision.models</code> architecture, <code>MODEL_WEIGHTS</code> is a specific set of pretrained weights and <code>DEFAULT</code> means the \"best available weights\".</li> </ul> </li> </ol> <p>We saw an example of each of these in 06. PyTorch Transfer Learning section 2.</p> <p>Let's see first an example of manually creating a <code>torchvision.transforms</code> pipeline (creating a transforms pipeline this way gives the most customization but can potentially result in performance degradation if the transforms don't match the pretrained model).</p> <p>The main manual transformation we need to be sure of is that all of our images are normalized in ImageNet format (this is because pretrained <code>torchvision.models</code> are all pretrained on ImageNet).</p> <p>We can do this with:</p> <pre>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n</pre>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#21-create-dataloaders-using-manually-created-transforms","title":"2.1 Create DataLoaders using manually created transforms\u00b6","text":""},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#22-create-dataloaders-using-automatically-created-transforms","title":"2.2 Create DataLoaders using automatically created transforms\u00b6","text":"<p>Data transformed and DataLoaders created!</p> <p>Let's now see what the same transformation pipeline looks like but this time by using automatic transforms.</p> <p>We can do this by first instantiating a set of pretrained weights (for example <code>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT</code>)  we'd like to use and calling the <code>transforms()</code> method on it.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#3-getting-a-pretrained-model-freezing-the-base-layers-and-changing-the-classifier-head","title":"3. Getting a pretrained model, freezing the base layers and changing the classifier head\u00b6","text":"<p>Before we run and track multiple modelling experiments, let's see what it's like to run and track a single one.</p> <p>And since our data is ready, the next thing we'll need is a model.</p> <p>Let's download the pretrained weights for a <code>torchvision.models.efficientnet_b0()</code> model and prepare it for use with our own data.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#4-train-model-and-track-results","title":"4. Train model and track results\u00b6","text":"<p>Model ready to go!</p> <p>Let's get ready to train it by creating a loss function and an optimizer.</p> <p>Since we're working with multiple classes, we'll use <code>torch.nn.CrossEntropyLoss()</code> as the loss function.</p> <p>And we'll stick with <code>torch.optim.Adam()</code> with learning rate of <code>0.001</code> for the optimizer.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#adjust-train-function-to-track-results-with-summarywriter","title":"Adjust <code>train()</code> function to track results with <code>SummaryWriter()</code>\u00b6","text":"<p>Beautiful!</p> <p>All of the pieces of our training code are starting to come together.</p> <p>Let's now add the final piece to track our experiments.</p> <p>Previously, we've tracked our modelling experiments using multiple Python dictionaries (one for each model).</p> <p>But you can imagine this could get out of hand if we were running anything more than a few experiments.</p> <p>Not to worry, there's a better option!</p> <p>We can use PyTorch's <code>torch.utils.tensorboard.SummaryWriter()</code> class to save various parts of our model's training progress to file.</p> <p>By default, the <code>SummaryWriter()</code> class saves various information about our model to a file set by the <code>log_dir</code> parameter.</p> <p>The default location for <code>log_dir</code> is under <code>runs/CURRENT_DATETIME_HOSTNAME</code>, where the <code>HOSTNAME</code> is the name of your computer.</p> <p>But of course, you can change where your experiments are tracked (the filename is as customisable as you'd like).</p> <p>The outputs of the <code>SummaryWriter()</code> are saved in TensorBoard format.</p> <p>TensorBoard is a part of the TensorFlow deep learning library and is an excellent way to visualize different parts of your model.</p> <p>To start tracking our modelling experiments, let's create a default <code>SummaryWriter()</code> instance.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#5-view-our-models-results-in-tensorboard","title":"5. View our model's results in TensorBoard\u00b6","text":"<p>The <code>SummaryWriter()</code> class stores our model's results in a directory called <code>runs/</code> in TensorBoard format by default.</p> <p>TensorBoard is a visualization program created by the TensorFlow team to view and inspect information about models and data.</p> <p>You know what that means?</p> <p>It's time to follow the data visualizer's motto and visualize, visualize, visualize!</p> <p>You can view TensorBoard in a number of ways:</p> Code environment How to view TensorBoard Resource VS Code (notebooks or Python scripts) Press <code>SHIFT + CMD + P</code> to open the Command Palette and search for the command \"Python: Launch TensorBoard\". VS Code Guide on TensorBoard and PyTorch Jupyter and Colab Notebooks Make sure TensorBoard is installed, load it with <code>%load_ext tensorboard</code> and then view your results with <code>%tensorboard --logdir DIR_WITH_LOGS</code>. <code>torch.utils.tensorboard</code> and Get started with TensorBoard <p>You can also upload your experiments to tensorboard.dev to share them publicly with others.</p> <p>Running the following code in a Google Colab or Jupyter Notebook will start an interactive TensorBoard session to view TensorBoard files in the <code>runs/</code> directory.</p> <pre>%load_ext tensorboard # line magic to load TensorBoard\n%tensorboard --logdir runs # run TensorBoard session with the \"runs/\" directory\n</pre>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#6-create-a-helper-function-to-build-summarywriter-instances","title":"6. Create a helper function to build <code>SummaryWriter()</code> instances\u00b6","text":"<p>The <code>SummaryWriter()</code> class logs various information to a directory specified by the <code>log_dir</code> parameter.</p> <p>How about we make a helper function to create a custom directory per experiment?</p> <p>In essence, each experiment gets its own logs directory.</p> <p>For example, say we'd like to track things like:</p> <ul> <li>Experiment date/timestamp - when did the experiment take place?</li> <li>Experiment name - is there something we'd like to call the experiment?</li> <li>Model name - what model was used?</li> <li>Extra - should anything else be tracked?</li> </ul> <p>You could track almost anything here and be as creative as you want but these should be enough to start.</p> <p>Let's create a helper function called <code>create_writer()</code> that produces a <code>SummaryWriter()</code> instance tracking to a custom <code>log_dir</code>.</p> <p>Ideally, we'd like the <code>log_dir</code> to be something like:</p> <p><code>runs/YYYY-MM-DD/experiment_name/model_name/extra</code></p> <p>Where <code>YYYY-MM-DD</code> is the date the experiment was run (you could add the time if you wanted to as well).</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#61-update-the-train-function-to-include-a-writer-parameter","title":"6.1 Update the <code>train()</code> function to include a <code>writer</code> parameter\u00b6","text":"<p>Our <code>create_writer()</code> function works fantastic.</p> <p>How about we give our <code>train()</code> function the ability to take in a <code>writer</code> parameter so we actively update the <code>SummaryWriter()</code> instance we're using each time we call <code>train()</code>.</p> <p>For example, say we're running a series of experiments, calling <code>train()</code> multiple times for multiple different models, it would be good if each experiment used a different <code>writer</code>.</p> <p>One <code>writer</code> per experiment = one logs directory per experiment.</p> <p>To adjust the <code>train()</code> function we'll add a <code>writer</code> parameter to the function and then we'll add some code to see if there's a <code>writer</code> and if so, we'll track our information there.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#7-setting-up-a-series-of-modelling-experiments","title":"7. Setting up a series of modelling experiments\u00b6","text":"<p>It's to step things up a notch.</p> <p>Previously we've been running various experiments and inspecting the results one by one.</p> <p>But what if we could run multiple experiments and then inspect the results all together?</p> <p>You in?</p> <p>C'mon, let's go.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#71-what-kind-of-experiments-should-you-run","title":"7.1 What kind of experiments should you run?\u00b6","text":"<p>That's the million dollar question in machine learning.</p> <p>Because there's really no limit to the experiments you can run.</p> <p>Such a freedom is why machine learning is so exciting and terrifying at the same time.</p> <p>This is where you'll have to put on your scientist coat and remember the machine learning practitioner's motto: experiment, experiment, experiment!</p> <p>Every hyperparameter stands as a starting point for a different experiment:</p> <ul> <li>Change the number of epochs.</li> <li>Change the number of layers/hidden units.</li> <li>Change the amount of data.</li> <li>Change the learning rate.</li> <li>Try different kinds of data augmentation.</li> <li>Choose a different model architecture.</li> </ul> <p>With practice and running many different experiments, you'll start to build an intuition of what might help your model.</p> <p>I say might on purpose because there's no guarantees.</p> <p>But generally, in light of The Bitter Lesson (I've mentioned this twice now because it's an important essay in the world of AI), generally the bigger your model (more learnable parameters) and the more data you have (more opportunities to learn), the better the performance.</p> <p>However, when you're first approaching a machine learning problem: start small and if something works, scale it up.</p> <p>Your first batch of experiments should take no longer than a few seconds to a few minutes to run.</p> <p>The quicker you can experiment, the faster you can work out what doesn't work, in turn, the faster you can work out what does work.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#72-what-experiments-are-we-going-to-run","title":"7.2 What experiments are we going to run?\u00b6","text":"<p>Our goal is to improve the model powering FoodVision Mini without it getting too big.</p> <p>In essence, our ideal model achieves a high level of test set accuracy (90%+) but doesn't take too long to train/perform inference (make predictions).</p> <p>We've got plenty of options but how about we keep things simple?</p> <p>Let's try a combination of:</p> <ol> <li>A different amount of data (10% of Pizza, Steak, Sushi vs. 20%)</li> <li>A different model (<code>torchvision.models.efficientnet_b0</code> vs. <code>torchvision.models.efficientnet_b2</code>)</li> <li>A different training time (5 epochs vs. 10 epochs)</li> </ol> <p>Breaking these down we get:</p> Experiment number Training Dataset Model (pretrained on ImageNet) Number of epochs 1 Pizza, Steak, Sushi 10% percent EfficientNetB0 5 2 Pizza, Steak, Sushi 10% percent EfficientNetB2 5 3 Pizza, Steak, Sushi 10% percent EfficientNetB0 10 4 Pizza, Steak, Sushi 10% percent EfficientNetB2 10 5 Pizza, Steak, Sushi 20% percent EfficientNetB0 5 6 Pizza, Steak, Sushi 20% percent EfficientNetB2 5 7 Pizza, Steak, Sushi 20% percent EfficientNetB0 10 8 Pizza, Steak, Sushi 20% percent EfficientNetB2 10 <p>Notice how we're slowly scaling things up.</p> <p>With each experiment we slowly increase the amount of data, the model size and the length of training.</p> <p>By the end, experiment 8 will be using double the data, double the model size and double the length of training compared to experiment 1.</p> <p>Note: I want to be clear that there truly is no limit to amount of experiments you can run. What we've designed here is only a very small subset of options. However, you can't test everything so best to try a few things to begin with and then follow the ones which work the best.</p> <p>And as a reminder, the datasets we're using are a subset of the Food101 dataset (3 classes, pizza, steak, suhsi, instead of 101) and 10% and 20% of the images rather than 100%. If our experiments work, we could start to run more on more data (though this will take longer to compute). You can see how the datasets were created via the <code>04_custom_data_creation.ipynb</code> notebook.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#73-download-different-datasets","title":"7.3 Download different datasets\u00b6","text":"<p>Before we start running our series of experiments, we need to make sure our datasets are ready.</p> <p>We'll need two forms of a training set:</p> <ol> <li>A training set with 10% of the data of Food101 pizza, steak, sushi images (we've already created this above but we'll do it again for completeness).</li> <li>A training set with 20% of the data of Food101 pizza, steak, sushi images.</li> </ol> <p>For consistency, all experiments will use the same testing dataset (the one from the 10% data split).</p> <p>We'll start by downloading the various datasets we need using the <code>download_data()</code> function we created earlier.</p> <p>Both datasets are available from the course GitHub:</p> <ol> <li>Pizza, steak, sushi 10% training data.</li> <li>Pizza, steak, sushi 20% training data.</li> </ol>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#74-transform-datasets-and-create-dataloaders","title":"7.4 Transform Datasets and create DataLoaders\u00b6","text":"<p>Next we'll create a series of transforms to prepare our images for our model(s).</p> <p>To keep things consistent, we'll manually create a transform (just like we did above) and use the same transform across all of the datasets.</p> <p>The transform will:</p> <ol> <li>Resize all the images (we'll start with 224, 224 but this could be changed).</li> <li>Turn them into tensors with values between 0 &amp; 1.</li> <li>Normalize them in way so their distributions are inline with the ImageNet dataset (we do this because our models from <code>torchvision.models</code> have been pretrained on ImageNet).</li> </ol>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#75-create-feature-extractor-models","title":"7.5 Create feature extractor models\u00b6","text":"<p>Time to start building our models.</p> <p>We're going to create two feature extractor models:</p> <ol> <li><code>torchvision.models.efficientnet_b0()</code> pretrained backbone + custom classifier head (EffNetB0 for short).</li> <li><code>torchvision.models.efficientnet_b2()</code> pretrained backbone + custom classifier head (EffNetB2 for short).</li> </ol> <p>To do this, we'll freeze the base layers (the feature layers) and update the model's classifier heads (output layers) to suit our problem just like we did in 06. PyTorch Transfer Learning section 3.4.</p> <p>We saw in the previous chapter the <code>in_features</code> parameter to the classifier head of EffNetB0 is <code>1280</code> (the backbone turns the input image into a feature vector of size <code>1280</code>).</p> <p>Since EffNetB2 has a different number of layers and parameters, we'll need to adapt it accordingly.</p> <p>Note: Whenever you use a different model, one of the first things you should inspect is the input and output shapes. That way you'll know how you'll have to prepare your input data/update the model to have the correct output shape.</p> <p>We can find the input and output shapes of EffNetB2 using <code>torchinfo.summary()</code> and passing in the <code>input_size=(32, 3, 224, 224)</code> parameter (<code>(32, 3, 224, 224)</code> is equivalent to <code>(batch_size, color_channels, height, width)</code>, i.e we pass in an example of what a single batch of data would be to our model).</p> <p>Note: Many modern models can handle input images of varying sizes thanks to <code>torch.nn.AdaptiveAvgPool2d()</code> layer, this layer adaptively adjusts the <code>output_size</code> of a given input as required. You can try this out by passing different size input images to <code>torchinfo.summary()</code> or to your own models using the layer.</p> <p>To find the required input shape to the final layer of EffNetB2, let's:</p> <ol> <li>Create an instance of <code>torchvision.models.efficientnet_b2(pretrained=True)</code>.</li> <li>See the various input and output shapes by running <code>torchinfo.summary()</code>.</li> <li>Print out the number of <code>in_features</code> by inspecting <code>state_dict()</code> of the classifier portion of EffNetB2 and printing the length of the weight matrix.<ul> <li>Note: You could also just inspect the output of <code>effnetb2.classifier</code>.</li> </ul> </li> </ol>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code","title":"7.6 Create experiments and set up training code\u00b6","text":"<p>We've prepared our data and prepared our models, the time has come to setup some experiments!</p> <p>We'll start by creating two lists and a dictionary:</p> <ol> <li>A list of the number of epochs we'd like to test (<code>[5, 10]</code>)</li> <li>A list of the models we'd like to test (<code>[\"effnetb0\", \"effnetb2\"]</code>)</li> <li>A dictionary of the different training DataLoaders</li> </ol>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#8-view-experiments-in-tensorboard","title":"8. View experiments in TensorBoard\u00b6","text":"<p>Ho, ho!</p> <p>Look at us go!</p> <p>Training eight models in one go?</p> <p>Now that's living up to the motto!</p> <p>Experiment, experiment, experiment!</p> <p>How about we check out the results in TensorBoard?</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it","title":"9. Load in the best model and make predictions with it\u00b6","text":"<p>Looking at the TensorBoard logs for our eight experiments, it seems experiment number eight achieved the best overall results (highest test accuracy, second lowest test loss).</p> <p>This is the experiment that used:</p> <ul> <li>EffNetB2 (double the parameters of EffNetB0)</li> <li>20% pizza, steak, sushi training data (double the original training data)</li> <li>10 epochs (double the original training time)</li> </ul> <p>In essence, our biggest model achieved the best results.</p> <p>Though it wasn't as if these results were far better than the other models.</p> <p>The same model on the same data achieved similar results in half the training time (experiment number 6).</p> <p>This suggests that potentially the most influential parts of our experiments were the number of parameters and the amount of data.</p> <p>Inspecting the results further it seems that generally a model with more parameters (EffNetB2) and more data (20% pizza, steak, sushi training data) performs better (lower test loss and higher test accuracy).</p> <p>More experiments could be done to further test this but for now, let's import our best performing model from experiment eight (saved to: <code>models/07_effnetb2_data_20_percent_10_epochs.pth</code>, you can download this model from the course GitHub) and perform some qualitative evaluations.</p> <p>In other words, let's visualize, visualize, visualize!</p> <p>We can import the best saved model by creating a new instance of EffNetB2 using the <code>create_effnetb2()</code> function and then load in the saved <code>state_dict()</code> with <code>torch.load()</code>.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#91-predict-on-a-custom-image-with-the-best-model","title":"9.1 Predict on a custom image with the best model\u00b6","text":"<p>Making predictions on the test dataset is cool but the real magic of machine learning is making predictions on custom images of your own.</p> <p>So let's import the trusty pizza dad image (a photo of my dad in front of a pizza) we've been using for the past couple of sections and see how our model performs on it.</p>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#main-takeaways","title":"Main takeaways\u00b6","text":"<p>We've now gone full circle on the PyTorch workflow introduced in 01. PyTorch Workflow Fundamentals, we've gotten data ready, we've built and picked a pretrained model, we've used our various helper functions to train and evaluate the model and in this notebook we've improved our FoodVision Mini model by running and tracking a series of experiments.</p> <p></p> <p>You should be proud of yourself, this is no small feat!</p> <p>The main ideas you should take away from this Milestone Project 1 are:</p> <ul> <li>The machine learning practioner's motto: experiment, experiment, experiment! (though we've been doing plenty of this already).</li> <li>In the beginning, keep your experiments small so you can work fast, your first few experiments shouldn't take more than a few seconds to a few minutes to run.</li> <li>The more experiments you do, the quicker you can figure out what doesn't work.</li> <li>Scale up when you find something that works. For example, since we've found a pretty good performing model with EffNetB2 as a feature extractor, perhaps you'd now like to see what happens when you scale it up to the whole Food101 dataset from <code>torchvision.datasets</code>.</li> <li>Programmatically tracking your experiments takes a few steps to set up but it's worth it in the long run so you can figure out what works and what doesn't.<ul> <li>There are many different machine learning experiment trackers out there so explore a few and try them out.</li> </ul> </li> </ul>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#exercises","title":"Exercises\u00b6","text":"<p>Note: These exercises expect the use of <code>torchvision</code> v0.13+ (currently in beta as of June 2022). You can install the nightly version via the PyTorch getting started page.</p> <p>All of the exercises are focused on practicing the code above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 07</li> <li>Example solutions notebook for 07 (try the exercises before looking at this)<ul> <li>See a live video walkthrough of the solutions on YouTube (errors and all)</li> </ul> </li> </ul> <ol> <li>Pick a larger model from <code>torchvision.models</code> to add to the list of experiments (for example, EffNetB3 or higher).<ul> <li>How does it perform compared to our existing models?</li> </ul> </li> <li>Introduce data augmentation to the list of experiments using the 20% pizza, steak, sushi training and test datasets, does this change anything?<ul> <li>For example, you could have one training DataLoader that uses data augmentation (e.g. <code>train_dataloader_20_percent_aug</code> and <code>train_dataloader_20_percent_no_aug</code>) and then compare the results of two of the same model types training on these two DataLoaders.</li> <li>Note: You may need to alter the <code>create_dataloaders()</code> function to be able to take a transform for the training data and the testing data (because you don't need to perform data augmentation on the test data). See 04. PyTorch Custom Datasets section 6 for examples of using data augmentation or the script below for an example:</li> </ul> </li> </ol> <pre># Note: Data augmentation transform like this should only be performed on training data\ntrain_transform_data_aug = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(),\n    transforms.ToTensor(),\n    normalize\n])\n\n# Helper function to view images in a DataLoader (works with data augmentation transforms or not) \ndef view_dataloader_images(dataloader, n=10):\n    if n &gt; 10:\n        print(f\"Having n higher than 10 will create messy plots, lowering to 10.\")\n        n = 10\n    imgs, labels = next(iter(dataloader))\n    plt.figure(figsize=(16, 8))\n    for i in range(n):\n        # Min max scale the image for display purposes\n        targ_image = imgs[i]\n        sample_min, sample_max = targ_image.min(), targ_image.max()\n        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n\n        # Plot images with appropriate axes information\n        plt.subplot(1, 10, i+1)\n        plt.imshow(sample_scaled.permute(1, 2, 0)) # resize for Matplotlib requirements\n        plt.title(class_names[labels[i]])\n        plt.axis(False)\n\n# Have to update `create_dataloaders()` to handle different augmentations\nimport os\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\n\nNUM_WORKERS = os.cpu_count() # use maximum number of CPUs for workers to load data \n\n# Note: this is an update version of data_setup.create_dataloaders to handle\n# differnt train and test transforms.\ndef create_dataloaders(\n    train_dir, \n    test_dir, \n    train_transform, # add parameter for train transform (transforms on train dataset)\n    test_transform,  # add parameter for test transform (transforms on test dataset)\n    batch_size=32, num_workers=NUM_WORKERS\n):\n    # Use ImageFolder to create dataset(s)\n    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n\n    # Get class names\n    class_names = train_data.classes\n\n    # Turn images into data loaders\n    train_dataloader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    test_dataloader = DataLoader(\n        test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    return train_dataloader, test_dataloader, class_names\n</pre> <ol> <li>Scale up the dataset to turn FoodVision Mini into FoodVision Big using the entire Food101 dataset from <code>torchvision.models</code><ul> <li>You could take the best performing model from your various experiments or even the EffNetB2 feature extractor we created in this notebook and see how it goes fitting for 5 epochs on all of Food101.</li> <li>If you try more than one model, it would be good to have the model's results tracked.</li> <li>If you load the Food101 dataset from <code>torchvision.models</code>, you'll have to create PyTorch DataLoaders to use it in training.</li> <li>Note: Due to the larger amount of data in Food101 compared to our pizza, steak, sushi dataset, this model will take longer to train.</li> </ul> </li> </ol>"},{"location":"Learn/z2m-pytorch/07_pytorch_experiment_tracking/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Read The Bitter Lesson blog post by Richard Sutton to get an idea of how many of the latest advancements in AI have come from increased scale (bigger datasets and bigger models) and more general (less meticulously crafted) methods.</li> <li>Go through the PyTorch YouTube/code tutorial for TensorBoard for 20-minutes and see how it compares to the code we've written in this notebook.</li> <li>Perhaps you may want to view and rearrange your model's TensorBoard logs with a DataFrame (so you can sort the results by lowest loss or highest accuracy), there's a guide for this in the TensorBoard documentation.</li> <li>If you like to use VSCode for development using scripts or notebooks (VSCode can now use Jupyter Notebooks natively), you can setup TensorBoard right within VSCode using the  PyTorch Development in VSCode guide.</li> <li>To go further with experiment tracking and see how your PyTorch model is performing from a speed perspective (are there any bottlenecks that could be improved to speed up training?), see the PyTorch documentation for the PyTorch profiler.</li> <li>Made With ML is an outstanding resource for all things machine learning by Goku Mohandas and their guide on experiment tracking contains a fantastic introduction to tracking machine learning experiments with MLflow.</li> </ul>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/","title":"(WIP) 08. PyTorch Paper Replicating","text":"In\u00a0[2]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>torch version: 1.12.0+cu102\ntorchvision version: 0.13.0+cu102\n</pre> <p>Note: If you're using Google Colab, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of <code>torch</code> and <code>torchvision</code>.</p> <p>Now we'll continue with the regular imports, setting up device agnostic code and this time we'll also get the <code>helper_functions.py</code> script from GitHub.</p> <p>The <code>helper_functions.py</code> script contains several functions we created in previous sections:</p> <ul> <li><code>set_seeds()</code> to set the random seeds (created in 07. PyTorch Experiment Tracking section 0).</li> <li><code>download_data()</code> to download a data source given a link (created in 07. PyTorch Experiment Tracking section 1).</li> <li><code>plot_loss_curves()</code> to inspect our model's training results (created in 04. PyTorch Custom Datasets section 7.8)</li> </ul> <p>Note: It may be a better idea for many of the functions in the <code>helper_functions.py</code> script to be merged into <code>going_modular/going_modular/utils.py</code>, perhaps that's an extension you'd like to try.</p> In\u00a0[3]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>.</p> In\u00a0[4]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[4]: <pre>'cuda'</pre> In\u00a0[5]: Copied! <pre># Download pizza, steak, sushi images from GitHub\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> # Download pizza, steak, sushi images from GitHub image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n</pre> Out[5]: <pre>PosixPath('data/pizza_steak_sushi')</pre> <p>Beautiful! Data downloaded, let's setup the training and test directories.</p> In\u00a0[6]: Copied! <pre># Setup directory paths to train and test images\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n</pre> # Setup directory paths to train and test images train_dir = image_path / \"train\" test_dir = image_path / \"test\" In\u00a0[7]: Copied! <pre># Create image size (from Table 3 in the ViT paper) \nIMG_SIZE = 224\n\n# Create transform pipeline manually\nmanual_transforms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n])           \nprint(f\"Manually created transforms: {manual_transforms}\")\n</pre> # Create image size (from Table 3 in the ViT paper)  IMG_SIZE = 224  # Create transform pipeline manually manual_transforms = transforms.Compose([     transforms.Resize((IMG_SIZE, IMG_SIZE)),     transforms.ToTensor(), ])            print(f\"Manually created transforms: {manual_transforms}\") <pre>Manually created transforms: Compose(\n    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n    ToTensor()\n)\n</pre> In\u00a0[8]: Copied! <pre># Set the batch size\nBATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=BATCH_SIZE\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Set the batch size BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small  # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=manual_transforms, # use manually created transforms     batch_size=BATCH_SIZE )  train_dataloader, test_dataloader, class_names Out[8]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa8cc731550&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fa8cc731520&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[9]: Copied! <pre># Get a batch of images\nimage_batch, label_batch = next(iter(train_dataloader))\n\n# Get a single image from the batch\nimage, label = image_batch[0], label_batch[0]\n\n# View the batch shapes\nimage.shape, label\n</pre> # Get a batch of images image_batch, label_batch = next(iter(train_dataloader))  # Get a single image from the batch image, label = image_batch[0], label_batch[0]  # View the batch shapes image.shape, label Out[9]: <pre>(torch.Size([3, 224, 224]), tensor(0))</pre> <p>Wonderful!</p> <p>Now let's plot the image and its label with <code>matplotlib</code>.</p> In\u00a0[10]: Copied! <pre># Plot image with matplotlib\nplt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels]\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # Plot image with matplotlib plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels] plt.title(class_names[label]) plt.axis(False); <p>Nice!</p> <p>Looks like our images are importing correctly, let's continue with the paper replication.</p> In\u00a0[11]: Copied! <pre># Create example values\nheight = 224 # H (\"The training resolution is 224.\")\nwidth = 224 # W\ncolor_channels = 3 # C\npatch_size = 16 # P\n\n# Calculate N (number of patches)\nnumber_of_patches = int((height * width) / patch_size**2)\nprint(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")\n</pre> # Create example values height = 224 # H (\"The training resolution is 224.\") width = 224 # W color_channels = 3 # C patch_size = 16 # P  # Calculate N (number of patches) number_of_patches = int((height * width) / patch_size**2) print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\") <pre>Number of patches (N) with image height (H=224), width (W=224) and patch size (P=16): 196\n</pre> <p>We've got the number of patches, how about we create the image output size as well?</p> <p>Better yet, let's replicate the input and output shapes of the patch embedding layer.</p> <p>Recall:</p> <ul> <li>Input: The image starts as 2D with size ${H \\times W \\times C}$.</li> <li>Output: The image gets converted to a sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.</li> </ul> In\u00a0[12]: Copied! <pre># Input shape\ninput_shape = (height, width, color_channels)\n\n# Output shape\noutput_shape = (number_of_patches, patch_size**2 * color_channels)\n\nprint(f\"Input shape (2D image): {input_shape}\")\nprint(f\"Output shape (flattened 2D patches): {output_shape}\")\n</pre> # Input shape input_shape = (height, width, color_channels)  # Output shape output_shape = (number_of_patches, patch_size**2 * color_channels)  print(f\"Input shape (2D image): {input_shape}\") print(f\"Output shape (flattened 2D patches): {output_shape}\") <pre>Input shape (2D image): (224, 224, 3)\nOutput shape (flattened 2D patches): (196, 768)\n</pre> <p>Input and output shapes acquired!</p> In\u00a0[13]: Copied! <pre># View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # View single image plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); <p>We want to turn this image into patches of itself inline with Figure 1 of the ViT paper.</p> <p>How about we start by just visualizing the top row of patched pixels?</p> <p>We can do this by indexing on the different image dimensions.</p> In\u00a0[14]: Copied! <pre># Change image shape to be compatible with matplotlib (color_channels, height, width) -&gt; (height, width, color_channels) \nimage_permuted = image.permute(1, 2, 0)\n\n# Index to plot the top row of patched pixels\npatch_size = 16\nplt.figure(figsize=(patch_size, patch_size))\nplt.imshow(image_permuted[:patch_size, :, :]);\n</pre> # Change image shape to be compatible with matplotlib (color_channels, height, width) -&gt; (height, width, color_channels)  image_permuted = image.permute(1, 2, 0)  # Index to plot the top row of patched pixels patch_size = 16 plt.figure(figsize=(patch_size, patch_size)) plt.imshow(image_permuted[:patch_size, :, :]); <p>Now we've got the top row, let's turn it into patches.</p> <p>We can do this by iterating through the number of patches there'd be in the top row.</p> In\u00a0[15]: Copied! <pre># Setup hyperparameters and make sure img_size and patch_size are compatible\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size \nassert img_size % patch_size == 0, \"Image size must be divisible by patch size\" \nprint(f\"Number of patches per row: {num_patches}\")\n\n# Create a series of subplots\nfig, axs = plt.subplots(nrows=1, \n                        ncols=img_size // patch_size, # one column for each patch\n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# Iterate through number of patches in the top row\nfor i, patch in enumerate(range(0, img_size, patch_size)):\n    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n    axs[i].set_xlabel(i+1) # set the label\n    axs[i].set_xticks([])\n    axs[i].set_yticks([])\n</pre> # Setup hyperparameters and make sure img_size and patch_size are compatible img_size = 224 patch_size = 16 num_patches = img_size/patch_size  assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"  print(f\"Number of patches per row: {num_patches}\")  # Create a series of subplots fig, axs = plt.subplots(nrows=1,                          ncols=img_size // patch_size, # one column for each patch                         figsize=(num_patches, num_patches),                         sharex=True,                         sharey=True)  # Iterate through number of patches in the top row for i, patch in enumerate(range(0, img_size, patch_size)):     axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index     axs[i].set_xlabel(i+1) # set the label     axs[i].set_xticks([])     axs[i].set_yticks([]) <pre>Number of patches per row: 14.0\n</pre> <p>Those are some nice looking patches!</p> <p>How about we do it for the whole image?</p> <p>This time we'll iterate through the indexs for height and width and plot each patch as it's own subplot.</p> In\u00a0[16]: Copied! <pre># Setup hyperparameters and make sure img_size and patch_size are compatible\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size \nassert img_size % patch_size == 0, \"Image size must be divisible by patch size\" \nprint(f\"Number of patches per row: {num_patches}\\nNumber of patches per column: {num_patches}\\nTotal patches: {num_patches*num_patches}\")\n\n# Create a series of subplots\nfig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n                        ncols=img_size // patch_size, \n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# Loop through height and width of image\nfor i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n        \n        # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))\n        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height \n                                        patch_width:patch_width+patch_size, # iterate through width\n                                        :]) # get all color channels\n        \n        # Set up label information, remove the ticks for clarity and set labels to outside\n        axs[i, j].set_ylabel(i+1, \n                             rotation=\"horizontal\", \n                             horizontalalignment=\"right\", \n                             verticalalignment=\"center\") \n        axs[i, j].set_xlabel(j+1) \n        axs[i, j].set_xticks([])\n        axs[i, j].set_yticks([])\n        axs[i, j].label_outer()\n\n# Set a super title\nfig.suptitle(f\"{class_names[label]} -&gt; Patchified\", fontsize=16)\nplt.show()\n</pre> # Setup hyperparameters and make sure img_size and patch_size are compatible img_size = 224 patch_size = 16 num_patches = img_size/patch_size  assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"  print(f\"Number of patches per row: {num_patches}\\nNumber of patches per column: {num_patches}\\nTotal patches: {num_patches*num_patches}\")  # Create a series of subplots fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float                         ncols=img_size // patch_size,                          figsize=(num_patches, num_patches),                         sharex=True,                         sharey=True)  # Loop through height and width of image for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height     for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width                  # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))         axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height                                          patch_width:patch_width+patch_size, # iterate through width                                         :]) # get all color channels                  # Set up label information, remove the ticks for clarity and set labels to outside         axs[i, j].set_ylabel(i+1,                               rotation=\"horizontal\",                               horizontalalignment=\"right\",                               verticalalignment=\"center\")          axs[i, j].set_xlabel(j+1)          axs[i, j].set_xticks([])         axs[i, j].set_yticks([])         axs[i, j].label_outer()  # Set a super title fig.suptitle(f\"{class_names[label]} -&gt; Patchified\", fontsize=16) plt.show() <pre>Number of patches per row: 14.0\nNumber of patches per column: 14.0\nTotal patches: 196.0\n</pre> <p>Image patchified!</p> <p>Woah, that looks cool.</p> <p>Now how do we turn each of these patches into an embedding and convert them into a sequence?</p> <p>Hint: we can use PyTorch layers. Can you guess which?</p> In\u00a0[17]: Copied! <pre>from torch import nn\n\n# Set the patch size\npatch_size=16\n\n# Create the Conv2d layer with hyperparameters from the ViT paper\nconv2d = nn.Conv2d(in_channels=3, # number of color channels\n                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n                   stride=patch_size,\n                   padding=0)\n</pre> from torch import nn  # Set the patch size patch_size=16  # Create the Conv2d layer with hyperparameters from the ViT paper conv2d = nn.Conv2d(in_channels=3, # number of color channels                    out_channels=768, # from Table 1: Hidden size D, this is the embedding size                    kernel_size=patch_size, # could also use (patch_size, patch_size)                    stride=patch_size,                    padding=0) <p>Now we've got a convoluational layer, let's see what happens when we pass a single image through it.</p> In\u00a0[18]: Copied! <pre># View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # View single image plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); In\u00a0[19]: Copied! <pre># Pass the image through the convolutional layer \nimage_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels)\nprint(image_out_of_conv.shape)\n</pre> # Pass the image through the convolutional layer  image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels) print(image_out_of_conv.shape) <pre>torch.Size([1, 768, 14, 14])\n</pre> <p>Passing our image through the convolutional layer turns it into a series of 768 (this is the embedding size or $D$) feature/activation maps.</p> <p>So its output shape can be read as:</p> <pre>torch.Size([1, 768, 14, 14]) -&gt; [batch_size, embedding_dim, feature_map_height, feature_map_width]\n</pre> <p>Let's visualize five random feature maps and see what they look like.</p> In\u00a0[20]: Copied! <pre># Plot random 5 convolutional feature maps\nimport random\nrandom_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\nprint(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n\n# Create plot\nfig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n\n# Plot random image feature maps\nfor i, idx in enumerate(random_indexes):\n    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);\n</pre> # Plot random 5 convolutional feature maps import random random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")  # Create plot fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))  # Plot random image feature maps for i, idx in enumerate(random_indexes):     image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer     axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())     axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]); <pre>Showing random convolutional feature maps from indexes: [180, 39, 286, 72, 105]\n</pre> <p>Notice how the feature maps all kind of represent the original image, visualizing a few you can see the different major outlines and some major features.</p> <p>The important thing to note is that these features may change over time as the neural network learns.</p> <p>And because of these, these feature maps can be considered a learnable embedding of our image.</p> <p>Let's check one out in numerical form.</p> In\u00a0[21]: Copied! <pre># Get a single feature map in tensor form\nsingle_feature_map = image_out_of_conv[:, 0, :, :]\nsingle_feature_map, single_feature_map.requires_grad\n</pre> # Get a single feature map in tensor form single_feature_map = image_out_of_conv[:, 0, :, :] single_feature_map, single_feature_map.requires_grad Out[21]: <pre>(tensor([[[0.2642, 1.1367, 1.0221, 0.9712, 1.0950, 1.2478, 1.2884, 1.1481,\n           0.9640, 0.6204, 0.5996, 0.5855, 0.5560, 0.4992],\n          [1.1578, 1.0633, 0.9593, 1.1931, 1.3194, 1.1306, 0.7317, 0.4322,\n           0.6025, 0.7246, 0.7891, 0.5383, 0.4786, 0.7098],\n          [1.0163, 0.9368, 1.1675, 1.3373, 1.0429, 0.7497, 0.7445, 0.4270,\n           0.7430, 0.8750, 0.6105, 0.5147, 1.0207, 0.7852],\n          [1.0669, 1.0157, 1.3291, 1.0117, 0.4848, 0.6802, 0.8365, 0.7736,\n           0.8618, 0.9144, 0.8926, 0.9795, 0.7475, 0.7585],\n          [0.9371, 1.1937, 1.0068, 0.6377, 0.7283, 0.9625, 1.0372, 0.8920,\n           0.9372, 0.9034, 0.9683, 0.9405, 0.5958, 0.8740],\n          [0.9419, 0.8599, 0.5429, 0.6954, 1.0202, 0.9093, 1.0003, 0.7619,\n           0.8472, 0.8062, 0.6418, 0.7741, 0.5791, 0.9816],\n          [0.7965, 0.7202, 0.6424, 0.9137, 0.8264, 1.0243, 1.0920, 0.9548,\n           0.9166, 0.7937, 0.4675, 0.5346, 0.7774, 1.1001],\n          [0.3298, 0.4832, 0.5324, 0.7486, 0.8303, 0.8101, 0.9969, 0.9931,\n           1.0058, 0.6002, 0.6643, 0.7254, 0.8453, 1.1323],\n          [0.5384, 0.4798, 0.6725, 0.8014, 0.7044, 0.7988, 0.8185, 0.8911,\n           0.9720, 0.8939, 0.6234, 0.5674, 0.5775, 1.0011],\n          [0.6199, 0.6465, 0.6503, 0.6215, 0.8154, 0.7950, 0.8647, 0.9872,\n           0.8513, 0.8833, 0.5799, 0.5914, 0.6936, 1.0554],\n          [0.5140, 0.6462, 0.6982, 0.7445, 0.7394, 0.8124, 0.7462, 0.9183,\n           0.7471, 0.9436, 0.7147, 0.6396, 0.5795, 1.0201],\n          [0.5467, 0.7408, 0.6854, 0.6624, 0.7465, 0.5077, 0.7633, 0.8709,\n           1.0026, 0.7276, 0.7847, 0.5811, 0.5521, 1.0318],\n          [0.8041, 0.8868, 0.5559, 0.5889, 0.7236, 0.6976, 0.7940, 0.9365,\n           0.9110, 0.8182, 0.7013, 0.4890, 0.8364, 1.0031],\n          [0.1976, 1.0262, 1.1979, 0.9982, 0.9644, 0.8868, 0.9556, 1.0204,\n           1.0060, 0.9586, 0.9351, 0.8819, 0.9290, 0.9289]]],\n        grad_fn=&lt;SliceBackward0&gt;),\n True)</pre> <p>The <code>grad_fn</code> output of the <code>single_feature_map</code> and the <code>required_grad=True</code> attribute means PyTorch is tracking the gradients of this feature map and it will be updated by gradient descent during training.</p> In\u00a0[22]: Copied! <pre># Current tensor shape\nprint(f\"Current tensor shape: {image_out_of_conv.shape} -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\")\n</pre> # Current tensor shape print(f\"Current tensor shape: {image_out_of_conv.shape} -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\") <pre>Current tensor shape: torch.Size([1, 768, 14, 14]) -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\n</pre> <p>Well we've got the 768 part ( $(P^{2} \\cdot C)$ ) but we still need the number of patches ($N$).</p> <p>Reading back through section 3.1 of the ViT paper it says (bold mine):</p> <p>As a special case, the patches can have spatial size $1 \\times 1$, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension.</p> <p>Flattening the spatial dimensions of the feature map hey?</p> <p>What layer do we have in PyTorch that can flatten?</p> <p>How about <code>torch.nn.Flatten()</code>?</p> <p>But we don't want to flatten the whole tensor, we only want to flatten the \"spatial dimensions of the feature map\".</p> <p>Which in our case is the <code>feature_map_height</code> and <code>feature_map_width</code> dimensions of <code>image_out_of_conv</code>.</p> <p>So how about we create a <code>torch.nn.Flatten()</code> layer to only flatten those dimensions, we can use the <code>start_dim</code> and <code>end_dim</code> parameters to set that up?</p> In\u00a0[23]: Copied! <pre># Create flatten layer\nflatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n                     end_dim=3) # flatten feature_map_width (dimension 3)\n</pre> # Create flatten layer flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)                      end_dim=3) # flatten feature_map_width (dimension 3) <p>Nice! Now let's put it all together!</p> <p>We'll:</p> <ol> <li>Take a single image.</li> <li>Put in through the convolutional layer (<code>conv2d</code>) to turn the image into 2D feature maps (patch embeddings).</li> <li>Flatten the 2D feature map into a single sequence.</li> </ol> In\u00a0[24]: Copied! <pre># 1. View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\nprint(f\"Original image shape: {image.shape}\")\n\n# 2. Turn image into feature maps\nimage_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\nprint(f\"Image feature map shape: {image_out_of_conv.shape}\")\n\n# 3. Flatten the feature maps\nimage_out_of_conv_flattened = flatten(image_out_of_conv)\nprint(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")\n</pre> # 1. View single image plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); print(f\"Original image shape: {image.shape}\")  # 2. Turn image into feature maps image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors print(f\"Image feature map shape: {image_out_of_conv.shape}\")  # 3. Flatten the feature maps image_out_of_conv_flattened = flatten(image_out_of_conv) print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\") <pre>Original image shape: torch.Size([3, 224, 224])\nImage feature map shape: torch.Size([1, 768, 14, 14])\nFlattened image feature map shape: torch.Size([1, 768, 196])\n</pre> <p>Woohoo! It looks like our <code>image_out_of_conv_flattened</code> shape is very close to our desired output shape:</p> <ul> <li>Desried output (flattened 2D patches): (196, 768) -&gt; ${N \\times\\left(P^{2} \\cdot C\\right)}$</li> <li>Current shape: (1, 768, 196)</li> </ul> <p>The only difference is our current shape has a batch size and the dimensions are in a different order to the desired output.</p> <p>How could we fix this?</p> <p>Well, how about we rearrange the dimensions?</p> <p>We can do so with <code>torch.Tensor.permute()</code> just like we do when rearranging image tensors to plot them with matplotlib.</p> <p>Let's try.</p> In\u00a0[25]: Copied! <pre># Get flattened image patch embeddings in right shape \nimage_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C]\nprint(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -&gt; [batch_size, num_patches, embedding_size]\")\n</pre> # Get flattened image patch embeddings in right shape  image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C] print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -&gt; [batch_size, num_patches, embedding_size]\") <pre>Patch embedding sequence shape: torch.Size([1, 196, 768]) -&gt; [batch_size, num_patches, embedding_size]\n</pre> <p>Yes!!!</p> <p>We've now matched the desired input and output shapes for the patch embedding layer of the ViT architecture using a couple of PyTorch layers.</p> <p>How about we visualize one of the flattened feature maps?</p> In\u00a0[26]: Copied! <pre># Get a single flattened feature map\nsingle_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0]\n\n# Plot the flattened feature map visually\nplt.figure(figsize=(22, 22))\nplt.imshow(single_flattened_feature_map.detach().numpy())\nplt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\nplt.axis(False);\n</pre> # Get a single flattened feature map single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0]  # Plot the flattened feature map visually plt.figure(figsize=(22, 22)) plt.imshow(single_flattened_feature_map.detach().numpy()) plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\") plt.axis(False); <p>Hmm, the flattened feature map doesn't look like much visually, but that's not what we're concerned about, this is what will be the output of the patching embedding layer and the input to the rest of the ViT architecture.</p> <p>TK image - single image -&gt; conv2d -&gt; flatten -&gt; get the output above (show the workflow and transformation, this could be the gif we've but using but extended to work with the flatten section)</p> <p>Note: The original Transformer architecture was designed to work with text. The Vision Transformer architecture (ViT) had the goal of using the original Transformer for images. This is why the input to the ViT architecture is processed in the way it is. We're essentially taking a 2D image and formatting it so it appears as a 1D sequence of text.</p> <p>How about we view the flattened feature map in tensor form?</p> In\u00a0[27]: Copied! <pre># See the flattened feature map as a tensor\nsingle_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape\n</pre> # See the flattened feature map as a tensor single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape Out[27]: <pre>(tensor([[0.2642, 1.1367, 1.0221, 0.9712, 1.0950, 1.2478, 1.2884, 1.1481, 0.9640,\n          0.6204, 0.5996, 0.5855, 0.5560, 0.4992, 1.1578, 1.0633, 0.9593, 1.1931,\n          1.3194, 1.1306, 0.7317, 0.4322, 0.6025, 0.7246, 0.7891, 0.5383, 0.4786,\n          0.7098, 1.0163, 0.9368, 1.1675, 1.3373, 1.0429, 0.7497, 0.7445, 0.4270,\n          0.7430, 0.8750, 0.6105, 0.5147, 1.0207, 0.7852, 1.0669, 1.0157, 1.3291,\n          1.0117, 0.4848, 0.6802, 0.8365, 0.7736, 0.8618, 0.9144, 0.8926, 0.9795,\n          0.7475, 0.7585, 0.9371, 1.1937, 1.0068, 0.6377, 0.7283, 0.9625, 1.0372,\n          0.8920, 0.9372, 0.9034, 0.9683, 0.9405, 0.5958, 0.8740, 0.9419, 0.8599,\n          0.5429, 0.6954, 1.0202, 0.9093, 1.0003, 0.7619, 0.8472, 0.8062, 0.6418,\n          0.7741, 0.5791, 0.9816, 0.7965, 0.7202, 0.6424, 0.9137, 0.8264, 1.0243,\n          1.0920, 0.9548, 0.9166, 0.7937, 0.4675, 0.5346, 0.7774, 1.1001, 0.3298,\n          0.4832, 0.5324, 0.7486, 0.8303, 0.8101, 0.9969, 0.9931, 1.0058, 0.6002,\n          0.6643, 0.7254, 0.8453, 1.1323, 0.5384, 0.4798, 0.6725, 0.8014, 0.7044,\n          0.7988, 0.8185, 0.8911, 0.9720, 0.8939, 0.6234, 0.5674, 0.5775, 1.0011,\n          0.6199, 0.6465, 0.6503, 0.6215, 0.8154, 0.7950, 0.8647, 0.9872, 0.8513,\n          0.8833, 0.5799, 0.5914, 0.6936, 1.0554, 0.5140, 0.6462, 0.6982, 0.7445,\n          0.7394, 0.8124, 0.7462, 0.9183, 0.7471, 0.9436, 0.7147, 0.6396, 0.5795,\n          1.0201, 0.5467, 0.7408, 0.6854, 0.6624, 0.7465, 0.5077, 0.7633, 0.8709,\n          1.0026, 0.7276, 0.7847, 0.5811, 0.5521, 1.0318, 0.8041, 0.8868, 0.5559,\n          0.5889, 0.7236, 0.6976, 0.7940, 0.9365, 0.9110, 0.8182, 0.7013, 0.4890,\n          0.8364, 1.0031, 0.1976, 1.0262, 1.1979, 0.9982, 0.9644, 0.8868, 0.9556,\n          1.0204, 1.0060, 0.9586, 0.9351, 0.8819, 0.9290, 0.9289]],\n        grad_fn=&lt;SelectBackward0&gt;),\n True,\n torch.Size([1, 196]))</pre> <p>Beautiful!</p> <p>We've turned our single 2D image into a single 1D learnable embedding vector (or \"Linear Projection of Flattned Patches\" in Figure 1 of the ViT paper).</p> In\u00a0[28]: Copied! <pre># 1. Create a class which subclasses nn.Module\nclass PatchEmbedding(nn.Module):\n\"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n    Args:\n        in_channels (int): Number of color channels for the input images. Defaults to 3.\n        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n    \"\"\" \n    # 2. Initialize the class with appropriate variables\n    def __init__(self, \n                 in_channels:int=3,\n                 patch_size:int=16,\n                 embedding_dim:int=768):\n        super().__init__()\n        \n        # 3. Create a layer to turn an image into patches\n        self.patcher = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=embedding_dim,\n                                 kernel_size=patch_size,\n                                 stride=patch_size,\n                                 padding=0)\n\n        # 4. Create a layer to flatten the patch feature maps into a single dimension\n        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n                                  end_dim=3)\n\n    # 5. Define the forward method \n    def forward(self, x):\n        # Create assertion to check that inputs are the correct shape\n        image_resolution = x.shape[-1]\n        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n        \n        # Perform the forward pass\n        x_patched = self.patcher(x)\n        x_flattened = self.flatten(x_patched) \n        # 6. Make sure the output shape has the right order \n        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C]\n</pre> # 1. Create a class which subclasses nn.Module class PatchEmbedding(nn.Module):     \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.          Args:         in_channels (int): Number of color channels for the input images. Defaults to 3.         patch_size (int): Size of patches to convert input image into. Defaults to 16.         embedding_dim (int): Size of embedding to turn image into. Defaults to 768.     \"\"\"      # 2. Initialize the class with appropriate variables     def __init__(self,                   in_channels:int=3,                  patch_size:int=16,                  embedding_dim:int=768):         super().__init__()                  # 3. Create a layer to turn an image into patches         self.patcher = nn.Conv2d(in_channels=in_channels,                                  out_channels=embedding_dim,                                  kernel_size=patch_size,                                  stride=patch_size,                                  padding=0)          # 4. Create a layer to flatten the patch feature maps into a single dimension         self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector                                   end_dim=3)      # 5. Define the forward method      def forward(self, x):         # Create assertion to check that inputs are the correct shape         image_resolution = x.shape[-1]         assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"                  # Perform the forward pass         x_patched = self.patcher(x)         x_flattened = self.flatten(x_patched)          # 6. Make sure the output shape has the right order          return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C] <p><code>PatchEmbedding</code> layer created!</p> <p>Let's try it out on a single image.</p> In\u00a0[29]: Copied! <pre>set_seeds()\n\n# Create an instance of patch embedding layer\npatchify = PatchEmbedding(in_channels=3,\n                          patch_size=16,\n                          embedding_dim=768)\n\n# Pass a single image through\nprint(f\"Input image shape: {image.unsqueeze(0).shape}\")\npatch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\nprint(f\"Output patch embedding shape: {patch_embedded_image.shape}\")\n</pre> set_seeds()  # Create an instance of patch embedding layer patchify = PatchEmbedding(in_channels=3,                           patch_size=16,                           embedding_dim=768)  # Pass a single image through print(f\"Input image shape: {image.unsqueeze(0).shape}\") patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error print(f\"Output patch embedding shape: {patch_embedded_image.shape}\") <pre>Input image shape: torch.Size([1, 3, 224, 224])\nOutput patch embedding shape: torch.Size([1, 196, 768])\n</pre> <p>Beautiful!</p> <p>The output shape matches the ideal input and output shapes we'd like to see from the patch embedding layer:</p> <ul> <li>Input: The image starts as 2D with size ${H \\times W \\times C}$.</li> <li>Output: The image gets converted to a sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.</li> </ul> <p>Where:</p> <ul> <li>$(H, W)$ is the resolution of the original image.</li> <li>$C$ is the number of channels.</li> <li>$(P, P)$ is the resolution of each image patch (patch size).</li> <li>$N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.</li> </ul> <p>We've now replicated the patch embedding for equation 1 but not the class token/position embedding.</p> <p>We'll get to these later on.</p> <p></p> <p>Our <code>PatchEmbedding</code> class (right) replicates the patch embedding of the ViT architecture from Figure 1 and Equation 1 from the ViT paper (left). However, the learnable class embedding and position embeddings haven't been created yet. These will come soon.</p> <p>Let's now get a summary of our <code>PatchEmbedding</code> layer.</p> In\u00a0[30]: Copied! <pre># Create random input sizes\nrandom_input_image = (1, 3, 224, 224)\nrandom_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size\n\n# Get a summary of the input and outputs of PatchEmbedding\nsummary(PatchEmbedding(), \n        input_size=random_input_image, # try swapping this for \"random_input_image_error\" \n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])\n</pre> # Create random input sizes random_input_image = (1, 3, 224, 224) random_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size  # Get a summary of the input and outputs of PatchEmbedding summary(PatchEmbedding(),          input_size=random_input_image, # try swapping this for \"random_input_image_error\"          col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"]) Out[30]: <pre>========================================================================================================================\nLayer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n========================================================================================================================\nPatchEmbedding (PatchEmbedding)          [1, 3, 224, 224]     [1, 196, 768]        --                   True\n\u251c\u2500Conv2d (patcher)                       [1, 3, 224, 224]     [1, 768, 14, 14]     590,592              True\n\u251c\u2500Flatten (flatten)                      [1, 768, 14, 14]     [1, 768, 196]        --                   --\n========================================================================================================================\nTotal params: 590,592\nTrainable params: 590,592\nNon-trainable params: 0\nTotal mult-adds (M): 115.76\n========================================================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 1.20\nParams size (MB): 2.36\nEstimated Total Size (MB): 4.17\n========================================================================================================================</pre> In\u00a0[31]: Copied! <pre># View the patch embedding and patch embedding shape\nprint(patch_embedded_image) \nprint(f\"Patch embedding shape: {patch_embedded_image.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # View the patch embedding and patch embedding shape print(patch_embedded_image)  print(f\"Patch embedding shape: {patch_embedded_image.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[-0.4923,  0.0265, -0.0909,  ...,  0.1478, -0.0986,  0.2243],\n         [-0.9849,  0.3805, -0.3638,  ...,  0.6115, -0.0805,  0.2097],\n         [-0.6015,  0.1235, -0.2506,  ...,  0.6307, -0.4673,  0.2756],\n         ...,\n         [-0.6668,  0.1713, -0.1711,  ...,  0.4699, -0.2881,  0.2599],\n         [-0.6983,  0.1949, -0.1884,  ...,  0.5152, -0.3126,  0.2151],\n         [-0.6889,  0.1862, -0.1444,  ...,  0.5019, -0.3564,  0.2378]]],\n       grad_fn=&lt;PermuteBackward0&gt;)\nPatch embedding shape: torch.Size([1, 196, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>To \"prepend a learnable embedding to the sequence of embedded patches\" we need to create a learnable embedding in the shape of the <code>embedding_dimension</code> ($D$) and then add it to the <code>number_of_patches</code> dimension.</p> <p>Or in pseudocode:</p> <pre>patch_embedding = [image_patch_1, image_patch_2, image_patch_3...]\nclass_token = learnable_embedding\npatch_embedding_with_class_token = torch.cat((class_token, patch_embedding), dim=1)\n</pre> <p>Notice the concatenation (<code>torch.cat()</code>) happens on <code>dim=1</code> (the <code>number_of_patches</code> dimension).</p> <p>Let's create a learnable embedding for the class token.</p> <p>To do so, we'll get the batch size and embedding dimension shape and then we'll create a <code>torch.ones()</code> tensor in the shape <code>[batch_size, 1, embedding_dimension]</code>.</p> <p>And we'll make the tensor learnable by passing it to <code>nn.Parameter()</code> with <code>requires_grad=True</code>.</p> In\u00a0[32]: Copied! <pre># Get the batch size and embedding dimension\nbatch_size = patch_embedded_image.shape[0]\nembedding_dimension = patch_embedded_image.shape[-1]\n\n# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_patches, embedding_dimension]\n                           requires_grad=True) # make sure the embedding is learnable\n\n# Show the first 10 examples of the class_token\nprint(class_token[:, :, :10])\n\n# Print the class_token shape\nprint(f\"Class token shape: {class_token.shape} -&gt; [batch_size, number_of_tokens, embedding_dimension]\")\n</pre> # Get the batch size and embedding dimension batch_size = patch_embedded_image.shape[0] embedding_dimension = patch_embedded_image.shape[-1]  # Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D) class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_patches, embedding_dimension]                            requires_grad=True) # make sure the embedding is learnable  # Show the first 10 examples of the class_token print(class_token[:, :, :10])  # Print the class_token shape print(f\"Class token shape: {class_token.shape} -&gt; [batch_size, number_of_tokens, embedding_dimension]\") <pre>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)\nClass token shape: torch.Size([1, 1, 768]) -&gt; [batch_size, number_of_tokens, embedding_dimension]\n</pre> <p>Note: Here we're only creating the class token embedding as <code>torch.ones()</code> for demonstration purposes, in reality, you'd likely create the class token embedding with <code>torch.randn()</code> (start with a random number).</p> <p>See how the <code>number_of_patches</code> dimension of <code>class_token</code> is <code>1</code> since we only want to prepend one class token value to the start of the patch embedding sequence.</p> <p>Now we've got the class token embedding, let's prepend it to our sequence of image patches, <code>patch_embedded_image</code>.</p> <p>We can so using <code>torch.cat()</code> and set <code>dim=1</code> (so <code>class_token</code>'s <code>number_of_patches</code> dimension is preprended to <code>patch_embedded_image</code>'s <code>number_of_patches</code> dimension).</p> In\u00a0[33]: Copied! <pre># Add the class token embedding to the front of the patch embedding\npatch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image), \n                                                      dim=1) # concat on first dimension\n\n# Print the sequence of patch embeddings with the prepended class token embedding\nprint(patch_embedded_image_with_class_embedding)\nprint(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Add the class token embedding to the front of the patch embedding patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),                                                        dim=1) # concat on first dimension  # Print the sequence of patch embeddings with the prepended class token embedding print(patch_embedded_image_with_class_embedding) print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n         [-0.4923,  0.0265, -0.0909,  ...,  0.1478, -0.0986,  0.2243],\n         [-0.9849,  0.3805, -0.3638,  ...,  0.6115, -0.0805,  0.2097],\n         ...,\n         [-0.6668,  0.1713, -0.1711,  ...,  0.4699, -0.2881,  0.2599],\n         [-0.6983,  0.1949, -0.1884,  ...,  0.5152, -0.3126,  0.2151],\n         [-0.6889,  0.1862, -0.1444,  ...,  0.5019, -0.3564,  0.2378]]],\n       grad_fn=&lt;CatBackward0&gt;)\nSequence of patch embeddings with class token prepended shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>Nice! Learnable class token prepended!</p> <p></p> <p>Reviewing what we've done to create the learnable class token, we start with a sequence of image patch embeddings created by <code>PatchEmbedding()</code> on single image, we then created a learnable class token with one value for each of the embedding dimensions and then prepended it to the original sequence of patch embeddings. Note: Using <code>torch.ones()</code> to create the learnable class token is mostly for demonstration purposes only, in practice, you'd like create it with <code>torch.randn()</code>.</p> In\u00a0[34]: Copied! <pre># View the sequence of patch embeddings with the prepended class embedding\npatch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape\n</pre> # View the sequence of patch embeddings with the prepended class embedding patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape Out[34]: <pre>(tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n          [-0.4923,  0.0265, -0.0909,  ...,  0.1478, -0.0986,  0.2243],\n          [-0.9849,  0.3805, -0.3638,  ...,  0.6115, -0.0805,  0.2097],\n          ...,\n          [-0.6668,  0.1713, -0.1711,  ...,  0.4699, -0.2881,  0.2599],\n          [-0.6983,  0.1949, -0.1884,  ...,  0.5152, -0.3126,  0.2151],\n          [-0.6889,  0.1862, -0.1444,  ...,  0.5019, -0.3564,  0.2378]]],\n        grad_fn=&lt;CatBackward0&gt;),\n torch.Size([1, 197, 768]))</pre> <p>Equation 1 states that the position embeddings should have the shape $(N + 1) \\times D$ where:</p> <ul> <li>$N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.</li> <li>$D$ is the size of the patch embeddings, different values for $D$ can be found in Table 1.</li> </ul> <p>Luckily we've got both of these values already.</p> <p>So let's make a learnable 1D embedding with <code>torch.ones()</code> to create $\\mathbf{E}_{\\text {pos }}$.</p> In\u00a0[35]: Copied! <pre># Calculate N (number of patches)\nnumber_of_patches = int((height * width) / patch_size**2)\n\n# Get embedding dimension\nembedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n\n# Create the learnable 1D position embedding\nposition_embedding = nn.Parameter(torch.ones(1,\n                                             number_of_patches+1, \n                                             embedding_dimension),\n                                  requires_grad=True) # make sure it's learnable\n\n# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\nprint(position_embedding[:, :10, :10])\nprint(f\"Position embeddding shape: {position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Calculate N (number of patches) number_of_patches = int((height * width) / patch_size**2)  # Get embedding dimension embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]  # Create the learnable 1D position embedding position_embedding = nn.Parameter(torch.ones(1,                                              number_of_patches+1,                                               embedding_dimension),                                   requires_grad=True) # make sure it's learnable  # Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding print(position_embedding[:, :10, :10]) print(f\"Position embeddding shape: {position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)\nPosition embeddding shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>Note: Only creating the position embedding as <code>torch.ones()</code> for demonstration purposes, in reality, you'd likely create the position embedding with <code>torch.randn()</code> (start with a random number and improve via gradient descent).</p> <p>Position embeddings created!</p> <p>Let's add them to our sequence of patch embeddings with a prepended class token.</p> In\u00a0[36]: Copied! <pre># Add the position embedding to the patch and class token embedding\npatch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\nprint(patch_and_position_embedding)\nprint(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Add the position embedding to the patch and class token embedding patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding print(patch_and_position_embedding) print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[2.0000, 2.0000, 2.0000,  ..., 2.0000, 2.0000, 2.0000],\n         [0.5077, 1.0265, 0.9091,  ..., 1.1478, 0.9014, 1.2243],\n         [0.0151, 1.3805, 0.6362,  ..., 1.6115, 0.9195, 1.2097],\n         ...,\n         [0.3332, 1.1713, 0.8289,  ..., 1.4699, 0.7119, 1.2599],\n         [0.3017, 1.1949, 0.8116,  ..., 1.5152, 0.6874, 1.2151],\n         [0.3111, 1.1862, 0.8556,  ..., 1.5019, 0.6436, 1.2378]]],\n       grad_fn=&lt;AddBackward0&gt;)\nPatch embeddings, class token prepended and positional embeddings added shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>Notice how the values of each of the elements in the embedding tensor increases by 1 (this is because of the position embeddings being created with <code>torch.ones()</code>).</p> <p>Note: We could put both the class token embedding and position embedding into their own layer if we wanted to. But we'll see later on how they can be incorporated into the overall ViT architecture's <code>forward()</code> method.</p> <p></p> <p>The workflow we've used for adding the position embeddings to the sequence of patch embeddings and class token. Note: <code>torch.ones()</code> only used to create embeddings for illustration purposes, in practice, you'd likely use <code>torch.randn()</code> to start with a random number.</p> In\u00a0[37]: Copied! <pre>set_seeds()\n\n# 1. Set patch size\npatch_size = 16\n\n# 2. Print shape of original image tensor and get the image dimensions\nprint(f\"Image tensor shape: {image.shape}\")\nheight, width = image.shape[1], image.shape[2]\n\n# 3. Get image tensor and add batch dimension\nx = image.unsqueeze(0)\nprint(f\"Input image with batch dimension shape: {x.shape}\")\n\n# 4. Create patch embedding layer\npatch_embedding_layer = PatchEmbedding(in_channels=3,\n                                       patch_size=patch_size,\n                                       embedding_dim=768)\n\n# 5. Pass image through patch embedding layer\npatch_embedding = patch_embedding_layer(x)\nprint(f\"Patching embedding shape: {patch_embedding.shape}\")\n\n# 6. Create class token embedding\nbatch_size = patch_embedding.shape[0]\nembedding_dimension = patch_embedding.shape[-1]\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n                           requires_grad=True) # make sure it's learnable\nprint(f\"Class token embedding shape: {class_token.shape}\")\n\n# 7. Prepend class token embedding to patch embedding\npatch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\nprint(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n\n# 8. Create position embedding\nnumber_of_patches = int((height * width) / patch_size**2)\nposition_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n                                  requires_grad=True) # make sure it's learnable\n\n# 9. Add position embedding to patch embedding with class token\npatch_and_position_embedding = patch_embedding_class_token + position_embedding\nprint(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")\n</pre> set_seeds()  # 1. Set patch size patch_size = 16  # 2. Print shape of original image tensor and get the image dimensions print(f\"Image tensor shape: {image.shape}\") height, width = image.shape[1], image.shape[2]  # 3. Get image tensor and add batch dimension x = image.unsqueeze(0) print(f\"Input image with batch dimension shape: {x.shape}\")  # 4. Create patch embedding layer patch_embedding_layer = PatchEmbedding(in_channels=3,                                        patch_size=patch_size,                                        embedding_dim=768)  # 5. Pass image through patch embedding layer patch_embedding = patch_embedding_layer(x) print(f\"Patching embedding shape: {patch_embedding.shape}\")  # 6. Create class token embedding batch_size = patch_embedding.shape[0] embedding_dimension = patch_embedding.shape[-1] class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),                            requires_grad=True) # make sure it's learnable print(f\"Class token embedding shape: {class_token.shape}\")  # 7. Prepend class token embedding to patch embedding patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1) print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")  # 8. Create position embedding number_of_patches = int((height * width) / patch_size**2) position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),                                   requires_grad=True) # make sure it's learnable  # 9. Add position embedding to patch embedding with class token patch_and_position_embedding = patch_embedding_class_token + position_embedding print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\") <pre>Image tensor shape: torch.Size([3, 224, 224])\nInput image with batch dimension shape: torch.Size([1, 3, 224, 224])\nPatching embedding shape: torch.Size([1, 196, 768])\nClass token embedding shape: torch.Size([1, 1, 768])\nPatch embedding with class token shape: torch.Size([1, 197, 768])\nPatch and position embedding shape: torch.Size([1, 197, 768])\n</pre> <p>Woohoo!</p> <p>From a single image to patch and position embeddings in a single cell of code.</p> <p></p> <p>Mapping equation 1 from the ViT paper to our PyTorch code. This is the essence of paper replicating, taking a research paper and turning it into usable code.</p> <p>Now we've got a way to encode our images and pass them to the Transformer Encoder in Figure 1 of the ViT paper.</p> <p>Animating the entire ViT workflow: from patch embeddings to transformer encoder to MLP head.</p> <p>From a code perspective, creating the patch embedding is probably the largest section of replicating the ViT paper.</p> <p>Many of the other parts of the ViT paper such as the Multi-Head Attention and Norm layers can be created using existing PyTorch layers.</p> <p>Onwards!</p> In\u00a0[38]: Copied! <pre># 1. Create a class that inherits from nn.Module\nclass MultiheadSelfAttentionBlock(nn.Module):\n\"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n    \"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1\n    def __init__(self,\n                 embedding_dim:int=768, # from Table 1 for ViT-Base\n                 num_heads:int=12, # from Table 1 for ViT-Base\n                 attn_dropout:int=0): # doesn't look like the paper uses any dropout in MSABlocks\n        super().__init__()\n        \n        # 3. Create the Norm layer (LN)\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        \n        # 4. Create the Multi-Head Attention (MSA) layer\n        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n                                                    num_heads=num_heads,\n                                                    dropout=attn_dropout,\n                                                    batch_first=True) # does our batch dimension come first?\n        \n    # 5. Create a forward() method to pass the data throguh the layers\n    def forward(self, x):\n        x = self.layer_norm(x)\n        attn_output, _ = self.multihead_attn(query=x, # query embeddings \n                                             key=x, # key embeddings\n                                             value=x, # value embeddings\n                                             need_weights=False) # do we need the weights or just the layer outputs?\n        return attn_output\n</pre> # 1. Create a class that inherits from nn.Module class MultiheadSelfAttentionBlock(nn.Module):     \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).     \"\"\"     # 2. Initialize the class with hyperparameters from Table 1     def __init__(self,                  embedding_dim:int=768, # from Table 1 for ViT-Base                  num_heads:int=12, # from Table 1 for ViT-Base                  attn_dropout:int=0): # doesn't look like the paper uses any dropout in MSABlocks         super().__init__()                  # 3. Create the Norm layer (LN)         self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)                  # 4. Create the Multi-Head Attention (MSA) layer         self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,                                                     num_heads=num_heads,                                                     dropout=attn_dropout,                                                     batch_first=True) # does our batch dimension come first?              # 5. Create a forward() method to pass the data throguh the layers     def forward(self, x):         x = self.layer_norm(x)         attn_output, _ = self.multihead_attn(query=x, # query embeddings                                               key=x, # key embeddings                                              value=x, # value embeddings                                              need_weights=False) # do we need the weights or just the layer outputs?         return attn_output <p>Note: Unlike Figure 1, our <code>MultiheadSelfAttentionBlock()</code> doesn't include a skip or residual connection (\"$+\\mathbf{z}_{\\ell-1}$\" in equation 2), we'll include this when we create the entire Transformer encoder later on.</p> <p>MSABlock created!</p> <p>Let's try it out by create an instance of our <code>MultiheadSelfAttentionBlock</code> and passing through the <code>patch_and_position_embedding</code> variable we created in section 4.8.</p> In\u00a0[40]: Copied! <pre># Create an instance of MSABlock\nmultihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1 \n                                                             num_heads=12) # from Table 1\n\n# Pass patch and position image embedding through MSABlock\npatched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\nprint(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\nprint(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")\n</pre> # Create an instance of MSABlock multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1                                                               num_heads=12) # from Table 1  # Pass patch and position image embedding through MSABlock patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding) print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\") print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\") <pre>Input shape of MSA block: torch.Size([1, 197, 768])\nOutput shape MSA block: torch.Size([1, 197, 768])\n</pre> <p>Notice how the input and output shape of our data stays the same when it goes through the MSA block.</p> <p>This doesn't mean the data doesn't change as it goes through.</p> <p>You could try printing the input and output tensor to see how it changes (though this change will be across <code>1 * 197 * 768</code> values).</p> <p></p> <p>Left: Vision Transformer architecture from Figure 1 with Multi-Head Attention and LayerNorm layers highlighted, these layers make up equation 2 from section 3.1 of the paper. Right: Replicating equation 2 (without the skip connection on the end) using PyTorch layers.</p> <p>We've now officially replicated equation 2 (except for the residual connection on the end but we'll get to this in section 7)!</p> <p>Onto the next!</p> In\u00a0[39]: Copied! <pre># Could also call this \"FeedForward\"\nclass MLPBlock(nn.Module):\n\"\"\"Creates an MLPBlock of the Vision Transformer architecture.\"\"\"\n    def __init__(self,\n                 embedding_dim, # embedding dimension (Hidden Size D in Table 1)\n                 mlp_size, # MLP size in Table 1\n                 dropout=0): # \"Dropout... is applied to every dense layer... (Appendix B.1)\"\n        super().__init__()\n        \n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        \n        self.mlp = nn.Sequential(\n            nn.Linear(in_features=embedding_dim,\n                      out_features=mlp_size),\n            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n            nn.Dropout(p=dropout),\n            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n                      out_features=embedding_dim), # take back to embedding_dim\n            nn.Dropout(p=dropout)\n        )\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        return x\n</pre> # Could also call this \"FeedForward\" class MLPBlock(nn.Module):     \"\"\"Creates an MLPBlock of the Vision Transformer architecture.\"\"\"     def __init__(self,                  embedding_dim, # embedding dimension (Hidden Size D in Table 1)                  mlp_size, # MLP size in Table 1                  dropout=0): # \"Dropout... is applied to every dense layer... (Appendix B.1)\"         super().__init__()                  self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)                  self.mlp = nn.Sequential(             nn.Linear(in_features=embedding_dim,                       out_features=mlp_size),             nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"             nn.Dropout(p=dropout),             nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above                       out_features=embedding_dim), # take back to embedding_dim             nn.Dropout(p=dropout)         )      def forward(self, x):         x = self.layer_norm(x)         x = self.mlp(x)         return x In\u00a0[40]: Copied! <pre>mlp_block = MLPBlock(embedding_dim=768, # Table 1 \n                     mlp_size=3072) # Table 1\npatched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\npatched_image_through_mlp_block.shape\n</pre> mlp_block = MLPBlock(embedding_dim=768, # Table 1                       mlp_size=3072) # Table 1 patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block) patched_image_through_mlp_block.shape Out[40]: <pre>torch.Size([1, 197, 768])</pre> In\u00a0[41]: Copied! <pre>class TransformerEncoderBlock(nn.Module):\n\"\"\"Creates a Transformer Encoder block.\"\"\"\n    def __init__(self,\n                 embedding_dim=768, # From Table 1\n                 num_heads=12, # From Table 1\n                 mlp_size=3072, # From Table 1\n                 mlp_dropout=0.1,\n                 attn_dropout=0):\n        super().__init__()\n\n        # Create MSA Block (for equation 2)\n        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n                                                     num_heads=num_heads,\n                                                     attn_dropout=attn_dropout)\n        # Create MLP Block (for equation 3)\n        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n                                   mlp_size=mlp_size,\n                                   dropout=mlp_dropout)\n        \n    def forward(self, x):\n        x = self.msa_block(x) + x # Create skip connection\n        x = self.mlp_block(x) + x # Create skip connection\n        return x\n</pre> class TransformerEncoderBlock(nn.Module):     \"\"\"Creates a Transformer Encoder block.\"\"\"     def __init__(self,                  embedding_dim=768, # From Table 1                  num_heads=12, # From Table 1                  mlp_size=3072, # From Table 1                  mlp_dropout=0.1,                  attn_dropout=0):         super().__init__()          # Create MSA Block (for equation 2)         self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,                                                      num_heads=num_heads,                                                      attn_dropout=attn_dropout)         # Create MLP Block (for equation 3)         self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,                                    mlp_size=mlp_size,                                    dropout=mlp_dropout)              def forward(self, x):         x = self.msa_block(x) + x # Create skip connection         x = self.mlp_block(x) + x # Create skip connection         return x In\u00a0[42]: Copied! <pre>class ViT(nn.Module):\n\"\"\"Creates a Vision Transformer architecture.\"\"\"\n    def __init__(self,\n                 img_size=224, # From Table 3 in ViT paper\n                 in_channels=3,\n                 patch_size=16,\n                 num_transformer_layers=12, # From Table 1 in ViT paper\n                 embedding_dim=768,\n                 mlp_size=3072,\n                 num_heads=12,\n                 attn_dropout=0,\n                 mlp_dropout=0.1,\n                 embedding_dropout=0.1,\n                 num_classes=1000): # default for ImageNet\n        super().__init__() # don't forget the super().__init__()!\n    \n        # Get image size\n        self.img_height, self.img_width = img_size, img_size\n        \n        # Calculate number of patches (height * width/patch^2)\n        self.num_patches = (self.img_height * self.img_width) // patch_size**2\n        \n                 \n        # Create class embedding (needs to go at front of sequence embedding)\n        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n                                            requires_grad=True)\n        \n         # Create position embedding\n        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n                                               requires_grad=True)\n                \n        # Create embedding dropout\n        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n        \n        # Create patch embedding layer\n        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n                                              patch_size=patch_size,\n                                              embedding_dim=embedding_dim)\n        \n        # Create transformer encoder blocks\n        self.transformer_enedoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n                                                                            num_heads=num_heads,\n                                                                            mlp_size=mlp_size,\n                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n       \n        # Create classifier head (equation 4)\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim, \n                      out_features=num_classes)\n        )\n    \n    def forward(self, x):\n        # Get batch size\n        batch_size = x.shape[0]\n        # Create class token embedding\n        class_token = self.class_embedding.expand(batch_size, -1, -1)\n\n        # Create patch embedding\n        x = self.patch_embedding(x)\n\n        # Concat class embedding and patch embedding (equation 1)\n        x = torch.cat((class_token, x), dim=1)\n\n        # Add position embedding to patch embedding (equation 1) for every batch\n        x = self.position_embedding + x\n\n        # Run embedding dropout\n        x = self.embedding_dropout(x)\n\n        # Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)\n        x = self.transformer_enedoder(x)\n\n        # Put 0 index logit through classifier (equation 4)\n        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n\n        return x\n</pre> class ViT(nn.Module):     \"\"\"Creates a Vision Transformer architecture.\"\"\"     def __init__(self,                  img_size=224, # From Table 3 in ViT paper                  in_channels=3,                  patch_size=16,                  num_transformer_layers=12, # From Table 1 in ViT paper                  embedding_dim=768,                  mlp_size=3072,                  num_heads=12,                  attn_dropout=0,                  mlp_dropout=0.1,                  embedding_dropout=0.1,                  num_classes=1000): # default for ImageNet         super().__init__() # don't forget the super().__init__()!              # Get image size         self.img_height, self.img_width = img_size, img_size                  # Calculate number of patches (height * width/patch^2)         self.num_patches = (self.img_height * self.img_width) // patch_size**2                                    # Create class embedding (needs to go at front of sequence embedding)         self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),                                             requires_grad=True)                   # Create position embedding         self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),                                                requires_grad=True)                          # Create embedding dropout         self.embedding_dropout = nn.Dropout(p=embedding_dropout)                  # Create patch embedding layer         self.patch_embedding = PatchEmbedding(in_channels=in_channels,                                               patch_size=patch_size,                                               embedding_dim=embedding_dim)                  # Create transformer encoder blocks         self.transformer_enedoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,                                                                             num_heads=num_heads,                                                                             mlp_size=mlp_size,                                                                             mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])                 # Create classifier head (equation 4)         self.classifier = nn.Sequential(             nn.LayerNorm(normalized_shape=embedding_dim),             nn.Linear(in_features=embedding_dim,                        out_features=num_classes)         )          def forward(self, x):         # Get batch size         batch_size = x.shape[0]         # Create class token embedding         class_token = self.class_embedding.expand(batch_size, -1, -1)          # Create patch embedding         x = self.patch_embedding(x)          # Concat class embedding and patch embedding (equation 1)         x = torch.cat((class_token, x), dim=1)          # Add position embedding to patch embedding (equation 1) for every batch         x = self.position_embedding + x          # Run embedding dropout         x = self.embedding_dropout(x)          # Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)         x = self.transformer_enedoder(x)          # Put 0 index logit through classifier (equation 4)         x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index          return x          In\u00a0[43]: Copied! <pre>batch_size = 32\nclass_tokens = nn.Parameter(data=torch.randn(1, 1, 768))\nclass_tokens.expand(batch_size, -1, -1).shape\n</pre> batch_size = 32 class_tokens = nn.Parameter(data=torch.randn(1, 1, 768)) class_tokens.expand(batch_size, -1, -1).shape Out[43]: <pre>torch.Size([32, 1, 768])</pre> In\u00a0[44]: Copied! <pre>set_seeds()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nrand_image = torch.randn(1, 3, 224, 224)\n# vit = ViT(num_classes=len(class_names)) \nvit = ViT(num_classes=3)\nvit(rand_image)\n</pre> set_seeds() device = \"cuda\" if torch.cuda.is_available() else \"cpu\" rand_image = torch.randn(1, 3, 224, 224) # vit = ViT(num_classes=len(class_names))  vit = ViT(num_classes=3) vit(rand_image) Out[44]: <pre>tensor([[-0.2377,  0.7360,  1.2137]], grad_fn=&lt;AddmmBackward0&gt;)</pre> In\u00a0[45]: Copied! <pre>from torchinfo import summary\n\n# TK - clean up the summary so it looks nice when it prints out \n# Print a summary using torchinfo (uncomment for actual output)\nsummary(model=vit, \n        input_size=(128, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n</pre> from torchinfo import summary  # TK - clean up the summary so it looks nice when it prints out  # Print a summary using torchinfo (uncomment for actual output) summary(model=vit,          input_size=(128, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"         # col_names=[\"input_size\"], # uncomment for smaller output         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"] ) Out[45]: <pre>======================================================================================================================================================\nLayer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable\n======================================================================================================================================================\nViT (ViT)                                                              [128, 3, 224, 224]   [128, 3]             152,064              True\n\u251c\u2500Dropout (embedding_dropout)                                          [128, 197, 768]      [128, 197, 768]      --                   --\n\u251c\u2500PatchEmbedding (patch_embedding)                                     [128, 3, 224, 224]   [128, 196, 768]      --                   True\n\u2502    \u2514\u2500Conv2d (patcher)                                                [128, 3, 224, 224]   [128, 768, 14, 14]   590,592              True\n\u2502    \u2514\u2500Flatten (flatten)                                               [128, 768, 14, 14]   [128, 768, 196]      --                   --\n\u251c\u2500Dropout (embedding_dropout)                                          [128, 197, 768]      [128, 197, 768]      --                   --\n\u251c\u2500Sequential (transformer_enedoder)                                    [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2514\u2500TransformerEncoderBlock (0)                                     [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2502    \u2514\u2500MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\n\u2502    \u2502    \u2514\u2500MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\n\u2502    \u2514\u2500TransformerEncoderBlock (1)                                     [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2502    \u2514\u2500MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\n\u2502    \u2502    \u2514\u2500MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\n\u2502    \u2514\u2500TransformerEncoderBlock (2)                                     [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2502    \u2514\u2500MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\n\u2502    \u2502    \u2514\u2500MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\n\u2502    \u2514\u2500TransformerEncoderBlock (3)                                     [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2502    \u2514\u2500MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\n\u2502    \u2502    \u2514\u2500MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\n\u2502    \u2514\u2500TransformerEncoderBlock (4)                                     [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2502    \u2514\u2500MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\n\u2502    \u2502    \u2514\u2500MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\n\u2502    \u2514\u2500TransformerEncoderBlock (5)                                     [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2502    \u2514\u2500MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\n\u2502    \u2502    \u2514\u2500MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\n\u2502    \u2514\u2500TransformerEncoderBlock (6)                                     [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2502    \u2514\u2500MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\n\u2502    \u2502    \u2514\u2500MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\n\u2502    \u2514\u2500TransformerEncoderBlock (7)                                     [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2502    \u2514\u2500MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\n\u2502    \u2502    \u2514\u2500MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\n\u2502    \u2514\u2500TransformerEncoderBlock (8)                                     [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2502    \u2514\u2500MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\n\u2502    \u2502    \u2514\u2500MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\n\u2502    \u2514\u2500TransformerEncoderBlock (9)                                     [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2502    \u2514\u2500MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\n\u2502    \u2502    \u2514\u2500MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\n\u2502    \u2514\u2500TransformerEncoderBlock (10)                                    [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2502    \u2514\u2500MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\n\u2502    \u2502    \u2514\u2500MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\n\u2502    \u2514\u2500TransformerEncoderBlock (11)                                    [128, 197, 768]      [128, 197, 768]      --                   True\n\u2502    \u2502    \u2514\u2500MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\n\u2502    \u2502    \u2514\u2500MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\n\u251c\u2500Sequential (classifier)                                              [128, 768]           [128, 3]             --                   True\n\u2502    \u2514\u2500LayerNorm (0)                                                   [128, 768]           [128, 768]           1,536                True\n\u2502    \u2514\u2500Linear (1)                                                      [128, 768]           [128, 3]             2,307                True\n======================================================================================================================================================\nTotal params: 85,800,963\nTrainable params: 85,800,963\nNon-trainable params: 0\nTotal mult-adds (G): 22.08\n======================================================================================================================================================\nInput size (MB): 77.07\nForward/backward pass size (MB): 13168.81\nParams size (MB): 257.55\nEstimated Total Size (MB): 13503.43\n======================================================================================================================================================</pre> <ul> <li>TK - same number of parameters as: https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16 -&gt; 86567656</li> </ul> In\u00a0[46]: Copied! <pre>batch_size = 32\ncls_embedding = nn.Parameter(torch.randn(1, 1, 768))\n# See here: https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html\ncls_embedding.shape, cls_embedding.expand(batch_size, -1, -1).shape\n</pre> batch_size = 32 cls_embedding = nn.Parameter(torch.randn(1, 1, 768)) # See here: https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html cls_embedding.shape, cls_embedding.expand(batch_size, -1, -1).shape Out[46]: <pre>(torch.Size([1, 1, 768]), torch.Size([32, 1, 768]))</pre> In\u00a0[47]: Copied! <pre>from going_modular.going_modular import engine\n\noptimizer = torch.optim.Adam(params=vit.parameters(), \n                             lr=1e-3,\n                             betas=(0.9, 0.999), # default\n                             weight_decay=0.1) # from the ViT paper section 4.1\nloss_fn = torch.nn.CrossEntropyLoss()\n\nset_seeds()\nresults = engine.train(model=vit,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=10,\n                       device=device)\n</pre> from going_modular.going_modular import engine  optimizer = torch.optim.Adam(params=vit.parameters(),                               lr=1e-3,                              betas=(0.9, 0.999), # default                              weight_decay=0.1) # from the ViT paper section 4.1 loss_fn = torch.nn.CrossEntropyLoss()  set_seeds() results = engine.train(model=vit,                        train_dataloader=train_dataloader,                        test_dataloader=test_dataloader,                        optimizer=optimizer,                        loss_fn=loss_fn,                        epochs=10,                        device=device) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 4.8759 | train_acc: 0.2891 | test_loss: 1.0465 | test_acc: 0.5417\nEpoch: 2 | train_loss: 1.5900 | train_acc: 0.2617 | test_loss: 1.5876 | test_acc: 0.1979\nEpoch: 3 | train_loss: 1.4644 | train_acc: 0.2617 | test_loss: 1.2738 | test_acc: 0.1979\nEpoch: 4 | train_loss: 1.3159 | train_acc: 0.2773 | test_loss: 1.7498 | test_acc: 0.1979\nEpoch: 5 | train_loss: 1.3114 | train_acc: 0.3008 | test_loss: 1.7444 | test_acc: 0.2604\nEpoch: 6 | train_loss: 1.2445 | train_acc: 0.3008 | test_loss: 1.9704 | test_acc: 0.1979\nEpoch: 7 | train_loss: 1.2050 | train_acc: 0.3984 | test_loss: 3.5480 | test_acc: 0.1979\nEpoch: 8 | train_loss: 1.4368 | train_acc: 0.4258 | test_loss: 1.8324 | test_acc: 0.2604\nEpoch: 9 | train_loss: 1.5757 | train_acc: 0.2344 | test_loss: 1.2848 | test_acc: 0.5417\nEpoch: 10 | train_loss: 1.4658 | train_acc: 0.4023 | test_loss: 1.2389 | test_acc: 0.2604\n</pre> In\u00a0[48]: Copied! <pre>from helper_functions import plot_loss_curves\n\nplot_loss_curves(results)\n</pre> from helper_functions import plot_loss_curves  plot_loss_curves(results) <p>TK - why do the loss curves look the way they do? (too big of a model, not enough data)</p> In\u00a0[49]: Copied! <pre># The following requires torch v0.12+ and torchvision v0.13+\nimport torch\nimport torchvision\nprint(torch.__version__) \nprint(torchvision.__version__)\n</pre> # The following requires torch v0.12+ and torchvision v0.13+ import torch import torchvision print(torch.__version__)  print(torchvision.__version__) <pre>1.12.0+cu102\n0.13.0+cu102\n</pre> In\u00a0[50]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[50]: <pre>'cuda'</pre> In\u00a0[51]: Copied! <pre># Set seeds\ndef set_seeds(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n</pre> # Set seeds def set_seeds(seed=42):     torch.manual_seed(seed)     torch.cuda.manual_seed(seed) In\u00a0[52]: Copied! <pre># Requires torchvision &gt;= 0.13\npretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\npretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n\n# Freeze the base parameters\nfor parameter in pretrained_vit.parameters():\n    parameter.requires_grad = False\n    \n# Change the classifier head\nset_seeds()\npretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n</pre> # Requires torchvision &gt;= 0.13 pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)  # Freeze the base parameters for parameter in pretrained_vit.parameters():     parameter.requires_grad = False      # Change the classifier head set_seeds() pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device) In\u00a0[53]: Copied! <pre># Print a summary using torchinfo (uncomment for actual output)\nsummary(model=pretrained_vit, \n        input_size=(128, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n</pre> # Print a summary using torchinfo (uncomment for actual output) summary(model=pretrained_vit,          input_size=(128, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"         # col_names=[\"input_size\"], # uncomment for smaller output         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"] ) Out[53]: <pre>======================================================================================================================================================\nLayer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable\n======================================================================================================================================================\nVisionTransformer (VisionTransformer)                                  [128, 3, 224, 224]   [128, 3]             768                  Partial\n\u251c\u2500Conv2d (conv_proj)                                                   [128, 3, 224, 224]   [128, 768, 14, 14]   (590,592)            False\n\u251c\u2500Encoder (encoder)                                                    [128, 197, 768]      [128, 197, 768]      151,296              False\n\u2502    \u2514\u2500Dropout (dropout)                                               [128, 197, 768]      [128, 197, 768]      --                   --\n\u2502    \u2514\u2500Sequential (layers)                                             [128, 197, 768]      [128, 197, 768]      --                   False\n\u2502    \u2502    \u2514\u2500EncoderBlock (encoder_layer_0)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\n\u2502    \u2502    \u2514\u2500EncoderBlock (encoder_layer_1)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\n\u2502    \u2502    \u2514\u2500EncoderBlock (encoder_layer_2)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\n\u2502    \u2502    \u2514\u2500EncoderBlock (encoder_layer_3)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\n\u2502    \u2502    \u2514\u2500EncoderBlock (encoder_layer_4)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\n\u2502    \u2502    \u2514\u2500EncoderBlock (encoder_layer_5)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\n\u2502    \u2502    \u2514\u2500EncoderBlock (encoder_layer_6)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\n\u2502    \u2502    \u2514\u2500EncoderBlock (encoder_layer_7)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\n\u2502    \u2502    \u2514\u2500EncoderBlock (encoder_layer_8)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\n\u2502    \u2502    \u2514\u2500EncoderBlock (encoder_layer_9)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\n\u2502    \u2502    \u2514\u2500EncoderBlock (encoder_layer_10)                            [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\n\u2502    \u2502    \u2514\u2500EncoderBlock (encoder_layer_11)                            [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\n\u2502    \u2514\u2500LayerNorm (ln)                                                  [128, 197, 768]      [128, 197, 768]      (1,536)              False\n\u251c\u2500Linear (heads)                                                       [128, 768]           [128, 3]             2,307                True\n======================================================================================================================================================\nTotal params: 85,800,963\nTrainable params: 2,307\nNon-trainable params: 85,798,656\nTotal mult-adds (G): 22.08\n======================================================================================================================================================\nInput size (MB): 77.07\nForward/backward pass size (MB): 13322.95\nParams size (MB): 257.55\nEstimated Total Size (MB): 13657.57\n======================================================================================================================================================</pre> In\u00a0[54]: Copied! <pre># TK - the above output has the same number of parameters as our own created model\n</pre> # TK - the above output has the same number of parameters as our own created model In\u00a0[55]: Copied! <pre># Download pizza, steak, sushi images from GitHub\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> # Download pizza, steak, sushi images from GitHub image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n</pre> Out[55]: <pre>PosixPath('data/pizza_steak_sushi')</pre> In\u00a0[56]: Copied! <pre>train_dir = image_path / \"train\"\ntest_dir = image_path / \"test\" \ntrain_dir, test_dir\n</pre> train_dir = image_path / \"train\" test_dir = image_path / \"test\"  train_dir, test_dir Out[56]: <pre>(PosixPath('data/pizza_steak_sushi/train'),\n PosixPath('data/pizza_steak_sushi/test'))</pre> In\u00a0[57]: Copied! <pre># Create dataset for pretrained ViT\npretrained_vit_transforms = pretrained_vit_weights.transforms()\nprint(pretrained_vit_transforms)\n\ntrain_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                                     test_dir=test_dir,\n                                                                                                     transform=pretrained_vit_transforms,\n                                                                                                     batch_size=1024) # From here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n</pre> # Create dataset for pretrained ViT pretrained_vit_transforms = pretrained_vit_weights.transforms() print(pretrained_vit_transforms)  train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                                      test_dir=test_dir,                                                                                                      transform=pretrained_vit_transforms,                                                                                                      batch_size=1024) # From here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)  <pre>ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n</pre> In\u00a0[58]: Copied! <pre># Train pretrained feature extractor ViT for 5 epochs on Pizza, Steak, Sushi\n# TK - can probably increase the batch_size here because we're using feature extraction and not \n# training the whole model\nfrom going_modular.going_modular import engine\n\noptimizer = torch.optim.Adam(params=pretrained_vit.parameters(), \n                             lr=1e-3)\nloss_fn = torch.nn.CrossEntropyLoss()\n\nset_seeds()\npretrained_vit_results = engine.train(model=pretrained_vit,\n                                      train_dataloader=train_dataloader_pretrained,\n                                      test_dataloader=test_dataloader_pretrained,\n                                      optimizer=optimizer,\n                                      loss_fn=loss_fn,\n                                      epochs=10,\n                                      device=device)\n</pre> # Train pretrained feature extractor ViT for 5 epochs on Pizza, Steak, Sushi # TK - can probably increase the batch_size here because we're using feature extraction and not  # training the whole model from going_modular.going_modular import engine  optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),                               lr=1e-3) loss_fn = torch.nn.CrossEntropyLoss()  set_seeds() pretrained_vit_results = engine.train(model=pretrained_vit,                                       train_dataloader=train_dataloader_pretrained,                                       test_dataloader=test_dataloader_pretrained,                                       optimizer=optimizer,                                       loss_fn=loss_fn,                                       epochs=10,                                       device=device) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.1490 | train_acc: 0.2356 | test_loss: 1.0584 | test_acc: 0.4667\nEpoch: 2 | train_loss: 1.0017 | train_acc: 0.5289 | test_loss: 0.9194 | test_acc: 0.6400\nEpoch: 3 | train_loss: 0.8716 | train_acc: 0.7244 | test_loss: 0.7983 | test_acc: 0.6667\nEpoch: 4 | train_loss: 0.7583 | train_acc: 0.8089 | test_loss: 0.6942 | test_acc: 0.7733\nEpoch: 5 | train_loss: 0.6608 | train_acc: 0.8622 | test_loss: 0.6060 | test_acc: 0.8800\nEpoch: 6 | train_loss: 0.5777 | train_acc: 0.8889 | test_loss: 0.5318 | test_acc: 0.8933\nEpoch: 7 | train_loss: 0.5076 | train_acc: 0.9156 | test_loss: 0.4700 | test_acc: 0.9067\nEpoch: 8 | train_loss: 0.4487 | train_acc: 0.9244 | test_loss: 0.4188 | test_acc: 0.9333\nEpoch: 9 | train_loss: 0.3993 | train_acc: 0.9378 | test_loss: 0.3765 | test_acc: 0.9333\nEpoch: 10 | train_loss: 0.3580 | train_acc: 0.9378 | test_loss: 0.3417 | test_acc: 0.9467\n</pre> In\u00a0[59]: Copied! <pre># Plot the loss curves\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(pretrained_vit_results)\n</pre> # Plot the loss curves from helper_functions import plot_loss_curves  plot_loss_curves(pretrained_vit_results)  In\u00a0[60]: Copied! <pre># Save the model\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=pretrained_vit,\n                 target_dir=\"models\",\n                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")\n</pre> # Save the model from going_modular.going_modular import utils  utils.save_model(model=pretrained_vit,                  target_dir=\"models\",                  model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\") <pre>[INFO] Saving model to: models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\n</pre> In\u00a0[61]: Copied! <pre>from pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024)\nprint(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")\n</pre> from pathlib import Path  # Get the model size in bytes then convert to megabytes pretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\") <pre>Pretrained ViT feature extractor model size: 327 MB\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#wip-08-pytorch-paper-replicating","title":"(WIP) 08. PyTorch Paper Replicating\u00b6","text":"<p>TK intro</p> <p>Want to recreate ViT paper: \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" - https://arxiv.org/abs/2010.11929 - TK will refer to this as \"ViT paper\" throughout.</p> <ul> <li><p>TK what is ViT?</p> </li> <li><p>TK - The name Transformer comes from the architecture name in the paper where it was originally introduced, Attention is all you need. An architecture is usually considered a Transformer variant if it uses attention layers in a specific pattern. Since the Transformer architecture originally focused on text data, the goal of the ViT paper was to bring it to the vision.</p> </li> <li><p>TK - The original transformer was made to work on sequences of text (1D), Vision Transformer turns images into sequences of \"patches\".</p> </li> <li><p>TK - original ViT also called \"vanilla vision transformer\"</p> </li> </ul>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-what-is-paper-replicating","title":"TK - What is paper replicating?\u00b6","text":"<p>It's no secret machine learning is advancing fast.</p> <p>Many of these advances get published in machine learning research papers.</p> <p>And the goal of paper replicating is to take replicate these advances with code so you can use the techniques for your own problem.</p> <p>For example, let's say a new model architecture gets released that performs better than any other architecture before on various benchmarks, wouldn't it be nice to try that architecture on your own problems?</p> <ul> <li>TK image: paper replicating = research paper (language + diagrams + math) -&gt; code (turn language, diagrams and math into usable code) / (translate a research paper into usable code)</li> </ul>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-what-is-a-machine-learning-research-paper","title":"TK - What is a machine learning research paper?\u00b6","text":"<p>A machine learning research paper is a scientific paper that details findings of a research group on a specific area.</p> <p>The contents of a machine learning research paper can vary from paper to paper but they generally follow the structure:</p> Section Contents Abstract An overview/summary of the paper's main findings/contributions. Introduction What's the paper's main problem and what are previous methods used to try and solve it? Method How did the researchers go about conducting their research? For example, what model(s) were used, data sources, training setups, etc. Results What are the outcomes of the paper? If a new type of model or training setup was used, how did the results of findings compare to previous works (this is where experiment tracking comes in handy)? Conclusion What are the limitations of the suggested methods? What are some next steps for the research community? References What resources/other papers did the researchers look at to build their own body of work? Appendix Are there any extra resources/findings to look at that weren't included in any of the above sections?"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-why-replicate-a-machine-learning-research-paper","title":"TK - Why replicate a machine learning research paper?\u00b6","text":"<p>A machine learning research paper is often a presentation of months of work and experiments done by some of the best machine learning teams in the world condensed into a few pages of text.</p> <p>And if these experiments lead to better results in an area related to the problem you're working on, it'd be nice to them out.</p> <p>Also, replicating the work of others is a fantastic way to practice your skills.</p> <p></p> <p>George Hotz is founder of comma.ai, a self-driving car company and livestreams machine learning coding on Twitch and those videos get posted in full to YouTube. I pulled this quote from one of his livestreams. The \"\u066d\" is to note that machine learning engineering often involves the extra step(s) of preprocessing data and making your models available for others to use (deployment).</p> <p>When you first start trying to replicate research papers, you'll likely be overwhelmed.</p> <p>That's normal.</p> <p>Research teams spend weeks, months and sometimes years creating these works so it makes sense if it takes you sometime to even read let alone reproduce the works.</p> <p>Replicating research is such a tough problem, phenomenal machine learning libraries and tools such as, HuggingFace, PyTorch Image Models (<code>timm</code> library) and fast.ai have been born out of making machine learning research more accessible.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-where-can-you-find-code-examples-for-machine-learning-research-papers","title":"TK - Where can you find code examples for machine learning research papers?\u00b6","text":"<p>One of the first things you'll notice when it comes to machine learning research is: there's a lot of it.</p> <p>So beware, trying to stay on top of it is like trying to outrun a hamster wheel.</p> <p>Follow your interest, pick a few things that stand out to you.</p> <p>In saying this, there are several places to find and read machine learning research papers:</p> <ul> <li>arXiv - Pronounced \"archive\", arXiv is a free and open resource for reading technical articles on everything from physics to computer science (inlcuding machine learning).</li> <li>Papers with Code - A curated collection of trending, active and greatest machine learning papers, many of which include code resources attached. Also includes a collection of common machine learning datasets, benchmarks and current state-of-the-art models.</li> <li>AK Twitter - The AK Twitter account publishes machine learning research highlights, often with live demos almost every day. I don't understand 9/10 posts but I find it fun to explore every so often.</li> <li>lucidrains' <code>vit-pytorch</code> GitHub repository - Less of a place to find research papers and more of an example of what paper replicating with code on a larger-scale looks like. The <code>vit-pytorch</code> repository is a collection of Vision Transformer model architectures from various research papers replicated with PyTorch code (much of the inspiration for this notebook was gathered from this repository).</li> </ul> <p>TK image: showcase the above</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-what-were-going-to-cover","title":"TK - What we're going to cover\u00b6","text":"<p>TODO</p> <ul> <li>ViT -&gt; FoodVision Mini</li> <li>Layers = collections of functions to manipulate data -&gt; Architectures = collections of layers (blocks) -&gt; All layers (and blocks) have inputs and outputs<ul> <li>Replicating research papers starts by figuring out the inputs and outputs of your layers -&gt; blocks -&gt; model</li> </ul> </li> </ul>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-where-can-you-get-help","title":"TK - Where can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-0-getting-setup","title":"TK 0. Getting setup\u00b6","text":"<p>As we've done previously, let's make sure we've got all of the modules we'll need for this section.</p> <p>We'll import the Python scripts (such as <code>data_setup.py</code> and <code>engine.py</code>) we created in 05. PyTorch Going Modular.</p> <p>To do so, we'll download <code>going_modular</code> directory from the <code>pytorch-deep-learning</code> repository (if we don't already have it).</p> <p>We'll also get the <code>torchinfo</code> package if it's not available.</p> <p><code>torchinfo</code> will help later on to give us a visual representation of our model.</p> <p>And since later on we'll be using a newer version of the <code>torchvision</code> package (as of June 2022), we'll make sure we've got the latest versions.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-1-get-data","title":"TK 1. Get Data\u00b6","text":"<p>Since we're continuing on with FoodVision Mini, let's download the pizza, steak and sushi image dataset we've been using.</p> <p>To do so we can use the <code>download_data()</code> function from <code>helper_functions.py</code> that we created in 07. PyTorch Experiment Tracking section 1.</p> <p>We'll <code>source</code> to the raw GitHub link of the <code>pizza_steak_sushi.zip</code> data and the <code>destination</code> to <code>pizza_steak_sushi</code>.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-2-create-datasets-and-dataloaders","title":"TK 2. Create Datasets and DataLoaders\u00b6","text":"<p>Since we've got some data, let's now turn it into <code>DataLoader</code>'s.</p> <p>To do so we can use the <code>create_dataloaders()</code> function in <code>data_setup.py</code>.</p> <p>First, we'll create a transform to prepare our images.</p> <p>This where one of the first references to the ViT paper will come in.</p> <p>In Table 3, the training resolution is mentioned as being 224 (height=224, width=224).</p> <p></p> <p>You can often find various hyperparameter settings listed in a table. In this case we're still preparing our data, so we're mainly concerned with things like image size and batch size. Source: Table 3 in ViT paper.</p> <p>So we'll make sure our transform resizes our images appropriately.</p> <p>And since we'll be training our model from scratch (no transfer learning to begin with), we won't provide a <code>normalize</code> transform like we did in 06. PyTorch Transfer Learning section 2.1.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#21-prepare-transforms-for-images","title":"2.1 Prepare transforms for images\u00b6","text":""},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#22-turn-images-into-dataloaders","title":"2.2 Turn images into <code>DataLoader</code>'s\u00b6","text":"<p>Transforms created!</p> <p>Let's now create our <code>DataLoader</code>'s.</p> <p>The ViT paper states the use of a batch size of 4096 which is 128x the size of the batch size we've been using (32).</p> <p>We're going to stick with a batch size of 32.</p> <p>Why?</p> <p>Because some hardware (including the free tier of Google Colab) may not be able to handle a batch size of 4096.</p> <p>Having a batch size of 4096 means that 4096 images need to fit into the GPU memory at a time.</p> <p>This works when you've got the hardware to handle it like a research team from Google often does but when you're running on a single GPU (such as using Google Colab), making sure things work with smaller batch size first is a good idea.</p> <p>An extension of this project could be to try a higher batch size value and see what happens.</p> <p>Note: We're using the <code>pin_memory=True</code> parameter in the <code>create_dataloaders()</code> function to speed up computation. <code>pin_memory=True</code> avoids unnecessary copying of memory between the CPU and GPU memory by \"pinning\" examples that have been seen before. For more on this concept. Though the benefits of this will likely be seen with larger dataset sizes (our FoodVision Mini dataset is quite small). See the PyTorch <code>torch.utils.data.DataLoader</code> documentation or Making Deep Learning Go Brrrr from First Principles by Horace He for more.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-23-visualize-a-single-image","title":"TK 2.3 Visualize a single image\u00b6","text":"<p>Now we've loaded our data, let's visualize, visualize, visualize!</p> <p>An important step in the ViT paper is preparing the images into patches.</p> <p>We'll get to what this means in a second but for now, let's view a single image and its label.</p> <p>To do so, let's get a single image and label from a batch of data and inspect their shapes.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-3-replicating-the-vit-paper-an-overview","title":"TK 3. Replicating the ViT paper: an overview\u00b6","text":"<p>Before we write anymore code, let's discuss what we're doing.</p> <p>We'd like to replicate the ViT paper for our own problem, FoodVision Mini.</p> <p>So our inputs are: images of pizza, steak and sushi.</p> <p>And our ideal model outputs are: predicted labels of pizza, steak or sushi.</p> <p>No different to what we've been doing throughout the previous sections.</p> <p>The question is: how do we go from our inputs to the desired outputs?</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#31-inputs-and-outputs-layers-and-blocks","title":"3.1 Inputs and outputs, layers and blocks\u00b6","text":"<p>ViT is a deep learning neural network architecture.</p> <p>And any neural network architecture is generally comprised of layers.</p> <p>And a collection of layers is often referred to as a block.</p> <p>And stacking many blocks together is what gives us the whole architecture.</p> <p>A layer takes an input (say an image tensor), performs some kind of function on it (for example what's in the layer's <code>forward()</code> method) and then returns an output.</p> <p>So if a single layer takes an input and gives an output, then a collection of layers or a block also takes an input and gives an output.</p> <p>Let's make this concrete:</p> <ul> <li>Layer - takes an input, performs a function on it, returns an output.</li> <li>Block - a collection of layers, takes an input, performs a series of functions on it, returns an output.</li> <li>Architecture (or model) - a collection of blocks, takes an input, performs a series of functions on it, returns an output.</li> </ul> <p>This ideology is what we're going to be using to replicate the ViT paper.</p> <p>We're going to take it layer by layer, block by block, function by function putting the pieces of the puzzle together like Lego to get our desired overall architecture.</p> <p>The reason we do this is because looking at a whole research paper can be intimidating.</p> <p>So for a better understanding, we'll break it down, starting with the inputs and outputs of single layer and working up to the inputs and outputs of the whole model.</p> <p>TK image: stacking the network together like lego (functions + layers + blocks = model).</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#32-getting-specific-whats-vit-made-of","title":"3.2 Getting specific: What's ViT made of?\u00b6","text":"<p>There are many little details about the ViT model sprinkled throughout the paper.</p> <p>Finding them all is like one big treasure hunt!</p> <p>Remember, a research paper is often months of work compressed into a few pages so it's understandable for it to take of practice to replicate.</p> <p>However, the main three resources we'll be looking at for the architecture design are:</p> <ol> <li>Figure 1 - This gives an overview of the model in a graphical sense, you could almost recreate the architecture with this figure alone.</li> <li>Four equations in section 3.1 - These equations give a little bit more of a mathematical grounding to the coloured blocks in Figure 1.</li> <li>Table 1 - This table shows the various hyperparameter settings (such as number of layers and number of hidden units) for different ViT model variants. We'll be focused on the smallest version, ViT-Base.</li> </ol>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-321-exploring-figure-1","title":"TK 3.2.1 Exploring Figure 1\u00b6","text":"<p>Let's start by going through Figure 1 of the ViT Paper.</p> <p>The main things we'll be paying attention to are:</p> <ol> <li>Layers - takes an input, performs an operation or function, produces an output.</li> <li>Blocks - a collection of layers, which in turn also takes an input and produces an output.</li> </ol> <p></p> <p>Figure 1 from the ViT Paper showcasing the different inputs, outputs, layers and blocks that create the architecture. Our goal will be to replicate each of these using PyTorch code.</p> <p>The ViT architecture is comprised of several stages:</p> <ul> <li>Patch + Position Embedding (inputs) - Turns the input image into a sequence of image patches and add a position number what order the patch comes in.</li> <li>Linear projection of flattened patches (Embedded Patches) - The image patches get turned into an embedding, the benefit of using an embedding rather than just the image values is that an embedding is a learnable representation (typically in the form of a vector) of the image that can improve with training.</li> <li>Norm - This is short for \"Layer Normalization\" or \"LayerNorm\", a technique for regularizing (reducing overfitting) a neural network, you can use LayerNorm via the PyTorch layer <code>torch.nn.LayerNorm()</code>.</li> <li>Multi-Head Attention - This is a Multi-Headed Self-Attention layer or \"MSA\" for short. You can create an MSA layer via the PyTorch layer <code>torch.nn.MultiheadAttention()</code>.</li> <li>MLP (or Multilayer perceptron) - A MLP can often refer to any collection of feedforward layers (or in PyTorch's case, a collection of layers with a <code>forward()</code> method). In the ViT Paper, the authors refer to the MLP as \"MLP block\" and it contains two <code>torch.nn.Linear()</code> layers with a <code>torch.nn.GELU()</code> non-linearity activation in between them (section 3.1) and a <code>torch.nn.Dropout()</code> layer after each (Appendex B.1).</li> <li>Transformer Encoder - The Transformer Encoder, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the \"+\" symbols) meaning the layer's inputs are fed directly to immediate layers as well as subsequent layers. The overall ViT architecture is comprised of a number of Transformer encoders stacked on top of eachother.</li> <li>MLP Head - This is the output layer of the architecture, it converts the learned features of an input to a class output. Since we're working on image classification, you could also call this the \"classifier head\". The structure of the MLP Head is similar to the MLP block.</li> </ul> <p>You might notice that many of the pieces of the ViT architecture can be created with existing PyTorch layers.</p> <p>This is because of how PyTorch is designed, it's one of the main purposes of PyTorch to create reusable neural network layers for both researchers and machine learning practitioners.</p> <p>Question: Why not code everything from scratch?</p> <p>You could definitely do that by reproducing all of the math equations from the paper with custom PyTorch layers and that would certainly be an educative exercise, however, using pre-existing PyTorch layers is usually favoured as pre-existing layers have often been extensively tested and performance checked to make sure they run correctly and fast.</p> <p>Note: We're going to focused on write PyTorch code to create these layers, for the background on what each of these layers does, I'd suggest reading the ViT Paper in full or reading the linked resources for each layer.</p> <p>Let's take Figure 1 and adapt it to our FoodVision Mini problem of classifying images of food into pizza, steak or sushi.</p> <p></p> <p>Figure 1 from the ViT Paper adapted for use with FoodVision Mini. An image of food goes in (pizza), the image gets turned into patches and then projected to an embedding. The embedding then travels through the various layers and blocks and (hopefully) the class \"pizza\" is returned.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-322-exploring-the-four-equations","title":"TK - 3.2.2 Exploring the Four Equations\u00b6","text":"<p>The next main part(s) of the ViT paper we're going to look at are the four equations in section 3.1.</p> <p></p> <p>These four equations represent the math behind the four major parts of the ViT architecture.</p> <p>Section 3.1 describes each of these (some of the text has been omitted for brevity, bolded text is mine):</p> Equation number Description from ViT paper section 3.1 1 ...The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings. 2 The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski &amp; Auli, 2019). 3 See above. 4 Similar to BERT's [ class ] token, we prepend a learnable embedding to the sequence of embedded patches $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)$, whose state at the output of the Transformer encoder $\\left(\\mathbf{z}_{L}^{0}\\right)$ serves as the image representation $\\mathbf{y}$ (Eq. 4)... <p>Let's map these descriptions to the ViT architecture in Figure 1.</p> <p></p> <p>Connecting Figure 1 from the ViT paper to the four equations from section 3.1 describing the math behind each of the layers/blocks. Some details such as \"residual connections after every block\" are referred to in Figure 1 and in the text but not in the equations.</p> <p>There's a lot happening in the image above but following the coloured lines and arrows reveals the main concepts of the ViT architecture.</p> <p>How about we break down each equation further (it will be our goal to recreate these with code)?</p> <p>In all equations (except equation 4), \"$\\mathbf{z}$\" is the raw output of a particular layer:</p> <ol> <li>$\\mathbf{z}_{0}$ is \"z zero\" (this is the output of the initial patch embedding layer)</li> <li>$\\mathbf{z}_{\\ell}^{\\prime}$ is \"z of a particular layer prime\" (or an intermediary value of z)</li> <li>$\\mathbf{z}_{\\ell}$ is \"z of a particular layer\"</li> </ol> <p>And $\\mathbf{y}$ is the overall output of the architecture.</p> <p>Equation 1</p> <p>$$ \\begin{aligned} \\mathbf{z}_{0} &amp;=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, &amp; &amp; \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\end{aligned} $$</p> <p>This equation deals with the class token, patch embedding and position embedding ($\\mathbf{E}$ is for embedding) of the input image.</p> <p>In vector form, the embedding might look something like:</p> <p>TK - update the vector form to reflect a real exmaple</p> <pre>x_input = [class_token, image_patch_1, image_patch_2, image_patch_3...] + [class_token_position, image_patch_1_position, image_patch_2_position, image_patch_3_position...]\n</pre> <p>Where each of the elements in the vector is learnable (their <code>requires_grad=True</code>).</p> <p>Equation 2</p> <p>$$ \\begin{aligned} \\mathbf{z}_{\\ell}^{\\prime} &amp;=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, &amp; &amp; \\ell=1 \\ldots L \\end{aligned} $$</p> <p>This says that for every layer from $1$ through to $L$ (the total number of layers), there's a Multi-Head Attention layer (MSA) wrapping a LayerNorm layer (LN).</p> <p>The addition on the end is the equivalent of adding the input to the output and forming a skip/residual connection.</p> <p>We'll call this layer the \"MSA block\".</p> <p>In pseudocode, this might look like:</p> <pre>x_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n</pre> <p>Notice the skip connection on the end (adding the input of the layers to the output of the layers).</p> <p>Equation 3</p> <p>$$ \\begin{aligned} \\mathbf{z}_{\\ell} &amp;=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, &amp; &amp; \\ell=1 \\ldots L \\\\ \\end{aligned} $$</p> <p>This says that for every layer from $1$ through to $L$ (the total number of layers), there's also a Multilayer Perceptron layer (MLP) wrapping a LayerNorm layer (LN).</p> <p>The addition on the end is showing the presence of a skip/residual connection.</p> <p>We'll call this layer the \"MLP block\".</p> <p>In pseudocode, this might look like:</p> <pre>x_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n</pre> <p>Notice the skip connection on the end (adding the input of the layers to the output of the layers).</p> <p>Equation 4</p> <p>$$ \\begin{aligned} \\mathbf{y} &amp;=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) &amp; &amp; \\end{aligned} $$</p> <p>This says for the last layer $L$, the output $y$ is the 0 index token of $z$ wrapped in a LayerNorm layer (LN).</p> <p>Or in our case, the 0 index of <code>x_output_MLP_block</code>:</p> <pre>y = LN_layer(Linear_layer(x_output_MLP_block[0]))\n</pre> <p>Of course there are some simplifications above but we'll take care of those when we start to write PyTorch code for each section.</p> <p>Note: The above section covers alot of information. But don't forget if something doesn't make sense, you can always research it further. By asking questions like \"what is a residual connection?\".</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-323-exploring-table-1","title":"TK - 3.2.3 Exploring Table 1\u00b6","text":"<p>The final piece of the ViT architecture puzzle we'll focus on (for now) is Table 1.</p> Model Layers Hidden size $D$ MLP size Heads Params ViT-Base 12 768 3072 12 $86M$ ViT-Large 24 1024 4096 16 $307M$ ViT-Huge 32 1280 5120 16 $632M$ Table 1: Details of Vision Transformer model variants. Source: ViT paper. <p>This table showcasing the various hyperparameters of each of the ViT architectures.</p> <p>You can see the numbers gradually increase from ViT-Base to ViT-Huge.</p> <p>We're going to focus on replicating ViT-Base (start small and scale up when necessary) but we'll be writing code that could easily scale up to the larger variants.</p> <p>Breaking the hyperparameters down:</p> <ul> <li>Layers - How many Transformer encoder blocks are there? (each of these will contain a MSA block and MLP block)</li> <li>Hidden size $D$ - This is the embedding dimension throughout the architecture, this will be the size of the vector that our image gets turned into when it gets patched and embedded. Generally, the larger the embedding dimension, the more information can be captured, the better results. However, a larger embedding comes at the cost of more compute.</li> <li>MLP size - What are the number of hidden units in the MLP layers?</li> <li>Heads - How many heads are there in the Multi-Head Attention layers?</li> <li>Params - What are the total number of parameters of the model? Generally, more parameters leads to better performance but at the cost of more compute. You'll notice even ViT-Base has far more parameters than any other model we've used so far.</li> </ul> <p>We'll use these values as the hyperparameter settings for our ViT architecture.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-33-my-workflow-for-replicating-papers","title":"TK - 3.3 My workflow for replicating papers\u00b6","text":"<p>When I start working on replicating a paper, I go through the following steps:</p> <ol> <li>Read the whole paper end-to-end once (to get an idea of the main concepts).</li> <li>Go back through each section and see how they line up with each other and start thinking about how they might be turned into code (just like above).</li> <li>Repeat step 2 until I've got a fairly good outline.</li> <li>Use mathpix.com (a very handy tool) to turn any sections of the paper into markdown/LaTeX to put into notebooks.</li> <li>Replicate the simplest version of the model possible.</li> <li>If I get stuck, look up other examples.</li> </ol> <p>TK - gif of mathpix</p> <p>We've already gone through the first few steps above (and if you haven't read the full paper yet, I'd encourage you to give it a go) but what we'll be focusing on next is step 5: replicating the simplest version fo the model possible.</p> <p>This is why we're starting with ViT-Base.</p> <p>Replicating the smallest version of the architecture possible, get it working and then we can scale up if we wanted to.</p> <p>Note: If you've never read a research paper before, many of the above steps can be intimidating. But don't worry, like anything, your skills at reading and replicating papers will improve with practice. Don't forget, a research paper is often months of work by many people compressed into a few pages. So trying to replicate it on your own is no small feat.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-4-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding","title":"TK 4. Equation 1: Split data into patches and creating the class, position and patch embedding\u00b6","text":"<p>I remember one of my machine learning engineer friends used to say \"it's all about the embedding.\"</p> <p>As in, if you can represent your data in a good, learnable way (as embeddings are learnable representations), chances are a learning algorithm will be able to perform well on them.</p> <p>So with that being said, let's start by creating the class, position and patch embeddings for the ViT architecture.</p> <p>We'll start with the patch embedding.</p> <p>This means we'll be turning our input images in a sequence of patches and then embedding those patches.</p> <p>Recall that an embedding is a learnable representation of some form and is often a vector. The term learnable is important because this means the representation of an input image can be improved and learned over time.</p> <p>We'll begin by following the opening paragraph of section 3.1 of the ViT paper (bold mine):</p> <p>The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_{p} \\in \\mathbb{R}^{N \\times\\left(P^{2} \\cdot C\\right)}$, where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.</p> <p>And size we're dealing with image shapes, let's keep in mind the line from Table 3 of the ViT paper:</p> <p>Training resolution is 224.</p> <p>Let's break down the text above.</p> <ul> <li>$D$ is the size of the patch embeddings, different values for $D$ can be found in Table 1.</li> <li>The image starts as 2D with size ${H \\times W \\times C}$.</li> <li>The image gets converted to a sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.<ul> <li>$(H, W)$ is the resolution of the original image.</li> <li>$C$ is the number of channels.</li> <li>$(P, P)$ is the resolution of each image patch (patch size).</li> <li>$N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.</li> </ul> </li> </ul> <p></p> <p>Mapping the patch and position embedding portion of the ViT architecture from Figure 1 to Equation 1. The opening paragraph of section 3.1 describes the different input and output shapes of the patch embedding layer.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-41-calculating-patch-embedding-input-and-output-shapes-by-hand","title":"TK - 4.1 Calculating patch embedding input and output shapes by hand\u00b6","text":"<p>How about we start by calculating these input and output shape values by hand?</p> <p>To do so, let's create some variables to mimic each of the terms (such as $H$, $W$ etc) above.</p> <p>We'll use a patch size ($P$) of 16 since it's the best performing version of ViT-Base uses (see column \"ViT-B/16\" of Table 5 in the ViT paper for more).</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-42-turning-a-single-image-into-patches","title":"TK - 4.2 Turning a single image into patches\u00b6","text":"<p>Now we know the ideal input and output shapes for our patch embedding layer.</p> <p>What we're doing here is breaking the overall architecture down into smaller pieces, focusing on the inputs and outputs of individual layers.</p> <p>So how do we create the patch embedding layer?</p> <p>We'll get to that shortly, first, let's visualize, visualize, visualize! what it looks like to turn an image into patches.</p> <p>Let's start with our single image.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-43-creating-image-patches-with-torchnnconv2d","title":"TK - 4.3 Creating image patches with <code>torch.nn.Conv2d()</code>\u00b6","text":"<p>It's time to start moving towards replicating the patch embedding layers with PyTorch.</p> <p>To visualize our single image we wrote code to loop through the different height and width dimensions of a single image and plot individual patches.</p> <p>This operation is very similar to the convolutional operation we saw in 03. PyTorch Computer Vision section 7.1: Stepping through <code>nn.Conv2d()</code>.</p> <p>In fact, the authors of the ViT paper mention in section 3.1 that the patch embedding is achievable with a convolutional neural network (CNN):</p> <p>Hybrid Architecture. As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection $\\mathbf{E}$ (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size $1 \\times 1$, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. The classification input embedding and position embeddings are added as described above.</p> <p>The \"feature map\" they're refering to are the weights/activations produced by a convolutional layer passing over a given image.</p> <p></p> <p>By setting the <code>kernel_size</code> and <code>stride</code> parameters of a <code>torch.nn.Conv2d()</code> layer equal to the <code>patch_size</code>, we can effectively get a layer that splits our image into patches and creates a learnable embedding (referred to as a \"Linear Projection\" in the ViT paper) of each patch.</p> <p>Remember our ideal input and output shapes for the patch embedding layer?</p> <ul> <li>Input: The image starts as 2D with size ${H \\times W \\times C}$.</li> <li>Output: The image gets converted to a sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.</li> </ul> <p>Or for an image size of 224 and patch size of 16:</p> <ul> <li>Input (2D image): (224, 224, 3)</li> <li>Output (flattened 2D patches): (196, 768)</li> </ul> <p>We can recreate these with:</p> <ul> <li><code>torch.nn.Conv2d()</code> for turning our image into patches of CNN feature maps.</li> <li><code>torch.nn.Flatten()</code> for flattening the spatial dimensions of the feature map.</li> </ul> <p>Let's start with the <code>torch.nn.Conv2d()</code> layer.</p> <p>We can replicate the creation of patches by setting the <code>kernel_size</code> and <code>stride</code> equal to <code>patch_size</code>.</p> <p>This means each convolutional kernel will be of size <code>(patch_size x patch_size)</code> or if <code>patch_size=16</code>, <code>(16 x 16)</code> (the equivalent of one whole patch)</p> <p>And each step or <code>stride</code> of the convolutional kernel will be <code>patch_size</code> pixels long or <code>16</code> pixels long (equivalent of stepping to the next patch).</p> <p>We'll set <code>in_channels=3</code> for the number of color channels in our image and we'll set <code>out_channels=768</code>, the same as the $D$ value in Table 1 for ViT-Base (this is the embedding dimension, each image will be embedded into a vector of size 768).</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-44-flattening-the-patch-embedding-with-torchnnflatten","title":"TK - 4.4 Flattening the patch embedding with <code>torch.nn.Flatten()</code>\u00b6","text":"<p>We've turned our image into patch embeddings but they're still in 2D format.</p> <p>How do we get them into the desired output shape of the patch embedding layer of the ViT model?</p> <ul> <li>Desried output (flattened 2D patches): (196, 768) -&gt; ${N \\times\\left(P^{2} \\cdot C\\right)}$</li> </ul> <p>Let's check the current shape.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-45-turning-the-vit-patch-embedding-layer-into-a-pytorch-module","title":"TK - 4.5 Turning the ViT patch embedding layer into a PyTorch module\u00b6","text":"<p>Time to put everything we've done for creating the patch embedding into a single PyTorch layer.</p> <p>We can do so by subclassing <code>nn.Module</code> and creating a small PyTorch \"model\" to do all of the steps above.</p> <p>Specifically we'll:</p> <ol> <li>Create a class called <code>PatchEmbedding</code> which subclasses <code>nn.Module</code> (so it can be used a PyTorch layer).</li> <li>Initialize the class with the parameters <code>in_channels=3</code>, <code>patch_size=16</code> (for ViT-Base) and <code>embedding_dim=768</code> (this is $D$ for ViT-Base from Table 1).</li> <li>Create a layer to turn an image into patches using <code>nn.Conv2d()</code> (just like in 4.3 above).</li> <li>Create a layer to flatten the patch feature maps into a single dimension (just like in 4.4 above).</li> <li>Define a <code>forward()</code> method to take an input and pass it through the layers created in 3 and 4.</li> <li>Make sure the output shape reflects the required output shape of the ViT architecture (${N \\times\\left(P^{2} \\cdot C\\right)}$).</li> </ol> <p>Let's do it!</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-46-creating-the-class-token-embedding","title":"TK 4.6 Creating the class token embedding\u00b6","text":"<p>Okay we've made the image patch embedding, time to get to work on the class token embedding.</p> <p>Or $\\mathbf{x}_\\text {class }$ from equation 1.</p> <p></p> <p>Left: Figure 1 from the ViT paper with the \"classification token\" or <code>[class]</code> embedding token we're going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the learnable class embedding token.</p> <p>Reading the second paragraph of section 3.1 from the ViT paper, we see the following description:</p> <p>Similar to BERT's <code>[ class ]</code> token, we prepend a learnable embedding to the sequence of embedded patches $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)$, whose state at the output of the Transformer encoder $\\left(\\mathbf{z}_{L}^{0}\\right)$ serves as the image representation $\\mathbf{y}$ (Eq. 4).</p> <p>Note: BERT (Bidirectional Encoder Representations from Transformers) is one of the original machine learning research papers to use the Transformer architecture to achieve outstanding results on natural language processing (NLP) tasks and is where the idea of having a <code>[ class ]</code> token at the start of a sequence originated, class being a description for the \"classification\" class the sequence belonged to.</p> <p>So we need to \"preprend a learnable embedding to the sequence of embedded patches\".</p> <p>Let's start by viewing our sequence of embedded patches tensor (created in 4.5) and its shape.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-47-creating-the-position-embedding","title":"TK 4.7 Creating the position embedding\u00b6","text":"<p>Well, we've got the class token embedding and the patch embedding, now how might we create the position embedding?</p> <p>Or $\\mathbf{E}_{\\text {pos }}$ from equation 1 where $E$ stands for \"embedding\".</p> <p></p> <p>Left: Figure 1 from the ViT paper with the position embedding we're going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the position embedding.</p> <p>Let's find out more by reading section 3.1 of the ViT paper (bold mine):</p> <p>Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.</p> <p>To start creating the position embeddings, let's view our current embeddings.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-48-putting-it-all-together-from-image-to-embedding","title":"TK 4.8 Putting it all together: from image to embedding\u00b6","text":"<p>Alright, we've come a long way in terms of turning our input images into an embedding and replicating equation 1 from section 3.1 of the ViT paper:</p> <p>$$ \\begin{aligned} \\mathbf{z}_{0} &amp;=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, &amp; &amp; \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\end{aligned} $$</p> <p>Let's now put everything together in a single code cell and go from input image ($x$) to output embedding ${z}_0$.</p> <p>We can do so by:</p> <ol> <li>Setting the patch size (we'll use <code>16</code> as it's widely used throughout the paper and for ViT-Base).</li> <li>Getting a single image, printing it's shape and storing its height and width.</li> <li>Adding a batch dimension to the single image so it's compatible with our <code>PatchEmbedding</code> layer.</li> <li>Creating a <code>PatchEmbedding</code> layer with a <code>patch_size=16</code> and <code>embedding_dim=768</code> (from Table 1 for ViT-Base).</li> <li>Passing the single image through the <code>PatchEmbedding</code> layer in 4 to create a sequence of patch embeddings.</li> <li>Creating a class token embedding like in section 4.6.</li> <li>Prepending the class token emebdding to the patch embeddings created in step 5.</li> <li>Creating a position embedding like in section 4.7.</li> <li>Adding the position embedding to the class token and patch embeddings created in step 7.</li> </ol> <p>We'll also make sure to set the random seeds with <code>set_seeds()</code> and print out the shapes of different tensors along the way.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-5-equation-2-multi-head-attention-msa","title":"TK. 5. Equation 2: Multi-Head Attention (MSA)\u00b6","text":"<p>We've got our input data patchified and embedded, now let's move onto the next part of the ViT architecture.</p> <p>To start, we'll break down the Transformer Encoder section into two parts (start small and increase when necessary).</p> <p>The first being equation 2 and the second being equation 3.</p> <p>Recall equation 2 states:</p> <p>$$ \\begin{aligned} \\mathbf{z}_{\\ell}^{\\prime} &amp;=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, &amp; &amp; \\ell=1 \\ldots L \\end{aligned} $$</p> <p>This indicates a Multi-Head Attention (MSA) layer wrapped in a LayerNorm (LN) layer with a residual connection (the input to the layer gets added to the output).</p> <p>Left: Figure 1 from the ViT paper with Multi-Head Attention and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. Right: Mapping the Multi-Head Self Attention (MSA) layer, Norm layer and residual connection to their respective parts of equation 2 in the ViT paper.</p> <p>Many layers you find in research papers are already implemented in modern deep learning frameworks such as PyTorch.</p> <p>In saying this, to replicate these layers and residual connection with PyTorch code we can use:</p> <ul> <li>Multi-Head Self Attention (MSA) - <code>torch.nn.MultiheadAttention()</code>.</li> <li>Norm (LN or LayerNorm) - <code>torch.nn.LayerNorm()</code>.</li> <li>Residual connection - add the input to output (we'll see this later on when we create the full Transformer Encoder block).</li> </ul>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#51-the-layernorm-ln-layer","title":"5.1 The LayerNorm (LN) layer\u00b6","text":"<p>Layer Normalization (<code>torch.nn.LayerNorm()</code> or Norm or LayerNorm or LN) normalizes an input over the last dimension.</p> <p>You can set <code>normalized_shape</code> to be equal to the dimension size you'd like to noramlize over (in our case it'll be $D$ or <code>768</code> for ViT-Base).</p> <p>You can find the formal definition of <code>torch.nn.LayerNorm()</code> in the PyTorch documentation.</p> <p>What does it do?</p> <p>Layer Normalization helps improve training time and model generalization (ability to adapt to unseen data).</p> <p>I like to think of any kind of normalization as \"getting the data into a similar format\" or \"getting data samples into a similar distribution\".</p> <p>Imagine trying to walk up (or down) a set of stairs all with differing heights and lengths.</p> <p>It'd take some adjustment each step right?</p> <p>And what you learn for each step wouldn't necessary help with the next one since they all differ.</p> <p>Normalization (including Layer Normalization) is the equivalent of making all the stairs the same height and length except the stairs are your data samples.</p> <p>So just like you can walk up (or down) stairs with similar heights and lengths much easier than those with unequal heights and widths, neural networks can optimize over data samples with similar distributions (similar mean and standard-deviations) easier than those with varying distributions.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#52-the-multi-head-self-attention-msa-layer","title":"5.2 The Multi-Head Self Attention (MSA) layer\u00b6","text":"<p>The power of the self-attention and multi-head attention (self-attention applied multiple times) were revealed in the form of the original Transformer architecture introduced in the Attention is all you need research paper.</p> <p>There are many resources online to learn more about the Transformer architeture and attention mechanism online such as Jay Alammar's wonderful Illustrated Transformer post and Illustrated Attention post.</p> <p>But we're going to focus more on coding an existing PyTorch MSA implementation than creating our own.</p> <p>However, you can find the formal defintion of the ViT paper's MSA implementation is defined in Appendix A:</p> <p>Left: Vision Transformer architecture overview from Figure 1 of the ViT paper. Right: Definitions of equation 2, section 3.1 and Appendix A of the ViT paper highlighted to reflect their respective parts in Figure 1.</p> <p>The image above highlights the triple input to the MSA layer.</p> <p>This is known as query, key, value input or qkv for short which is fundamental to the self-attention mechanism.</p> <p>In our case, the triple input will be three versions of the output of the Norm layer.</p> <p>Or three versions of our layer-normalized image patch and position embeddings created in section 4.8.</p> <p>We can implement the MSA layer in PyTorch with <code>torch.nn.MultiheadAttention()</code> with the parameters:</p> <ul> <li><code>embed_dim</code> - the embedding dimension from Table 1 (Hidden size $D$).</li> <li><code>num_heads</code> - how many attention heads to use (this is where the term \"multihead\" comes from), this value is also in Table 1 (Heads).</li> <li><code>dropout</code> - whether or not to apply dropout to the attention layer (according to Appendix B.1, dropout isn't used after the qkv-projections).</li> </ul>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#53-replicating-equation-2-with-pytorch-layers","title":"5.3 Replicating Equation 2 with PyTorch layers\u00b6","text":"<p>Let's put everything we've discussed about the LayerNorm (LN) and Multi-Head Attention (MSA) layers in equation 2 into practice.</p> <p>To do so, we'll:</p> <ol> <li>Create a class called <code>MultiheadSelfAttentionBlock()</code> that inherits from <code>torch.nn.Module</code>.</li> <li>Initialize the class with hyperparameters from Table 1 of the ViT paper for the ViT-Base model.</li> <li>Create a layer normalization (LN) layer with <code>torch.nn.LayerNorm()</code> with the <code>normalized_shape</code> parameter the same as our embedding dimension ($D$ from Table 1).</li> <li>Create a multi-head attention (MSA) layer with the appropriate <code>embed_dim</code>, <code>num_heads</code>, <code>dropout</code> and <code>batch_first</code> parameters.</li> <li>Create a <code>forward()</code> method for our class passing the in the inputs through the LN layer and MSA layer.</li> </ol>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-6-equation-3-multilayer-perceptron-mlp","title":"TK 6. Equation 3: Multilayer Perceptron (MLP)\u00b6","text":"<p>UPTOHERE:</p> <ul> <li><p>Replicate equation 3 like replicating equation 2</p> </li> <li><p>TK also called \"feedforward\"</p> </li> </ul> <p>Dropout, when used, is applied after every dense layer except for the the qkv-projections and directly after adding positional- to patch embeddings.</p> <p>The MLP contains two layers with a GELU non-linearity</p> <p>$$ \\begin{aligned} \\mathbf{z}_{\\ell} &amp;=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, &amp; &amp; \\ell=1 \\ldots L \\end{aligned} $$</p> <ul> <li>TK - GELU in PyTorch -- https://pytorch.org/docs/stable/generated/torch.nn.GELU.html</li> </ul>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-7-create-the-transformer-encoder","title":"TK 7. Create the Transformer Encoder\u00b6","text":"<ul> <li>Tk - what is an \"encoder\"?</li> <li>Tk - \"transformer block\" or \"transformer encoder\"? - line this up with the paper</li> </ul> <p>See here for pre-built transformer blocks/layers: https://pytorch.org/docs/stable/nn.html#transformer-layers</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-8-putting-it-all-together-to-create-vit","title":"TK 8. Putting it all together to create ViT\u00b6","text":"<p>TK - replicate this with the TransformerEncoderLayer - https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/</p> <p>Combine the transformer blocks and patched embedding into a ViT architecture.</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-9-inspect-the-model","title":"TK 9. Inspect the model\u00b6","text":"<p>Note: If you go too big, your hardware might not be able to handle it... (e.g. too high of a batch size...)</p> <p>TK - Number of parameters should be equivalent to: https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16 (<code>num_params=86,567,656</code>)</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-10-train-model","title":"TK 10. Train model\u00b6","text":""},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-11-evaluate-model","title":"TK 11. Evaluate model\u00b6","text":"<p>TK - plot the loss curves</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-12-bring-in-pretrained-vit-from-torchvisionmodels-on-same-dataset","title":"TK 12. Bring in pretrained ViT from <code>torchvision.models</code> on same dataset\u00b6","text":"<ul> <li>Get a similar model from here - https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16</li> </ul>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-things-this-replication-misses-out-on","title":"TK - Things this replication misses out on\u00b6","text":"<p>TK Put down the difference in the paper vs this replication</p> <ul> <li>Many of these things are in Table 3:<ul> <li>training data (ImageNet from scratch vs FoodVision Mini data)</li> <li>LR warmup</li> <li>LR decay</li> <li>Weight decay</li> <li>Number of epochs</li> </ul> </li> </ul>"},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-exercises","title":"TK - Exercises\u00b6","text":""},{"location":"Learn/z2m-pytorch/08_pytorch_paper_replicating/#tk-extra-curriculum","title":"TK - Extra-curriculum\u00b6","text":"<ul> <li>layernorm</li> <li>See the illustrated transformer for an overview of the Transformer model:  https://jalammar.github.io/illustrated-transformer/ + https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</li> <li>Attention is all you need paper - Yannic video</li> <li>Vision transformer - yannic video</li> </ul>"},{"location":"Learn/z2m-pytorch/08_pytorch_profiling/","title":"08: PyTorch Profiling","text":"In\u00a0[10]: Copied! <pre>import torch\nimport torchvision\nfrom torch import nn\nfrom torchvision import transforms, datasets\nfrom torchinfo import summary\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom going_modular import data_setup, engine\n</pre> import torch import torchvision from torch import nn from torchvision import transforms, datasets from torchinfo import summary  import numpy as np import matplotlib.pyplot as plt  from going_modular import data_setup, engine In\u00a0[11]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[11]: <pre>'cuda'</pre> In\u00a0[12]: Copied! <pre>import os\nimport requests\nfrom zipfile import ZipFile\n\ndef get_food_image_data():\n    if not os.path.exists(\"data/10_whole_foods\"):\n        os.makedirs(\"data/\", exist_ok=True)\n        # Download data\n        data_url = \"https://storage.googleapis.com/food-vision-image-playground/10_whole_foods.zip\"\n        print(f\"Downloading data from {data_url}...\")\n        requests.get(data_url)\n        # Unzip data\n        targ_dir = \"data/10_whole_foods\"\n        print(f\"Extracting data to {targ_dir}...\")\n        with ZipFile(\"10_whole_foods.zip\") as zip_ref:\n            zip_ref.extractall(targ_dir)\n    else:\n        print(\"data/10_whole_foods dir exists, skipping download\")\n\nget_food_image_data()\n</pre> import os import requests from zipfile import ZipFile  def get_food_image_data():     if not os.path.exists(\"data/10_whole_foods\"):         os.makedirs(\"data/\", exist_ok=True)         # Download data         data_url = \"https://storage.googleapis.com/food-vision-image-playground/10_whole_foods.zip\"         print(f\"Downloading data from {data_url}...\")         requests.get(data_url)         # Unzip data         targ_dir = \"data/10_whole_foods\"         print(f\"Extracting data to {targ_dir}...\")         with ZipFile(\"10_whole_foods.zip\") as zip_ref:             zip_ref.extractall(targ_dir)     else:         print(\"data/10_whole_foods dir exists, skipping download\")  get_food_image_data() <pre>data/10_whole_foods dir exists, skipping download\n</pre> In\u00a0[38]: Copied! <pre># Setup dirs\ntrain_dir = \"data/10_whole_foods/train\"\ntest_dir = \"data/10_whole_foods/test\"\n\n# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Create starter transform\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=32,\n    num_workers=8\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup dirs train_dir = \"data/10_whole_foods/train\" test_dir = \"data/10_whole_foods/test\"  # Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet) normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225])  # Create starter transform simple_transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     normalize ])             # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=simple_transform,     batch_size=32,     num_workers=8 )  train_dataloader, test_dataloader, class_names Out[38]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f052ab26e20&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f05299eed00&gt;,\n ['apple',\n  'banana',\n  'beef',\n  'blueberries',\n  'carrots',\n  'chicken_wings',\n  'egg',\n  'honey',\n  'mushrooms',\n  'strawberries'])</pre> In\u00a0[66]: Copied! <pre>model = torchvision.models.efficientnet_b0(pretrained=True).to(device)\n# model\n</pre> model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # model In\u00a0[67]: Copied! <pre># Update the classifier\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2),\n    nn.Linear(1280, len(class_names)).to(device))\n\n# Freeze all base layers \nfor param in model.features.parameters():\n    param.requires_grad = False\n</pre> # Update the classifier model.classifier = torch.nn.Sequential(     nn.Dropout(p=0.2),     nn.Linear(1280, len(class_names)).to(device))  # Freeze all base layers  for param in model.features.parameters():     param.requires_grad = False In\u00a0[68]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) In\u00a0[69]: Copied! <pre>model.name = \"EfficietNetB0\"\nmodel.name\n</pre> model.name = \"EfficietNetB0\" model.name Out[69]: <pre>'EfficietNetB0'</pre> In\u00a0[70]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\nfrom going_modular.engine import train_step, test_step\nfrom tqdm import tqdm\nwriter = SummaryWriter()\n</pre> from torch.utils.tensorboard import SummaryWriter from going_modular.engine import train_step, test_step from tqdm import tqdm writer = SummaryWriter() <p>Update the <code>train_step()</code> function to include the PyTorch profiler.</p> In\u00a0[71]: Copied! <pre>def train_step(model, dataloader, loss_fn, optimizer):\n    model.train()\n    train_loss, train_acc = 0, 0\n    ## NEW: Add PyTorch profiler\n\n    dir_to_save_logs = os.path.join(\"logs\", datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))\n    with torch.profiler.profile(\n        on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=dir_to_save_logs),\n        # with_stack=True # this adds a lot of overhead to training (tracing all the stack)\n    ):\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to GPU\n            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            \n            # Turn on mixed precision if available\n            with torch.autocast(device_type=device, enabled=True):\n                # 1. Forward pass\n                y_pred = model(X)\n\n                # 2. Calculate loss\n                loss = loss_fn(y_pred, y)\n\n            # 3. Optimizer zero grad\n            optimizer.zero_grad()\n\n            # 4. Loss backward\n            loss.backward()\n\n            # 5. Optimizer step\n            optimizer.step()\n\n            # 6. Calculate metrics\n            train_loss += loss.item()\n            y_pred_class = torch.softmax(y_pred, dim=1).argmax(dim=1)\n            # print(f\"y: \\n{y}\\ny_pred_class:{y_pred_class}\")\n            # print(f\"y argmax: {y_pred.argmax(dim=1)}\")\n            # print(f\"Equal: {(y_pred_class == y)}\")\n            train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n            # print(f\"batch: {batch} train_acc: {train_acc}\")\n\n    # Adjust returned metrics\n    return train_loss / len(dataloader), train_acc / len(dataloader)\n</pre> def train_step(model, dataloader, loss_fn, optimizer):     model.train()     train_loss, train_acc = 0, 0     ## NEW: Add PyTorch profiler      dir_to_save_logs = os.path.join(\"logs\", datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))     with torch.profiler.profile(         on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=dir_to_save_logs),         # with_stack=True # this adds a lot of overhead to training (tracing all the stack)     ):         for batch, (X, y) in enumerate(dataloader):             # Send data to GPU             X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)                          # Turn on mixed precision if available             with torch.autocast(device_type=device, enabled=True):                 # 1. Forward pass                 y_pred = model(X)                  # 2. Calculate loss                 loss = loss_fn(y_pred, y)              # 3. Optimizer zero grad             optimizer.zero_grad()              # 4. Loss backward             loss.backward()              # 5. Optimizer step             optimizer.step()              # 6. Calculate metrics             train_loss += loss.item()             y_pred_class = torch.softmax(y_pred, dim=1).argmax(dim=1)             # print(f\"y: \\n{y}\\ny_pred_class:{y_pred_class}\")             # print(f\"y argmax: {y_pred.argmax(dim=1)}\")             # print(f\"Equal: {(y_pred_class == y)}\")             train_acc += (y_pred_class == y).sum().item() / len(y_pred)             # print(f\"batch: {batch} train_acc: {train_acc}\")      # Adjust returned metrics     return train_loss / len(dataloader), train_acc / len(dataloader) <p>TK - Now to use the writer, we've got to adjust the <code>train()</code> function...</p> In\u00a0[72]: Copied! <pre>def train(\n    model,\n    train_dataloader,\n    test_dataloader,\n    optimizer,\n    loss_fn=nn.CrossEntropyLoss(),\n    epochs=5,\n):\n\n    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(\n            model=model,\n            dataloader=train_dataloader,\n            loss_fn=loss_fn,\n            optimizer=optimizer,\n        )\n        test_loss, test_acc = test_step(\n            model=model, dataloader=test_dataloader, loss_fn=loss_fn\n        )\n\n        # Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        # Add results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n    \n    # Close the writer\n    writer.close()\n\n    return results\n</pre> def train(     model,     train_dataloader,     test_dataloader,     optimizer,     loss_fn=nn.CrossEntropyLoss(),     epochs=5, ):      results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}      for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(             model=model,             dataloader=train_dataloader,             loss_fn=loss_fn,             optimizer=optimizer,         )         test_loss, test_acc = test_step(             model=model, dataloader=test_dataloader, loss_fn=loss_fn         )          # Print out what's happening         print(             f\"Epoch: {epoch+1} | \"             f\"train_loss: {train_loss:.4f} | \"             f\"train_acc: {train_acc:.4f} | \"             f\"test_loss: {test_loss:.4f} | \"             f\"test_acc: {test_acc:.4f}\"         )          # Update results         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)          # Add results to SummaryWriter         writer.add_scalars(main_tag=\"Loss\",                             tag_scalar_dict={\"train_loss\": train_loss,                                             \"test_loss\": test_loss},                            global_step=epoch)         writer.add_scalars(main_tag=\"Accuracy\",                             tag_scalar_dict={\"train_acc\": train_acc,                                             \"test_acc\": test_acc},                             global_step=epoch)          # Close the writer     writer.close()      return results In\u00a0[73]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated\nresults = train(model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=5)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated results = train(model=model,         train_dataloader=train_dataloader,         test_dataloader=test_dataloader,         optimizer=optimizer,         loss_fn=loss_fn,         epochs=5) <pre> 20%|\u2588\u2588        | 1/5 [00:05&lt;00:21,  5.27s/it]</pre> <pre>Epoch: 1 | train_loss: 1.9644 | train_acc: 0.4386 | test_loss: 1.5205 | test_acc: 0.7865\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:09&lt;00:14,  4.94s/it]</pre> <pre>Epoch: 2 | train_loss: 1.2589 | train_acc: 0.7878 | test_loss: 1.1589 | test_acc: 0.7604\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:14&lt;00:09,  4.72s/it]</pre> <pre>Epoch: 3 | train_loss: 0.8642 | train_acc: 0.8776 | test_loss: 0.9347 | test_acc: 0.7917\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.56s/it]</pre> <pre>Epoch: 4 | train_loss: 0.6827 | train_acc: 0.8856 | test_loss: 0.6637 | test_acc: 0.8750\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:23&lt;00:00,  4.65s/it]</pre> <pre>Epoch: 5 | train_loss: 0.5688 | train_acc: 0.9069 | test_loss: 0.6175 | test_acc: 0.8854\n</pre> <pre>\n</pre> <p>Looks like mixed precision doesn't offer much benefit for smaller feature extraction models...</p> In\u00a0[\u00a0]: Copied! <pre># # Without mixed precision\n#  20%|\u2588\u2588        | 1/5 [00:03&lt;00:14,  3.71s/it]Epoch: 1 | train_loss: 0.5229 | train_acc: 0.9054 | test_loss: 0.5776 | test_acc: 0.8542\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:07&lt;00:11,  3.74s/it]Epoch: 2 | train_loss: 0.4699 | train_acc: 0.9001 | test_loss: 0.5160 | test_acc: 0.8802\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:10&lt;00:07,  3.63s/it]Epoch: 3 | train_loss: 0.3913 | train_acc: 0.9196 | test_loss: 0.4888 | test_acc: 0.8906\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14&lt;00:03,  3.61s/it]Epoch: 4 | train_loss: 0.3724 | train_acc: 0.9371 | test_loss: 0.4931 | test_acc: 0.8698\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:18&lt;00:00,  3.61s/it]Epoch: 5 | train_loss: 0.3315 | train_acc: 0.9381 | test_loss: 0.4405 | test_acc: 0.8750\n\n# # With mixed precision\n#  20%|\u2588\u2588        | 1/5 [00:04&lt;00:17,  4.40s/it]Epoch: 1 | train_loss: 0.3027 | train_acc: 0.9554 | test_loss: 0.4386 | test_acc: 0.8802\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:08&lt;00:13,  4.49s/it]Epoch: 2 | train_loss: 0.2826 | train_acc: 0.9539 | test_loss: 0.4080 | test_acc: 0.8802\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13&lt;00:08,  4.48s/it]Epoch: 3 | train_loss: 0.2450 | train_acc: 0.9609 | test_loss: 0.4130 | test_acc: 0.8750\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.53s/it]Epoch: 4 | train_loss: 0.2450 | train_acc: 0.9594 | test_loss: 0.4158 | test_acc: 0.8802\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.49s/it]Epoch: 5 | train_loss: 0.2307 | train_acc: 0.9639 | test_loss: 0.4124 | test_acc: 0.8906\n</pre> # # Without mixed precision #  20%|\u2588\u2588        | 1/5 [00:03&lt;00:14,  3.71s/it]Epoch: 1 | train_loss: 0.5229 | train_acc: 0.9054 | test_loss: 0.5776 | test_acc: 0.8542 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:07&lt;00:11,  3.74s/it]Epoch: 2 | train_loss: 0.4699 | train_acc: 0.9001 | test_loss: 0.5160 | test_acc: 0.8802 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:10&lt;00:07,  3.63s/it]Epoch: 3 | train_loss: 0.3913 | train_acc: 0.9196 | test_loss: 0.4888 | test_acc: 0.8906 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14&lt;00:03,  3.61s/it]Epoch: 4 | train_loss: 0.3724 | train_acc: 0.9371 | test_loss: 0.4931 | test_acc: 0.8698 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:18&lt;00:00,  3.61s/it]Epoch: 5 | train_loss: 0.3315 | train_acc: 0.9381 | test_loss: 0.4405 | test_acc: 0.8750  # # With mixed precision #  20%|\u2588\u2588        | 1/5 [00:04&lt;00:17,  4.40s/it]Epoch: 1 | train_loss: 0.3027 | train_acc: 0.9554 | test_loss: 0.4386 | test_acc: 0.8802 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:08&lt;00:13,  4.49s/it]Epoch: 2 | train_loss: 0.2826 | train_acc: 0.9539 | test_loss: 0.4080 | test_acc: 0.8802 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13&lt;00:08,  4.48s/it]Epoch: 3 | train_loss: 0.2450 | train_acc: 0.9609 | test_loss: 0.4130 | test_acc: 0.8750 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.53s/it]Epoch: 4 | train_loss: 0.2450 | train_acc: 0.9594 | test_loss: 0.4158 | test_acc: 0.8802 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.49s/it]Epoch: 5 | train_loss: 0.2307 | train_acc: 0.9639 | test_loss: 0.4124 | test_acc: 0.8906 In\u00a0[74]: Copied! <pre># Unfreeze all base layers \nfor param in model.features.parameters():\n    param.requires_grad = True\n\n# for param in model.features.parameters():\n#     print(param.requires_grad)\n</pre> # Unfreeze all base layers  for param in model.features.parameters():     param.requires_grad = True  # for param in model.features.parameters(): #     print(param.requires_grad) In\u00a0[75]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated\nresults = train(model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=5)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated results = train(model=model,         train_dataloader=train_dataloader,         test_dataloader=test_dataloader,         optimizer=optimizer,         loss_fn=loss_fn,         epochs=5) <pre> 20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]</pre> <pre>Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]</pre> <pre>Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]</pre> <pre>Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]</pre> <pre>Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]</pre> <pre>Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># # Without mixed precision...\n#  20%|\u2588\u2588        | 1/5 [00:11&lt;00:46, 11.61s/it]Epoch: 1 | train_loss: 0.4507 | train_acc: 0.8648 | test_loss: 1.0603 | test_acc: 0.7604\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:24&lt;00:36, 12.21s/it]Epoch: 2 | train_loss: 0.1659 | train_acc: 0.9464 | test_loss: 0.6398 | test_acc: 0.8490\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:36&lt;00:24, 12.38s/it]Epoch: 3 | train_loss: 0.1261 | train_acc: 0.9698 | test_loss: 0.7149 | test_acc: 0.8542\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:49&lt;00:12, 12.53s/it]Epoch: 4 | train_loss: 0.1250 | train_acc: 0.9609 | test_loss: 0.7441 | test_acc: 0.7917\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:02&lt;00:00, 12.42s/it]Epoch: 5 | train_loss: 0.1282 | train_acc: 0.9564 | test_loss: 0.8701 | test_acc: 0.8385\n\n# # With mixed precision...\n#  20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125\n</pre> # # Without mixed precision... #  20%|\u2588\u2588        | 1/5 [00:11&lt;00:46, 11.61s/it]Epoch: 1 | train_loss: 0.4507 | train_acc: 0.8648 | test_loss: 1.0603 | test_acc: 0.7604 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:24&lt;00:36, 12.21s/it]Epoch: 2 | train_loss: 0.1659 | train_acc: 0.9464 | test_loss: 0.6398 | test_acc: 0.8490 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:36&lt;00:24, 12.38s/it]Epoch: 3 | train_loss: 0.1261 | train_acc: 0.9698 | test_loss: 0.7149 | test_acc: 0.8542 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:49&lt;00:12, 12.53s/it]Epoch: 4 | train_loss: 0.1250 | train_acc: 0.9609 | test_loss: 0.7441 | test_acc: 0.7917 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:02&lt;00:00, 12.42s/it]Epoch: 5 | train_loss: 0.1282 | train_acc: 0.9564 | test_loss: 0.8701 | test_acc: 0.8385  # # With mixed precision... #  20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125 <p>Checking the PyTorch profiler, it seems that mixed precision utilises some Tensor Cores, however, these aren't large numbers.</p> <p>E.g. it uses 9-12% Tensor Cores. Perhaps the slow down when using mixed precision is because the tensors have to get altered and converted when there isn't very many of them. For example only 9-12% of tensors get converted so the speed up gains aren't realised on these tensors because they get cancelled out by the conversion time.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Learn/z2m-pytorch/08_pytorch_profiling/#08-pytorch-profiling","title":"08: PyTorch Profiling\u00b6","text":"<p>This notebook is an experiment to try out the PyTorch profiler.</p> <p>See here for more:</p> <ul> <li>https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/</li> <li>https://pytorch.org/docs/stable/profiler.html</li> </ul>"},{"location":"Learn/z2m-pytorch/08_pytorch_profiling/#setup-device","title":"Setup device\u00b6","text":""},{"location":"Learn/z2m-pytorch/08_pytorch_profiling/#get-and-load-data","title":"Get and load data\u00b6","text":""},{"location":"Learn/z2m-pytorch/08_pytorch_profiling/#load-model","title":"Load model\u00b6","text":""},{"location":"Learn/z2m-pytorch/08_pytorch_profiling/#train-model-and-track-results","title":"Train model and track results\u00b6","text":""},{"location":"Learn/z2m-pytorch/08_pytorch_profiling/#adjust-training-function-to-track-results-with-summarywriter","title":"Adjust training function to track results with <code>SummaryWriter</code>\u00b6","text":""},{"location":"Learn/z2m-pytorch/08_pytorch_profiling/#try-mixed-precision-with-larger-model","title":"Try mixed precision with larger model\u00b6","text":"<p>Now we'll try turn on mixed precision with a larger model (e.g. EffifientNetB0 with all layers tuneable).</p>"},{"location":"Learn/z2m-pytorch/08_pytorch_profiling/#extensions","title":"Extensions\u00b6","text":"<ul> <li>Does changing the data input size to EfficientNetB4 change its results? E.g. input image size of (380, 380) instead of (224, 224)?</li> </ul>"},{"location":"Learn/z2m-pytorch/10_pytorch_profiling/","title":"08: PyTorch Profiling","text":"In\u00a0[10]: Copied! <pre>import torch\nimport torchvision\nfrom torch import nn\nfrom torchvision import transforms, datasets\nfrom torchinfo import summary\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom going_modular import data_setup, engine\n</pre> import torch import torchvision from torch import nn from torchvision import transforms, datasets from torchinfo import summary  import numpy as np import matplotlib.pyplot as plt  from going_modular import data_setup, engine In\u00a0[11]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[11]: <pre>'cuda'</pre> In\u00a0[12]: Copied! <pre>import os\nimport requests\nfrom zipfile import ZipFile\n\ndef get_food_image_data():\n    if not os.path.exists(\"data/10_whole_foods\"):\n        os.makedirs(\"data/\", exist_ok=True)\n        # Download data\n        data_url = \"https://storage.googleapis.com/food-vision-image-playground/10_whole_foods.zip\"\n        print(f\"Downloading data from {data_url}...\")\n        requests.get(data_url)\n        # Unzip data\n        targ_dir = \"data/10_whole_foods\"\n        print(f\"Extracting data to {targ_dir}...\")\n        with ZipFile(\"10_whole_foods.zip\") as zip_ref:\n            zip_ref.extractall(targ_dir)\n    else:\n        print(\"data/10_whole_foods dir exists, skipping download\")\n\nget_food_image_data()\n</pre> import os import requests from zipfile import ZipFile  def get_food_image_data():     if not os.path.exists(\"data/10_whole_foods\"):         os.makedirs(\"data/\", exist_ok=True)         # Download data         data_url = \"https://storage.googleapis.com/food-vision-image-playground/10_whole_foods.zip\"         print(f\"Downloading data from {data_url}...\")         requests.get(data_url)         # Unzip data         targ_dir = \"data/10_whole_foods\"         print(f\"Extracting data to {targ_dir}...\")         with ZipFile(\"10_whole_foods.zip\") as zip_ref:             zip_ref.extractall(targ_dir)     else:         print(\"data/10_whole_foods dir exists, skipping download\")  get_food_image_data() <pre>data/10_whole_foods dir exists, skipping download\n</pre> In\u00a0[38]: Copied! <pre># Setup dirs\ntrain_dir = \"data/10_whole_foods/train\"\ntest_dir = \"data/10_whole_foods/test\"\n\n# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Create starter transform\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=32,\n    num_workers=8\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup dirs train_dir = \"data/10_whole_foods/train\" test_dir = \"data/10_whole_foods/test\"  # Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet) normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225])  # Create starter transform simple_transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     normalize ])             # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=simple_transform,     batch_size=32,     num_workers=8 )  train_dataloader, test_dataloader, class_names Out[38]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f052ab26e20&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f05299eed00&gt;,\n ['apple',\n  'banana',\n  'beef',\n  'blueberries',\n  'carrots',\n  'chicken_wings',\n  'egg',\n  'honey',\n  'mushrooms',\n  'strawberries'])</pre> In\u00a0[66]: Copied! <pre>model = torchvision.models.efficientnet_b0(pretrained=True).to(device)\n# model\n</pre> model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # model In\u00a0[67]: Copied! <pre># Update the classifier\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2),\n    nn.Linear(1280, len(class_names)).to(device))\n\n# Freeze all base layers \nfor param in model.features.parameters():\n    param.requires_grad = False\n</pre> # Update the classifier model.classifier = torch.nn.Sequential(     nn.Dropout(p=0.2),     nn.Linear(1280, len(class_names)).to(device))  # Freeze all base layers  for param in model.features.parameters():     param.requires_grad = False In\u00a0[68]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) In\u00a0[69]: Copied! <pre>model.name = \"EfficietNetB0\"\nmodel.name\n</pre> model.name = \"EfficietNetB0\" model.name Out[69]: <pre>'EfficietNetB0'</pre> In\u00a0[70]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\nfrom going_modular.engine import train_step, test_step\nfrom tqdm import tqdm\nwriter = SummaryWriter()\n</pre> from torch.utils.tensorboard import SummaryWriter from going_modular.engine import train_step, test_step from tqdm import tqdm writer = SummaryWriter() <p>Update the <code>train_step()</code> function to include the PyTorch profiler.</p> In\u00a0[71]: Copied! <pre>def train_step(model, dataloader, loss_fn, optimizer):\n    model.train()\n    train_loss, train_acc = 0, 0\n    ## NEW: Add PyTorch profiler\n\n    dir_to_save_logs = os.path.join(\"logs\", datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))\n    with torch.profiler.profile(\n        on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=dir_to_save_logs),\n        # with_stack=True # this adds a lot of overhead to training (tracing all the stack)\n    ):\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to GPU\n            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            \n            # Turn on mixed precision if available\n            with torch.autocast(device_type=device, enabled=True):\n                # 1. Forward pass\n                y_pred = model(X)\n\n                # 2. Calculate loss\n                loss = loss_fn(y_pred, y)\n\n            # 3. Optimizer zero grad\n            optimizer.zero_grad()\n\n            # 4. Loss backward\n            loss.backward()\n\n            # 5. Optimizer step\n            optimizer.step()\n\n            # 6. Calculate metrics\n            train_loss += loss.item()\n            y_pred_class = torch.softmax(y_pred, dim=1).argmax(dim=1)\n            # print(f\"y: \\n{y}\\ny_pred_class:{y_pred_class}\")\n            # print(f\"y argmax: {y_pred.argmax(dim=1)}\")\n            # print(f\"Equal: {(y_pred_class == y)}\")\n            train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n            # print(f\"batch: {batch} train_acc: {train_acc}\")\n\n    # Adjust returned metrics\n    return train_loss / len(dataloader), train_acc / len(dataloader)\n</pre> def train_step(model, dataloader, loss_fn, optimizer):     model.train()     train_loss, train_acc = 0, 0     ## NEW: Add PyTorch profiler      dir_to_save_logs = os.path.join(\"logs\", datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))     with torch.profiler.profile(         on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=dir_to_save_logs),         # with_stack=True # this adds a lot of overhead to training (tracing all the stack)     ):         for batch, (X, y) in enumerate(dataloader):             # Send data to GPU             X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)                          # Turn on mixed precision if available             with torch.autocast(device_type=device, enabled=True):                 # 1. Forward pass                 y_pred = model(X)                  # 2. Calculate loss                 loss = loss_fn(y_pred, y)              # 3. Optimizer zero grad             optimizer.zero_grad()              # 4. Loss backward             loss.backward()              # 5. Optimizer step             optimizer.step()              # 6. Calculate metrics             train_loss += loss.item()             y_pred_class = torch.softmax(y_pred, dim=1).argmax(dim=1)             # print(f\"y: \\n{y}\\ny_pred_class:{y_pred_class}\")             # print(f\"y argmax: {y_pred.argmax(dim=1)}\")             # print(f\"Equal: {(y_pred_class == y)}\")             train_acc += (y_pred_class == y).sum().item() / len(y_pred)             # print(f\"batch: {batch} train_acc: {train_acc}\")      # Adjust returned metrics     return train_loss / len(dataloader), train_acc / len(dataloader) <p>TK - Now to use the writer, we've got to adjust the <code>train()</code> function...</p> In\u00a0[72]: Copied! <pre>def train(\n    model,\n    train_dataloader,\n    test_dataloader,\n    optimizer,\n    loss_fn=nn.CrossEntropyLoss(),\n    epochs=5,\n):\n\n    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(\n            model=model,\n            dataloader=train_dataloader,\n            loss_fn=loss_fn,\n            optimizer=optimizer,\n        )\n        test_loss, test_acc = test_step(\n            model=model, dataloader=test_dataloader, loss_fn=loss_fn\n        )\n\n        # Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        # Add results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n    \n    # Close the writer\n    writer.close()\n\n    return results\n</pre> def train(     model,     train_dataloader,     test_dataloader,     optimizer,     loss_fn=nn.CrossEntropyLoss(),     epochs=5, ):      results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}      for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(             model=model,             dataloader=train_dataloader,             loss_fn=loss_fn,             optimizer=optimizer,         )         test_loss, test_acc = test_step(             model=model, dataloader=test_dataloader, loss_fn=loss_fn         )          # Print out what's happening         print(             f\"Epoch: {epoch+1} | \"             f\"train_loss: {train_loss:.4f} | \"             f\"train_acc: {train_acc:.4f} | \"             f\"test_loss: {test_loss:.4f} | \"             f\"test_acc: {test_acc:.4f}\"         )          # Update results         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)          # Add results to SummaryWriter         writer.add_scalars(main_tag=\"Loss\",                             tag_scalar_dict={\"train_loss\": train_loss,                                             \"test_loss\": test_loss},                            global_step=epoch)         writer.add_scalars(main_tag=\"Accuracy\",                             tag_scalar_dict={\"train_acc\": train_acc,                                             \"test_acc\": test_acc},                             global_step=epoch)          # Close the writer     writer.close()      return results In\u00a0[73]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated\nresults = train(model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=5)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated results = train(model=model,         train_dataloader=train_dataloader,         test_dataloader=test_dataloader,         optimizer=optimizer,         loss_fn=loss_fn,         epochs=5) <pre> 20%|\u2588\u2588        | 1/5 [00:05&lt;00:21,  5.27s/it]</pre> <pre>Epoch: 1 | train_loss: 1.9644 | train_acc: 0.4386 | test_loss: 1.5205 | test_acc: 0.7865\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:09&lt;00:14,  4.94s/it]</pre> <pre>Epoch: 2 | train_loss: 1.2589 | train_acc: 0.7878 | test_loss: 1.1589 | test_acc: 0.7604\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:14&lt;00:09,  4.72s/it]</pre> <pre>Epoch: 3 | train_loss: 0.8642 | train_acc: 0.8776 | test_loss: 0.9347 | test_acc: 0.7917\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.56s/it]</pre> <pre>Epoch: 4 | train_loss: 0.6827 | train_acc: 0.8856 | test_loss: 0.6637 | test_acc: 0.8750\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:23&lt;00:00,  4.65s/it]</pre> <pre>Epoch: 5 | train_loss: 0.5688 | train_acc: 0.9069 | test_loss: 0.6175 | test_acc: 0.8854\n</pre> <pre>\n</pre> <p>Looks like mixed precision doesn't offer much benefit for smaller feature extraction models...</p> In\u00a0[\u00a0]: Copied! <pre># # Without mixed precision\n#  20%|\u2588\u2588        | 1/5 [00:03&lt;00:14,  3.71s/it]Epoch: 1 | train_loss: 0.5229 | train_acc: 0.9054 | test_loss: 0.5776 | test_acc: 0.8542\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:07&lt;00:11,  3.74s/it]Epoch: 2 | train_loss: 0.4699 | train_acc: 0.9001 | test_loss: 0.5160 | test_acc: 0.8802\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:10&lt;00:07,  3.63s/it]Epoch: 3 | train_loss: 0.3913 | train_acc: 0.9196 | test_loss: 0.4888 | test_acc: 0.8906\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14&lt;00:03,  3.61s/it]Epoch: 4 | train_loss: 0.3724 | train_acc: 0.9371 | test_loss: 0.4931 | test_acc: 0.8698\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:18&lt;00:00,  3.61s/it]Epoch: 5 | train_loss: 0.3315 | train_acc: 0.9381 | test_loss: 0.4405 | test_acc: 0.8750\n\n# # With mixed precision\n#  20%|\u2588\u2588        | 1/5 [00:04&lt;00:17,  4.40s/it]Epoch: 1 | train_loss: 0.3027 | train_acc: 0.9554 | test_loss: 0.4386 | test_acc: 0.8802\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:08&lt;00:13,  4.49s/it]Epoch: 2 | train_loss: 0.2826 | train_acc: 0.9539 | test_loss: 0.4080 | test_acc: 0.8802\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13&lt;00:08,  4.48s/it]Epoch: 3 | train_loss: 0.2450 | train_acc: 0.9609 | test_loss: 0.4130 | test_acc: 0.8750\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.53s/it]Epoch: 4 | train_loss: 0.2450 | train_acc: 0.9594 | test_loss: 0.4158 | test_acc: 0.8802\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.49s/it]Epoch: 5 | train_loss: 0.2307 | train_acc: 0.9639 | test_loss: 0.4124 | test_acc: 0.8906\n</pre> # # Without mixed precision #  20%|\u2588\u2588        | 1/5 [00:03&lt;00:14,  3.71s/it]Epoch: 1 | train_loss: 0.5229 | train_acc: 0.9054 | test_loss: 0.5776 | test_acc: 0.8542 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:07&lt;00:11,  3.74s/it]Epoch: 2 | train_loss: 0.4699 | train_acc: 0.9001 | test_loss: 0.5160 | test_acc: 0.8802 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:10&lt;00:07,  3.63s/it]Epoch: 3 | train_loss: 0.3913 | train_acc: 0.9196 | test_loss: 0.4888 | test_acc: 0.8906 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14&lt;00:03,  3.61s/it]Epoch: 4 | train_loss: 0.3724 | train_acc: 0.9371 | test_loss: 0.4931 | test_acc: 0.8698 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:18&lt;00:00,  3.61s/it]Epoch: 5 | train_loss: 0.3315 | train_acc: 0.9381 | test_loss: 0.4405 | test_acc: 0.8750  # # With mixed precision #  20%|\u2588\u2588        | 1/5 [00:04&lt;00:17,  4.40s/it]Epoch: 1 | train_loss: 0.3027 | train_acc: 0.9554 | test_loss: 0.4386 | test_acc: 0.8802 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:08&lt;00:13,  4.49s/it]Epoch: 2 | train_loss: 0.2826 | train_acc: 0.9539 | test_loss: 0.4080 | test_acc: 0.8802 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13&lt;00:08,  4.48s/it]Epoch: 3 | train_loss: 0.2450 | train_acc: 0.9609 | test_loss: 0.4130 | test_acc: 0.8750 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.53s/it]Epoch: 4 | train_loss: 0.2450 | train_acc: 0.9594 | test_loss: 0.4158 | test_acc: 0.8802 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.49s/it]Epoch: 5 | train_loss: 0.2307 | train_acc: 0.9639 | test_loss: 0.4124 | test_acc: 0.8906 In\u00a0[74]: Copied! <pre># Unfreeze all base layers \nfor param in model.features.parameters():\n    param.requires_grad = True\n\n# for param in model.features.parameters():\n#     print(param.requires_grad)\n</pre> # Unfreeze all base layers  for param in model.features.parameters():     param.requires_grad = True  # for param in model.features.parameters(): #     print(param.requires_grad) In\u00a0[75]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated\nresults = train(model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=5)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated results = train(model=model,         train_dataloader=train_dataloader,         test_dataloader=test_dataloader,         optimizer=optimizer,         loss_fn=loss_fn,         epochs=5) <pre> 20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]</pre> <pre>Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]</pre> <pre>Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]</pre> <pre>Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]</pre> <pre>Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]</pre> <pre>Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># # Without mixed precision...\n#  20%|\u2588\u2588        | 1/5 [00:11&lt;00:46, 11.61s/it]Epoch: 1 | train_loss: 0.4507 | train_acc: 0.8648 | test_loss: 1.0603 | test_acc: 0.7604\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:24&lt;00:36, 12.21s/it]Epoch: 2 | train_loss: 0.1659 | train_acc: 0.9464 | test_loss: 0.6398 | test_acc: 0.8490\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:36&lt;00:24, 12.38s/it]Epoch: 3 | train_loss: 0.1261 | train_acc: 0.9698 | test_loss: 0.7149 | test_acc: 0.8542\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:49&lt;00:12, 12.53s/it]Epoch: 4 | train_loss: 0.1250 | train_acc: 0.9609 | test_loss: 0.7441 | test_acc: 0.7917\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:02&lt;00:00, 12.42s/it]Epoch: 5 | train_loss: 0.1282 | train_acc: 0.9564 | test_loss: 0.8701 | test_acc: 0.8385\n\n# # With mixed precision...\n#  20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125\n</pre> # # Without mixed precision... #  20%|\u2588\u2588        | 1/5 [00:11&lt;00:46, 11.61s/it]Epoch: 1 | train_loss: 0.4507 | train_acc: 0.8648 | test_loss: 1.0603 | test_acc: 0.7604 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:24&lt;00:36, 12.21s/it]Epoch: 2 | train_loss: 0.1659 | train_acc: 0.9464 | test_loss: 0.6398 | test_acc: 0.8490 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:36&lt;00:24, 12.38s/it]Epoch: 3 | train_loss: 0.1261 | train_acc: 0.9698 | test_loss: 0.7149 | test_acc: 0.8542 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:49&lt;00:12, 12.53s/it]Epoch: 4 | train_loss: 0.1250 | train_acc: 0.9609 | test_loss: 0.7441 | test_acc: 0.7917 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:02&lt;00:00, 12.42s/it]Epoch: 5 | train_loss: 0.1282 | train_acc: 0.9564 | test_loss: 0.8701 | test_acc: 0.8385  # # With mixed precision... #  20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125 <p>Checking the PyTorch profiler, it seems that mixed precision utilises some Tensor Cores, however, these aren't large numbers.</p> <p>E.g. it uses 9-12% Tensor Cores. Perhaps the slow down when using mixed precision is because the tensors have to get altered and converted when there isn't very many of them. For example only 9-12% of tensors get converted so the speed up gains aren't realised on these tensors because they get cancelled out by the conversion time.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Learn/z2m-pytorch/10_pytorch_profiling/#08-pytorch-profiling","title":"08: PyTorch Profiling\u00b6","text":"<p>This notebook is an experiment to try out the PyTorch profiler.</p> <p>See here for more:</p> <ul> <li>https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/</li> <li>https://pytorch.org/docs/stable/profiler.html</li> </ul>"},{"location":"Learn/z2m-pytorch/10_pytorch_profiling/#setup-device","title":"Setup device\u00b6","text":""},{"location":"Learn/z2m-pytorch/10_pytorch_profiling/#get-and-load-data","title":"Get and load data\u00b6","text":""},{"location":"Learn/z2m-pytorch/10_pytorch_profiling/#load-model","title":"Load model\u00b6","text":""},{"location":"Learn/z2m-pytorch/10_pytorch_profiling/#train-model-and-track-results","title":"Train model and track results\u00b6","text":""},{"location":"Learn/z2m-pytorch/10_pytorch_profiling/#adjust-training-function-to-track-results-with-summarywriter","title":"Adjust training function to track results with <code>SummaryWriter</code>\u00b6","text":""},{"location":"Learn/z2m-pytorch/10_pytorch_profiling/#try-mixed-precision-with-larger-model","title":"Try mixed precision with larger model\u00b6","text":"<p>Now we'll try turn on mixed precision with a larger model (e.g. EffifientNetB0 with all layers tuneable).</p>"},{"location":"Learn/z2m-pytorch/10_pytorch_profiling/#extensions","title":"Extensions\u00b6","text":"<ul> <li>Does changing the data input size to EfficientNetB4 change its results? E.g. input image size of (380, 380) instead of (224, 224)?</li> </ul>"},{"location":"Learn/z2m-pytorch/helper_functions/","title":"Helper functions","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nA series of helper functions used throughout the course.\n\nIf a function gets defined once and could be used over and over, it'll go in here.\n\"\"\"\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> \"\"\" A series of helper functions used throughout the course.  If a function gets defined once and could be used over and over, it'll go in here. \"\"\" import torch import matplotlib.pyplot as plt import numpy as np In\u00a0[\u00a0]: Copied! <pre>from torch import nn\n</pre> from torch import nn In\u00a0[\u00a0]: Copied! <pre>import os\nimport zipfile\n</pre> import os import zipfile In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import requests\n</pre> import requests In\u00a0[\u00a0]: Copied! <pre># Walk through an image classification directory and find out how many files (images)\n# are in each subdirectory.\nimport os\n</pre> # Walk through an image classification directory and find out how many files (images) # are in each subdirectory. import os In\u00a0[\u00a0]: Copied! <pre>def walk_through_dir(dir_path):\n\"\"\"\n    Walks through dir_path returning its contents.\n    Args:\n    dir_path (str): target directory\n\n    Returns:\n    A print out of:\n      number of subdiretories in dir_path\n      number of images (files) in each subdirectory\n      name of each subdirectory\n    \"\"\"\n    for dirpath, dirnames, filenames in os.walk(dir_path):\n        print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n</pre> def walk_through_dir(dir_path):     \"\"\"     Walks through dir_path returning its contents.     Args:     dir_path (str): target directory      Returns:     A print out of:       number of subdiretories in dir_path       number of images (files) in each subdirectory       name of each subdirectory     \"\"\"     for dirpath, dirnames, filenames in os.walk(dir_path):         print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\") In\u00a0[\u00a0]: Copied! <pre>def plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n\"\"\"Plots decision boundaries of model predicting on X in comparison to y.\n\n    Source - https://madewithml.com/courses/foundations/neural-networks/ (with modifications)\n    \"\"\"\n    # Put everything to CPU (works better with NumPy + Matplotlib)\n    model.to(\"cpu\")\n    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n\n    # Setup prediction boundaries and grid\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n\n    # Make features\n    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n\n    # Make predictions\n    model.eval()\n    with torch.inference_mode():\n        y_logits = model(X_to_pred_on)\n\n    # Test for multi-class or binary and adjust logits to prediction labels\n    if len(torch.unique(y)) &gt; 2:\n        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # mutli-class\n    else:\n        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n\n    # Reshape preds and plot\n    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n</pre> def plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):     \"\"\"Plots decision boundaries of model predicting on X in comparison to y.      Source - https://madewithml.com/courses/foundations/neural-networks/ (with modifications)     \"\"\"     # Put everything to CPU (works better with NumPy + Matplotlib)     model.to(\"cpu\")     X, y = X.to(\"cpu\"), y.to(\"cpu\")      # Setup prediction boundaries and grid     x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1     y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1     xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))      # Make features     X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()      # Make predictions     model.eval()     with torch.inference_mode():         y_logits = model(X_to_pred_on)      # Test for multi-class or binary and adjust logits to prediction labels     if len(torch.unique(y)) &gt; 2:         y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # mutli-class     else:         y_pred = torch.round(torch.sigmoid(y_logits))  # binary      # Reshape preds and plot     y_pred = y_pred.reshape(xx.shape).detach().numpy()     plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)     plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)     plt.xlim(xx.min(), xx.max())     plt.ylim(yy.min(), yy.max()) In\u00a0[\u00a0]: Copied! <pre># Plot linear data or training and test and predictions (optional)\ndef plot_predictions(\n    train_data, train_labels, test_data, test_labels, predictions=None\n):\n\"\"\"\n  Plots linear training data and test data and compares predictions.\n  \"\"\"\n    plt.figure(figsize=(10, 7))\n\n    # Plot training data in blue\n    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n\n    # Plot test data in green\n    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n    if predictions is not None:\n        # Plot the predictions in red (predictions were made on the test data)\n        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n    # Show the legend\n    plt.legend(prop={\"size\": 14})\n</pre> # Plot linear data or training and test and predictions (optional) def plot_predictions(     train_data, train_labels, test_data, test_labels, predictions=None ):     \"\"\"   Plots linear training data and test data and compares predictions.   \"\"\"     plt.figure(figsize=(10, 7))      # Plot training data in blue     plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")      # Plot test data in green     plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")      if predictions is not None:         # Plot the predictions in red (predictions were made on the test data)         plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")      # Show the legend     plt.legend(prop={\"size\": 14}) In\u00a0[\u00a0]: Copied! <pre># Calculate accuracy (a classification metric)\ndef accuracy_fn(y_true, y_pred):\n\"\"\"Calculates accuracy between truth labels and predictions.\n\n    Args:\n        y_true (torch.Tensor): Truth labels for predictions.\n        y_pred (torch.Tensor): Predictions to be compared to predictions.\n\n    Returns:\n        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n    \"\"\"\n    correct = torch.eq(y_true, y_pred).sum().item()\n    acc = (correct / len(y_pred)) * 100\n    return acc\n</pre> # Calculate accuracy (a classification metric) def accuracy_fn(y_true, y_pred):     \"\"\"Calculates accuracy between truth labels and predictions.      Args:         y_true (torch.Tensor): Truth labels for predictions.         y_pred (torch.Tensor): Predictions to be compared to predictions.      Returns:         [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45     \"\"\"     correct = torch.eq(y_true, y_pred).sum().item()     acc = (correct / len(y_pred)) * 100     return acc In\u00a0[\u00a0]: Copied! <pre>def print_train_time(start, end, device=None):\n\"\"\"Prints difference between start and end time.\n\n    Args:\n        start (float): Start time of computation (preferred in timeit format). \n        end (float): End time of computation.\n        device ([type], optional): Device that compute is running on. Defaults to None.\n\n    Returns:\n        float: time between start and end in seconds (higher is longer).\n    \"\"\"\n    total_time = end - start\n    print(f\"\\nTrain time on {device}: {total_time:.3f} seconds\")\n    return total_time\n</pre> def print_train_time(start, end, device=None):     \"\"\"Prints difference between start and end time.      Args:         start (float): Start time of computation (preferred in timeit format).          end (float): End time of computation.         device ([type], optional): Device that compute is running on. Defaults to None.      Returns:         float: time between start and end in seconds (higher is longer).     \"\"\"     total_time = end - start     print(f\"\\nTrain time on {device}: {total_time:.3f} seconds\")     return total_time In\u00a0[\u00a0]: Copied! <pre># Plot loss curves of a model\ndef plot_loss_curves(results):\n\"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n    loss = results[\"train_loss\"]\n    test_loss = results[\"test_loss\"]\n\n    accuracy = results[\"train_acc\"]\n    test_accuracy = results[\"test_acc\"]\n\n    epochs = range(len(results[\"train_loss\"]))\n\n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label=\"train_loss\")\n    plt.plot(epochs, test_loss, label=\"test_loss\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n    plt.title(\"Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n</pre> # Plot loss curves of a model def plot_loss_curves(results):     \"\"\"Plots training curves of a results dictionary.      Args:         results (dict): dictionary containing list of values, e.g.             {\"train_loss\": [...],              \"train_acc\": [...],              \"test_loss\": [...],              \"test_acc\": [...]}     \"\"\"     loss = results[\"train_loss\"]     test_loss = results[\"test_loss\"]      accuracy = results[\"train_acc\"]     test_accuracy = results[\"test_acc\"]      epochs = range(len(results[\"train_loss\"]))      plt.figure(figsize=(15, 7))      # Plot loss     plt.subplot(1, 2, 1)     plt.plot(epochs, loss, label=\"train_loss\")     plt.plot(epochs, test_loss, label=\"test_loss\")     plt.title(\"Loss\")     plt.xlabel(\"Epochs\")     plt.legend()      # Plot accuracy     plt.subplot(1, 2, 2)     plt.plot(epochs, accuracy, label=\"train_accuracy\")     plt.plot(epochs, test_accuracy, label=\"test_accuracy\")     plt.title(\"Accuracy\")     plt.xlabel(\"Epochs\")     plt.legend() In\u00a0[\u00a0]: Copied! <pre># Pred and plot image function from notebook 04\n# See creation: https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function\nfrom typing import List\nimport torchvision\n</pre> # Pred and plot image function from notebook 04 # See creation: https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function from typing import List import torchvision In\u00a0[\u00a0]: Copied! <pre>def pred_and_plot_image(\n    model: torch.nn.Module,\n    image_path: str,\n    class_names: List[str] = None,\n    transform=None,\n    device: torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n):\n\"\"\"Makes a prediction on a target image with a trained model and plots the image.\n\n    Args:\n        model (torch.nn.Module): trained PyTorch image classification model.\n        image_path (str): filepath to target image.\n        class_names (List[str], optional): different class names for target image. Defaults to None.\n        transform (_type_, optional): transform of target image. Defaults to None.\n        device (torch.device, optional): target device to compute on. Defaults to \"cuda\" if torch.cuda.is_available() else \"cpu\".\n    Returns:\n        Matplotlib plot of target image and model prediction as title.\n\n    Example usage:\n        pred_and_plot_image(model=model,\n                            image=\"some_image.jpeg\",\n                            class_names=[\"class_1\", \"class_2\", \"class_3\"],\n                            transform=torchvision.transforms.ToTensor(),\n                            device=device)\n    \"\"\"\n\n    # 1. Load in image and convert the tensor values to float32\n    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n\n    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n    target_image = target_image / 255.0\n\n    # 3. Transform if necessary\n    if transform:\n        target_image = transform(target_image)\n\n    # 4. Make sure the model is on the target device\n    model.to(device)\n\n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n        # Add an extra dimension to the image\n        target_image = target_image.unsqueeze(dim=0)\n\n        # Make a prediction on image with an extra dimension and send it to the target device\n        target_image_pred = model(target_image.to(device))\n\n    # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 7. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    # 8. Plot the image alongside the prediction and prediction probability\n    plt.imshow(\n        target_image.squeeze().permute(1, 2, 0)\n    )  # make sure it's the right size for matplotlib\n    if class_names:\n        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    else:\n        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    plt.title(title)\n    plt.axis(False)\n</pre> def pred_and_plot_image(     model: torch.nn.Module,     image_path: str,     class_names: List[str] = None,     transform=None,     device: torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\", ):     \"\"\"Makes a prediction on a target image with a trained model and plots the image.      Args:         model (torch.nn.Module): trained PyTorch image classification model.         image_path (str): filepath to target image.         class_names (List[str], optional): different class names for target image. Defaults to None.         transform (_type_, optional): transform of target image. Defaults to None.         device (torch.device, optional): target device to compute on. Defaults to \"cuda\" if torch.cuda.is_available() else \"cpu\".          Returns:         Matplotlib plot of target image and model prediction as title.      Example usage:         pred_and_plot_image(model=model,                             image=\"some_image.jpeg\",                             class_names=[\"class_1\", \"class_2\", \"class_3\"],                             transform=torchvision.transforms.ToTensor(),                             device=device)     \"\"\"      # 1. Load in image and convert the tensor values to float32     target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)      # 2. Divide the image pixel values by 255 to get them between [0, 1]     target_image = target_image / 255.0      # 3. Transform if necessary     if transform:         target_image = transform(target_image)      # 4. Make sure the model is on the target device     model.to(device)      # 5. Turn on model evaluation mode and inference mode     model.eval()     with torch.inference_mode():         # Add an extra dimension to the image         target_image = target_image.unsqueeze(dim=0)          # Make a prediction on image with an extra dimension and send it to the target device         target_image_pred = model(target_image.to(device))      # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)     target_image_pred_probs = torch.softmax(target_image_pred, dim=1)      # 7. Convert prediction probabilities -&gt; prediction labels     target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)      # 8. Plot the image alongside the prediction and prediction probability     plt.imshow(         target_image.squeeze().permute(1, 2, 0)     )  # make sure it's the right size for matplotlib     if class_names:         title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"     else:         title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"     plt.title(title)     plt.axis(False) In\u00a0[\u00a0]: Copied! <pre>def set_seeds(seed: int=42):\n\"\"\"Sets random sets for torch operations.\n\n    Args:\n        seed (int, optional): Random seed to set. Defaults to 42.\n    \"\"\"\n    # Set the seed for general torch operations\n    torch.manual_seed(seed)\n    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n    torch.cuda.manual_seed(seed)\n</pre> def set_seeds(seed: int=42):     \"\"\"Sets random sets for torch operations.      Args:         seed (int, optional): Random seed to set. Defaults to 42.     \"\"\"     # Set the seed for general torch operations     torch.manual_seed(seed)     # Set the seed for CUDA torch operations (ones that happen on the GPU)     torch.cuda.manual_seed(seed) In\u00a0[\u00a0]: Copied! <pre>def download_data(source: str, \n                  destination: str,\n                  remove_source: bool = True) -&gt; Path:\n\"\"\"Downloads a zipped dataset from source and unzips to destination.\n\n    Args:\n        source (str): A link to a zipped file containing data.\n        destination (str): A target directory to unzip data to.\n        remove_source (bool): Whether to remove the source after downloading and extracting.\n    Returns:\n        pathlib.Path to downloaded data.\n    Example usage:\n        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                      destination=\"pizza_steak_sushi\")\n    \"\"\"\n    # Setup path to data folder\n    data_path = Path(\"data/\")\n    image_path = data_path / destination\n\n    # If the image folder doesn't exist, download it and prepare it... \n    if image_path.is_dir():\n        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n    else:\n        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n        image_path.mkdir(parents=True, exist_ok=True)\n        \n        # Download pizza, steak, sushi data\n        target_file = Path(source).name\n        with open(data_path / target_file, \"wb\") as f:\n            request = requests.get(source)\n            print(f\"[INFO] Downloading {target_file} from {source}...\")\n            f.write(request.content)\n\n        # Unzip pizza, steak, sushi data\n        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n            print(f\"[INFO] Unzipping {target_file} data...\") \n            zip_ref.extractall(image_path)\n\n        # Remove .zip file\n        if remove_source:\n            os.remove(data_path / target_file)\n    \n    return image_path\n</pre> def download_data(source: str,                    destination: str,                   remove_source: bool = True) -&gt; Path:     \"\"\"Downloads a zipped dataset from source and unzips to destination.      Args:         source (str): A link to a zipped file containing data.         destination (str): A target directory to unzip data to.         remove_source (bool): Whether to remove the source after downloading and extracting.          Returns:         pathlib.Path to downloaded data.          Example usage:         download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                       destination=\"pizza_steak_sushi\")     \"\"\"     # Setup path to data folder     data_path = Path(\"data/\")     image_path = data_path / destination      # If the image folder doesn't exist, download it and prepare it...      if image_path.is_dir():         print(f\"[INFO] {image_path} directory exists, skipping download.\")     else:         print(f\"[INFO] Did not find {image_path} directory, creating one...\")         image_path.mkdir(parents=True, exist_ok=True)                  # Download pizza, steak, sushi data         target_file = Path(source).name         with open(data_path / target_file, \"wb\") as f:             request = requests.get(source)             print(f\"[INFO] Downloading {target_file} from {source}...\")             f.write(request.content)          # Unzip pizza, steak, sushi data         with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:             print(f\"[INFO] Unzipping {target_file} data...\")              zip_ref.extractall(image_path)          # Remove .zip file         if remove_source:             os.remove(data_path / target_file)          return image_path"}]}