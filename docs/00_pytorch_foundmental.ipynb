{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e047654d-2219-44f0-bc44-f38c02637e81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 前言\n",
    "\n",
    "目前我也不是初学者了，对于深度学习相关的知识已经建立了相应的了解，但是说实话是没有形成知识框架的，因此还是需要再看的。\n",
    "\n",
    "其次我最开始的项目是基于Tensorflow的，我对他的感觉是模块性好，但定制性不够。通过它来作为新手学习的话并不是非常有利于知识的了解。而pytorch我觉得更加的直观，开放。\n",
    "\n",
    "pytorch的相关内容我已经了解过了，但是时间间隔一个月，又放下了，所以这里重新在看一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3606630-3fea-4569-8c75-119aa05632dc",
   "metadata": {},
   "source": [
    "# 资料来源\n",
    "Learn PyTorch for Deep Learning: Zero to Mastery book. [link](https://www.learnpytorch.io)，从0开始的pytorch学习。\n",
    "\n",
    "作者认为这是第二好的学习pytorch的地方，（第一好的是pytorhc的文档）。粗看起来确实不错，接下来的内容是根据相关的内容添加自己的理解写成的。英语不错的话还是推荐看[原内容](https://www.learnpytorch.io)，作者提供的非常丰富的形式来进行展示（colab，Youtube）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee978fc-4b0c-4caa-866c-3ca0e890870f",
   "metadata": {},
   "source": [
    "# Chapter 00 Pytorch 基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b4e59-4525-4da8-be77-6cb962b0c4d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## pytorch简介\n",
    "\n",
    "pytorch是一个开源的机器学习和深度学习的框架，它可以基于python来处理数据和构建机器学习的算法。pytorch非常优秀，包括微软，Tesla和OpenAI等多个大的团队都是给予pytorch来进行他们的研究。\n",
    "\n",
    "它的发展也非常迅速，在2022年已发表文献的代码类型中占比59%，已经是研究者最爱使用的代码。pytorch对于GPU加速的支持非常的好，所以你可以专注于你的数据和算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474d517-6ad0-4afa-84d2-f23d924ba1b7",
   "metadata": {},
   "source": [
    "## pytorch的安装和导入\n",
    "\n",
    "在pytorch的官网上，可以根据自己的系统类型等需求安装合适的torch版本，我这里是在linux系统中，通过conda安装的pytorch2.0版本，CUDA是11.8。命令如下`conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia`\n",
    "\n",
    "2.0版本是近期发布的（2023.3），增加了torch.compile，更快更强。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543dc4fd-d33b-4623-97ee-82580eb5fa41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd760a0-a7f5-4b01-a1ae-acec8b08c08b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 张量（Tensor）\n",
    "Tensor可以理解为多维数组，如标量可以看作0维张量，向量可以看作1维张量，矩阵可以看作2维张量。Tensor是构成机器学习的最基本的模块。\n",
    "\n",
    "比如对于有个255x255的RGB图像，将其表示为张量，它的表示形式是[3,255,255],分别指代的是图像通道，长和宽。\n",
    "\n",
    "接下来我们通过pytorch来分别构建标量，向量，矩阵和张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e3fd5d9-7284-440d-a702-3d5940aed06c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5), 0, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scalar\n",
    "scalar = torch.tensor(5)\n",
    "scalar,scalar.ndim,scalar.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15679380-4e66-4815-9efb-0533adeee023",
   "metadata": {},
   "source": [
    "我们构建了一个为5的标量，通过ndim属性（attribute）查看了它的维度是0维，通过item（）方法（method）查看了包含的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bbb7ee1-0889-42d2-bbe4-54a28df4b070",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5, 5, 5]), 1, torch.Size([3]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vector\n",
    "vector = torch.tensor([5,5,5])\n",
    "vector,vector.ndim,vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11147ba3-038d-4852-8dfe-3a6d0d74ad0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "在我们构建完成向量之后，可以通过ndim发现它的维度是1维的，通过shape参数可以发现其中包含多少个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d460229-cbdb-4f68-9bad-c685d0403310",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2],\n",
       "         [3, 4]]),\n",
       " 2,\n",
       " torch.Size([2, 2]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## matrix\n",
    "matrix = torch.tensor([[1,2],[3,4]])\n",
    "matrix,matrix.ndim,matrix.shape\n",
    "#这里结果是2维的，shape展示出是2维，每个维度又两个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6aa7e69-1dfe-4836-aae7-1abb8856fb71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2],\n",
       "          [3, 4]]]),\n",
       " 3,\n",
       " torch.Size([1, 2, 2]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor\n",
    "Tensor = torch.tensor([[[1,2],[3,4]]])\n",
    "Tensor,Tensor.ndim,Tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b89783-ed91-40cc-9640-203db1dbf226",
   "metadata": {},
   "source": [
    "通过上面的例子，就可以对Tensor有一定的理解。而在实际的应用中，我们在极少的情况下才需要手动构建张量。\n",
    "\n",
    "相反的是，在深度学习中我们需要经常随机构建足够大的张量，然后在后续的训练中不断更新和优化这个张量，使模型达到更好的效果，详细的内容后面会涉及。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cbe000d-dc98-4af0-852f-a6d991b0a254",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3783, 0.8493, 0.4532, 0.1768],\n",
       "         [0.4392, 0.9926, 0.8977, 0.7206],\n",
       "         [0.2378, 0.8719, 0.8344, 0.2842]]),\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## random tensor\n",
    "random_tensor = torch.rand(size = (3,4))\n",
    "random_tensor,random_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d47a1-18e1-4523-8470-d0996deff739",
   "metadata": {},
   "source": [
    "我们可以通过`torch.rand`来构建随机向量，其中size参数用来制定张量的形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f63ee8-9cd4-48c3-83aa-c087bb456c01",
   "metadata": {},
   "source": [
    "除了随机向量外，我们有时需要构建全为0或1的张量，torch也提供了特定的函数来构建,于随机张量的构建逻辑是相同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6729952f-1da7-4b6e-8352-86bf0482be7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero&one\n",
    "zeros = torch.zeros(size = (3,4))\n",
    "ones = torch.ones(size = (3,4))\n",
    "zeros,ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ad503-75db-4fa3-b783-1953bbeed18e",
   "metadata": {},
   "source": [
    "我们还可以通过特定范围和步长来构建一个张量，需要使用`torch.arange`函数可以实现，需要制定开始（start），结束（end）和步长（step）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fb1afa0-fd11-4c2c-a8d9-86b44b4964b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_tensor = torch.arange(start=10,end = 100,step = 10)\n",
    "range_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9012d36-2ec4-4595-8bd5-451f33d736fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "还有一种情况是我们需要根据已有向量来构建形状一样的新的向量，这里可以使用`torch.zeros_like(input)`和`torch.ones_like(input)`来构建，input是已有的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63d66e26-3d31-46d5-abb6-5a526b3ce5b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(range_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af2d56-9759-458a-a445-24e028435b1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "pytorch中tensor具有非常多的[数据类型](https://pytorch.org/docs/stable/tensors.html#data-types)，不同的数据类型对后续模型的计算有着很大的影响，比较重要的一个是数据越精确，计算量就越大。在构建tensor的时候还有几个参数可以考虑，dtype为数据类型，device为存储位置，raquires_grad跟后面进行梯度下降时相关，默认的dtype是 float32。我们可查看之前构建的随机张量相关默认信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "defcd69a-1ac6-49fc-997b-fede4db8f24a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, device(type='cpu'), False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.dtype,random_tensor.device,random_tensor.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a46d7-2110-4362-bf2c-c76fff3f32b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "综合上面的内容，我们在得到一个tensor时，可以通过shape,dtype,device,ndim等属性查看这个张量的相关信息，让我们可以对输入的数据有一个基本的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce10fcd-cfa7-45b2-b4ab-9db4806ef0ba",
   "metadata": {},
   "source": [
    "## 操作张量（tensor operations）\n",
    "\n",
    "### 张量计算\n",
    "\n",
    "我们已经了解了如何去构建张量，而在深度学习中，对张量的操作才是最为重要的。不同模型的架构本质上是使用不同的计算方式（策略）来去对张量进行处理。常见的计算方式是：加，减，乘，除和矩阵乘法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76ff0334-a42f-4104-952e-882bb5c69599",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([11, 12, 13]),\n",
       " tensor([10, 20, 30]),\n",
       " tensor([11, 12, 13]),\n",
       " tensor([10, 20, 30]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 基本的计算方式，+，-，*\n",
    "tensor = torch.tensor([1,2,3])\n",
    "#1. 不使用内置函数\n",
    "tensor1 = tensor + 10\n",
    "tensor2 = tensor * 10\n",
    "#2. 使用内置函数\n",
    "tensor3 = torch.add(tensor,10) # 等价于tensor.add(10)\n",
    "tensor4 = torch.mul(tensor,10) # 等价于tensor.mal(10)\n",
    "\n",
    "tensor1,tensor2,tensor3,tensor4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c357e1f-3778-4fdb-b0f3-51ac1888f035",
   "metadata": {},
   "source": [
    "\n",
    "从上面的计算中还有两个点，1.如果不重新赋值的话，原值并不会修改。2.计算过程中使用了广播机制（broadcast）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9cad0b40-4da5-4848-a389-e0f95286f91a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 4, 9]), tensor(14))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 元素相乘和矩阵相乘\n",
    "# 这里的矩阵相乘是涉及到线性代数中的知识\n",
    "tensor5 = tensor.mul(tensor)\n",
    "tensor6 = tensor.matmul(tensor)\n",
    "tensor5,tensor6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a21f07-2067-4d22-b7d2-396437bbaf1e",
   "metadata": {},
   "source": [
    "在进行矩阵乘法的时候，最容易出错的点是两个矩阵的形状不能相符合，两个矩阵要满足 x * m,m * y的形状，获得的是x * y的矩阵。tensor中提供个转置的函数，`tensor.T`和`torch.transpose`，可用来应对这种情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9eb79f0-5630-457c-804f-c195179192de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "tensor([[ 27.,  30.,  33.],\n",
      "        [ 61.,  68.,  75.],\n",
      "        [ 95., 106., 117.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]),\n",
       " torch.Size([3, 2]),\n",
       " torch.Size([2, 3]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)\n",
    "print(tensor_B.T)\n",
    "output = torch.matmul(tensor_A, tensor_B.T)\n",
    "print(output) \n",
    "tensor_A.shape,tensor_B.shape,tensor_B.T.shape,output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd37fd-7af7-457a-9b55-f22477bf3755",
   "metadata": {},
   "source": [
    "矩阵相乘的相关知识可以去看一点线性代数的内容，它的可视化计算可以看这个[网站](http://matrixmultiplication.xyz/)。可以使用`torch.mm`来当作矩阵计算的简写。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd42c0-6474-4f05-83ea-606d7bccbe3f",
   "metadata": {},
   "source": [
    "这里的内容可能会又写超前，可以先提前看一下：\n",
    "\n",
    "神经网络中计划全部都是点乘和矩阵运算，在pytorch中`troch.nn.Linear()`定义了最简单的神经网络-前馈神经网络或者叫全链接神经网络，它将输入的张量`x`乘以权重`A`的转置并且加上偏执项`b`得到到输出，模型的训练则是不断的更新权重`A`，是模型能够更好的表示数据的特征，偏执项（bias）的目的是为了更好的拟合数据。\n",
    "\n",
    "数学公式为：$y=x\\ast A^{T}+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4450949-65ed-4282-b2ec-d4876771fb02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 2])\n",
      "\n",
      "Output:\n",
      "tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n",
      "        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n",
      "        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# 设定随机数种子，目的是为了复现代码，因为神经网络在初始化权重的时候是随机初始化的\n",
    "torch.manual_seed(42)\n",
    "# This uses matrix multiplication\n",
    "linear = torch.nn.Linear(in_features=2, # \n",
    "                         out_features=6) # in_features和out_features是制定权重矩阵的形状 2*6\n",
    "x = tensor_A\n",
    "output = linear(x)\n",
    "print(f\"Input shape: {x.shape}\\n\")\n",
    "print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32de82-eecc-41b7-a41f-60ff32029f7a",
   "metadata": {},
   "source": [
    "### 张量特征\n",
    "\n",
    "pytorch提供了一系列的函数用来计算最大值，最小值，平均值，和等特征，而且可以去查找相应最大值或者最小值的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16aa569b-98cd-407f-991c-8983ce2ca8f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7104, 0.9464, 0.7890, 0.2814],\n",
       "         [0.7886, 0.5895, 0.7539, 0.1952],\n",
       "         [0.0050, 0.3068, 0.1165, 0.9103]]),\n",
       " tensor(0.9464),\n",
       " tensor(0.0050),\n",
       " tensor(6.3932),\n",
       " tensor(0.5328),\n",
       " tensor(1),\n",
       " tensor(8))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(size = (3,4))\n",
    "tensor,tensor.max(),tensor.min(),tensor.sum(),tensor.mean(),tensor.argmax(),tensor.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d00f410d-b218-44da-8749-ed387ba5de37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#也可以去改变tensor的数据类型，不过说实话这个需要使用到的场景很少\n",
    "tensor.dtype,tensor.type(torch.float64).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99490af-a50d-47c3-a5d6-28268880d868",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 张量的形状操作\n",
    "我们已经了解到了神经网络中几乎全是矩阵乘法，但是矩阵乘法对于两个张量的形状要求非常的严格，所以pytorch提供了一系列的函数来去修改张量的形状，以便于我们能够正确的进行矩阵运算。可供使用的函数有`torch.reshape()`,`torch.Tensor.view()`,`torch.stack()`,`torch.squeeze()`,`torch.unsqueeze()`,`torch.permute()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "60c3fc77-dd6b-486a-b8e3-6ea621a09bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2., 3., 4., 5., 6., 7.]), x.reshape: tensor([[1., 2., 3., 4., 5., 6., 7.]])\n",
      "x: tensor([10.,  2.,  3.,  4.,  5.,  6.,  7.]), x.reshape: tensor([[10.,  2.,  3.,  4.,  5.,  6.,  7.]])\n",
      "x: tensor([1., 2., 3., 4., 5., 6., 7.]), x.reshape: tensor([[1., 2., 3., 4., 5., 6., 7.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1., 8.)\n",
    "x_reshaped = x.reshape(1, 7)\n",
    "print(\"x: {}, x.reshape: {}\".format(x,x_reshaped))\n",
    "x[0] =10\n",
    "print(\"x: {}, x.reshape: {}\".format(x,x_reshaped))\n",
    "x_reshaped[:,0] = 1\n",
    "print(\"x: {}, x.reshape: {}\".format(x,x_reshaped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629124ae-0367-4005-87db-825a98d1480b",
   "metadata": {},
   "source": [
    "从上面的输出可以看出，reshape只会改变size，reshpe之后的赋值向量是于之前向量有连接关系的（使用相同的内存）。需要通过tensor.clone()来为其构建新的内存，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7ae35dc-f8f9-4911-9883-a457238b47c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1., 10.,  3.,  4.,  5.]),\n",
       " tensor([ 1., 10.,  3.,  4.,  5.]),\n",
       " tensor([1., 2., 3., 4., 5.]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(1.0,6.0)\n",
    "b = a\n",
    "c = a.clone()\n",
    "a[1] = 10\n",
    "a,b,c\n",
    "#从结果可以看出.colne()之后则不会进行同步变化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c628ee30-f9ed-4a7f-a3df-70d535d2f059",
   "metadata": {},
   "source": [
    "`torch.Tensor.view()`这个我目前还是不能理解其特性，总觉得他跟reshape是相互重叠的。\n",
    "\n",
    "通过`torch.stack()`函数可以对张量安装某个维度进行进行堆叠,其中dim(范围是[-2,1]))指定的是维度,常用的是dim=0就足够了,dim=1相当于转置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff46ca96-da72-4be6-84a8-67d82cc5e691",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4., 5., 6., 7.],\n",
      "        [1., 2., 3., 4., 5., 6., 7.]])\n",
      "tensor([[1., 1.],\n",
      "        [2., 2.],\n",
      "        [3., 3.],\n",
      "        [4., 4.],\n",
      "        [5., 5.],\n",
      "        [6., 6.],\n",
      "        [7., 7.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([x,x],dim=0))\n",
    "print(torch.stack([x,x],dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "86c817c4-6929-486b-ae9b-3cea4a72f292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7.]),\n",
       " tensor([[1., 2., 3., 4., 5., 6., 7.]]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.]),\n",
       " tensor([[1., 2., 3., 4., 5., 6., 7.]]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#squeeze和unsqueeze两个函数是分别对向量进行降维和升维，感觉并不特殊，reshape可以完成这两个任务。\n",
    "x,x.unsqueeze(dim=0),x.unsqueeze(dim=0).squeeze(),x.reshape([1,7]),x.reshape([1,7]).reshape(7) #原始，升维，降维，通过reshape升维，通过reshape降维"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87f6a1-13e8-4626-92de-9b66e55b2c72",
   "metadata": {},
   "source": [
    "permute是比较特殊的函数，它可以用来改变不同维度的位置，例子如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53c7118f-d64d-4821-969d-7c285f809e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: torch.Size([224, 224, 3])\n",
      "New shape: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x_original = torch.rand(size=(224, 224, 3))\n",
    "x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n",
    "\n",
    "print(f\"Previous shape: {x_original.shape}\")\n",
    "print(f\"New shape: {x_permuted.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f2fca-0644-480a-ad77-8b74b75e3b87",
   "metadata": {},
   "source": [
    "### 张量的索引\n",
    "获得Tensor特定位置的数据的行为是经常用到的。张量的索引跟numpy的方式非常的类似，这里展示两种方式（多个中括号和一个中括号）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9b75dbb7-0478-4a08-a1ba-3113e26a6dff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2, 3],\n",
       "          [4, 5, 6],\n",
       "          [7, 8, 9]]]),\n",
       " tensor([[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]),\n",
       " tensor([1, 2, 3]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1, 10).reshape(1, 3, 3)\n",
    "x,x[0],x[0][0],x[0][0][0] #第一种方式类似于R中的概念，通过`[]`的方式来解开中括号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "54767c81-64f7-4ff3-8938-f002001a084a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3]]), tensor([[1, 4, 7]]), tensor([1]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0,:],x[:,:,0],x[:,0,0] #第二种方式不太好理解，但是更加的灵活，可以进行竖排的取。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea804db0-ede3-4c35-8a94-9cc282ba8049",
   "metadata": {},
   "source": [
    "### numpy与pytorch tensor\n",
    "\n",
    "最开始的机器学习好像是基于numpy的，因此现有的深度学习框架中会有numpy与tensor的转换，numpy中的多维数组与tensor性质是相似的。\n",
    "\n",
    "pytorch有两个函数可以用来之间的互相转化：`torch.from_numpy(ndarray)`：ndarray to tensor;`torch.Tensor.numpy()`:tensor to ndarray。\n",
    "\n",
    "还有一个要关注的点是numpy中使用的是float64位，pytorch是float32位，因此可能需要一些数据类型的转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45761ff1-4476-4ecd-96c3-0f05fe4ef606",
   "metadata": {},
   "source": [
    "## 其他\n",
    "\n",
    "这里还有一些其他需要注意的要点：\n",
    "\n",
    "### 复现（Reproducibility）\n",
    "为什么需要设置随机数呢？因为我们想要复现每次的结果，有利于别人也有益于自己（比较不同超参数的影响）。pytorch通过了相应的函数来设置随机数种子：`torch.manual_seed()`。seed可以是任意的数字，该函数可以保证两次运行过程中生成的结果是一样的，但是要在一次运行中生成相同的随机数，需要重新在设置一次，如下：\n",
    "\n",
    "`torch.manual_seed()`也很强大，它会为所有的设备设置随机数，包括cpu和gpu。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c048e9d8-62e8-42f5-816f-dc409cc97636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor C:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Tensor D:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Does Tensor C equal Tensor D? (anywhere)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Set the random seed\n",
    "RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\n",
    "torch.manual_seed(seed=RANDOM_SEED) \n",
    "random_tensor_C = torch.rand(3, 4)\n",
    "\n",
    "# Have to reset the seed every time a new rand() is called \n",
    "# Without this, tensor_D would be different to tensor_C \n",
    "torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens，注释掉之后结果就不再想通了。\n",
    "random_tensor_D = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor C:\\n{random_tensor_C}\\n\")\n",
    "print(f\"Tensor D:\\n{random_tensor_D}\\n\")\n",
    "print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n",
    "random_tensor_C == random_tensor_D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c5a74c-5f44-49e7-a6ed-878c4cfc33be",
   "metadata": {},
   "source": [
    "### 运行设备\n",
    "深度学习模型中包含大量的矩阵运算，而GPU即显卡的矩阵运算能力要远强于CPU，通过GPU可以显著的提升运算的速度。GPU的详细资料就不再这里涉及了，日常使用中Nvidia的GPU是最常见的，我们可以通过`!nvidia-smi`命令来查看显卡的详细信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d1e575c2-bedf-4a66-bd51-ac188692484f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 12 14:48:43 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 38%   29C    P8    22W / 350W |  14032MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2726      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A   2117643      C   ...conda3/envs/dl/bin/python    10888MiB |\n",
      "|    0   N/A  N/A   2444529      C   ...conda3/envs/dl/bin/python     3136MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "924db0ff-c0c8-4dc4-a548-0122838ba7d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(),torch.cuda.device_count() #查看显卡是否可用与有多少个显卡可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7bece4fb-c5af-4605-a0d0-29eac792537d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" #这个命令将是后续最常用的命令，用来设置运行设备，有显卡的话则会在显卡上运行\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3b0001bd-36b8-4085-8670-0b545c7d935d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7104, 0.9464, 0.7890, 0.2814],\n",
       "         [0.7886, 0.5895, 0.7539, 0.1952],\n",
       "         [0.0050, 0.3068, 0.1165, 0.9103]]),\n",
       " tensor([[0.7104, 0.9464, 0.7890, 0.2814],\n",
       "         [0.7886, 0.5895, 0.7539, 0.1952],\n",
       "         [0.0050, 0.3068, 0.1165, 0.9103]], device='cuda:0'))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我么可以讲tensor和model放在GPU上进行运行\n",
    "tensor,tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705314b9-0216-499f-a087-746c019a9c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
