{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9b83a3-cd88-446c-8f15-0760f5b7f666",
   "metadata": {},
   "source": [
    "# basic Transformer code\n",
    "\n",
    "[资料](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
    "\n",
    "https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch\n",
    "\n",
    "model figure are form *Attention is all you need*\n",
    "\n",
    "![](../doc_images/learn/model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a7422482-de6c-4061-8596-c15f9cf1f455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472acaad-3c9b-4b96-a776-2c0a040009d3",
   "metadata": {},
   "source": [
    "## Postional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9396b95f-b850-4a1d-83cb-234f92bceddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout=0.0):\n",
    "        \"\"\"\n",
    "        :param d_model: length of vector\n",
    "        :param max_len: max sequence length\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoding = torch.zeros(1,max_len, d_model)\n",
    "        self.encoding.requires_grad = False\n",
    "        \n",
    "        pos = torch.arange(0,max_len)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        \n",
    "        _2i = torch.arange(0, d_model, step=2).float()\n",
    "        self.encoding[:, :, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, :, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        x = x + self.encoding.to(x.device)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2391d55b-cf5f-4059-8847-400642a53882",
   "metadata": {},
   "source": [
    "这个模块完成的模型输入部分，主要是包括编码向量和位置编码的相加。同时也考虑了dropout层和bacth_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c428fa7-5f5b-4ed6-8837-f2ae7dcf1ddf",
   "metadata": {},
   "source": [
    "## Scale Dot-Product attention\n",
    "![](../doc_images/learn/Scaled_Dot_Product_attention.png)\n",
    "\n",
    "注意力机制中是需要相应的计算方式的，Transformer使用的是点积注意力机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "299594a6-0ed0-43b8-ab45-0d4df7d099d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Scale_Dot_Produce_attention(query,key,value,mask=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query,key.transpose(-2,-1)) \\\n",
    "                                / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0,\"-inf\")\n",
    "    attention = torch.matmul(nn.softmax(dim = -1)(scores),value)\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb05ade-c8ff-4073-b92c-054d18f5eb71",
   "metadata": {},
   "source": [
    "## multi head attention\n",
    "\n",
    "![](../doc_images/learn/multihead_attention.png)\n",
    "\n",
    "论文的描述是通过多个W（线性层）映射到子空间里，但是实际操作时是通过一个映射然后切分，并且在代码操作中也没有切分，而是直接通过改变形状实现了并行运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "317e086b-ea3c-46ab-beb1-f77d4b20ca92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        q,k,v.shape = (b:batch_size,l:seq_len,d_m: dim of input)\n",
    "        '''\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query,key,value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear transformation for query, key, and value\n",
    "        query = self.query_linear(query)   # (b,l,d_m) -> (b,l,d_m)\n",
    "        key = self.key_linear(key)\n",
    "        value = self.value_linear(value)\n",
    "        \n",
    "        # Splitting heads\n",
    "        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2) \n",
    "        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        '''\n",
    "        .view(): (b,l,d_m) -> (b,l,h,d_m/h)\n",
    "        .transpose(1,2): (b,l,h,d_m/h) -> (b,h,l,d_m/h)\n",
    "        '''\n",
    "        # Attention scores and scaled dot-product attention\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim) #q*transpose(k) (b,h,l,d_m/h) * (b,h,d_m/h,l) = (b,h,l,l)\n",
    "       \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf')) # make zero to -inf\n",
    "        \n",
    "        attention = nn.Softmax(dim=-1)(scores)\n",
    "        output = torch.matmul(attention, value) # (b,h,l,l) * (b,h,l,d_m/h) = (b,h,l,d_m/h)\n",
    "        \n",
    "        # Concatenating heads and linear transformation\n",
    "        output = output.transpose(1, 2).view(batch_size, -1, self.d_model) \n",
    "        '''\n",
    "        .transpose(1,2): (b,h,l,d_m/h) -> (b,l,h,d_m/h)\n",
    "        .view() : (b,l,h,d_m/h) -> (b,l,d_m)\n",
    "        '''\n",
    "        output = self.output_linear(output)\n",
    "        \n",
    "        return output,attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8861c9-ae1d-4ecd-a1f2-85a949061d04",
   "metadata": {},
   "source": [
    "## Encoder block\n",
    "\n",
    "接下来实现它的Encoder部分，这部分要考虑的就是残差连接，层标准化和前馈神经网络。\n",
    "\n",
    "先残差了连接，然后在层标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5798bc2b-3342-4523-a3e9-84b8087d06ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,dim_FFN,dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model,num_heads)\n",
    "        \n",
    "        #Feed forward\n",
    "        self.FFW = torch.nn.Sequential(\n",
    "            nn.Linear(d_model,dim_FFN),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_FFN,d_model))\n",
    "        \n",
    "        #LayerNorm\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x,mask = None):\n",
    "        atten_out,atten = self.self_attention(x,x,x,mask)\n",
    "        x = x + self.dropout(atten_out)\n",
    "        x = self.norm1(x)\n",
    "        x = self.FFW(x)\n",
    "        x = x + self.dropout(atten_out)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c1f1b3db-bd76-48ad-91eb-96fd1b90adbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_layers,max_len,d_model,num_heads,dim_FNN,dropout):\n",
    "        super().__init__()\n",
    "        self.input = PositionalEncoding(d_model,max_len)\n",
    "        self.layers = nn.ModuleList([EncoderBlock(d_model,num_heads,dim_FNN,dropout) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self,x,mask=None):\n",
    "        x = self.input(x)\n",
    "        for l in self.layers:\n",
    "            x = l(x,mask = mask)\n",
    "        return x\n",
    "    \n",
    "    def get_attention_maps(self,x,mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _,atten_map = l.self_attention(x,x,x,mask)\n",
    "            attention_maps.append(atten_map)\n",
    "            x = l(x)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6b231-3203-4915-aaf7-881ac1c1210c",
   "metadata": {},
   "source": [
    "## Dcoder block\n",
    "\n",
    "Docoder相较于Encoder比较麻烦，存在一个mask操作，mask的含义是，在t时刻的输入，decoder的输入是无法看到t+1时刻的内容的，那么在实际操作的时候，是通过一个三角矩阵（右上角为0）将后面的内容变为0，在代码中替换为-inf。~~因此好像这一步操作不是在模型代码中构建的，而是在训练过程中构造的。~~\n",
    "\n",
    "在attention模块中，已经实现了mask的操作，即对query，key计算得到的权重进行mask:\n",
    "```\n",
    "if mask is not None:\n",
    "    scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "```\n",
    "他使用的一个下三角矩阵，相当于覆盖在权重矩阵上，将三角矩阵为零的对应权重矩阵的位置替换为负无穷（需要理解masked_fill的原理）。\n",
    "\n",
    "因此在构建Decoder时直接堆叠已经建好的模块即可。\n",
    "\n",
    "在Decoder中的decoder-encoder attntion的q,k,v：其中query来自于Decoder的self atention，k,v来自于encoder的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0d78a39a-f265-4a34-8c4c-bddc79bcbf53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,dim_FFN,dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model,num_heads)\n",
    "        \n",
    "        #Feed forward\n",
    "        self.FFW = torch.nn.Sequential(\n",
    "            nn.Linear(d_model,dim_FFN),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_FFN,d_model))\n",
    "        \n",
    "        #LayerNorm\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x,memory,src_mask = None,tar_mask = None):\n",
    "        self_atten_out,_ = self.self_attention(x,x,x,tar_mask)\n",
    "        x = x + self.dropout(self_atten_out)\n",
    "        x = self.norm1(x)\n",
    "        encoder_decoder_atten,_ = self.self_attention(x,memory,memory,src_mask)\n",
    "        x = x + self.dropout(encoder_decoder_atten)\n",
    "        x = self.norm2(x)\n",
    "        x = self.FFW(x)\n",
    "        x = x + self.dropout(x)\n",
    "        x = self.norm3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "95a8b8a6-2bef-4333-929a-7be1c6a590f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_layers,max_len,d_model,num_heads,dim_FNN,dropout):\n",
    "        super().__init__()\n",
    "        self.input = PositionalEncoding(d_model,max_len)\n",
    "        self.layers = nn.ModuleList([DecoderBlock(d_model,num_heads,dim_FNN,dropout) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self,x,memory,src_mask = None,tar_mask = None):\n",
    "        x = self.input(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,memory,src_mask,tar_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb12c3-9d11-40ec-a971-e0332685c67e",
   "metadata": {},
   "source": [
    "## 最终的Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "acc73c38-a8f9-4dbb-9c83-54157b15ba37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self,x,src_mask, tar_mask, tar):\n",
    "        x = self.encoder(x,src_mask)\n",
    "        return self.decoder(tar,x,src_mask,tar_mask)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a6bc134d-f2ce-418c-8914-0f30a92c6c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_layers = 4\n",
    "max_len = 30\n",
    "d_model= 512\n",
    "num_heads = 8\n",
    "dim_FNN = 128\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8eae0290-fad0-441a-ad12-187b22279fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (input): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x EncoderBlock(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (FFW): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=128, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (input): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x DecoderBlock(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (FFW): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=128, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(n_layers,max_len,d_model,num_heads,dim_FNN,dropout)\n",
    "decoder = TransformerDecoder(n_layers,max_len,d_model,num_heads,dim_FNN,dropout)\n",
    "model = Transformer(encoder,decoder)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b35e81-9ec7-4c6a-9125-b100e05a3151",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "基本上已经完全里清楚Transformer的模型框架了，上面的代码目前看起来没有问题，不过还需要实际测试才能知道。还有就是对于输入的嵌入并没有定义，因为直接使用`nn.embedding()`层可行是可行，不过直观觉得也需要自己来定义编码方式。其次，原始的模型框架是用来进行翻译的会有soruce_sequence和target_sequence。但是我并不做机器翻译，现有的框架并不适合我解决问题，需要根据自己的内容来进行修改。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
